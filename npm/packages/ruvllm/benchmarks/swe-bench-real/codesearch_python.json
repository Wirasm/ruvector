[{"code": "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    \"\"\"\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        \u251c\u2500\u2500 <person1>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 <person2>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\n        \u2514\u2500\u2500 ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    \"\"\"\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face", "docstring": "Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        \u251c\u2500\u2500 <person1>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 <person2>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\n        \u2514\u2500\u2500 ...\n\n    :param model", "func_name": "train", "language": "python"}, {"code": "def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n    \"\"\"\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    \"\"\"\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n\n    if knn_clf is None and model_path is None:\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    # Load image file and find face locations\n    X_img = face_recognition.load_image_file(X_img_path)\n    X_face_locations = face_recognition.face_locations(X_img)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test iamge\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict cla", "docstring": "Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an u", "func_name": "predict", "language": "python"}, {"code": "def show_prediction_labels_on_image(img_path, predictions):\n    \"\"\"\n    Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:\n    \"\"\"\n    pil_image = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode(\"UTF-8\")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs\n    del draw\n\n    # Display the resulting image\n    pil_image.show()", "docstring": "Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:", "func_name": "show_prediction_labels_on_image", "language": "python"}, {"code": "def _rect_to_css(rect):\n    \"\"\"\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return rect.top(), rect.right(), rect.bottom(), rect.left()", "docstring": "Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order", "func_name": "_rect_to_css", "language": "python"}, {"code": "def _trim_css_to_bounds(css, image_shape):\n    \"\"\"\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)", "docstring": "Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order", "func_name": "_trim_css_to_bounds", "language": "python"}, {"code": "def face_distance(face_encodings, face_to_compare):\n    \"\"\"\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n    \"\"\"\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)", "docstring": "Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array", "func_name": "face_distance", "language": "python"}, {"code": "def load_image_file(file, mode='RGB'):\n    \"\"\"\n    Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array\n    \"\"\"\n    im = PIL.Image.open(file)\n    if mode:\n        im = im.convert(mode)\n    return np.array(im)", "docstring": "Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array", "func_name": "load_image_file", "language": "python"}, {"code": "def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    if model == \"cnn\":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)", "docstring": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' o", "func_name": "_raw_face_locations", "language": "python"}, {"code": "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]", "docstring": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of fou", "func_name": "face_locations", "language": "python"}, {"code": "def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))", "docstring": "Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images ", "func_name": "batch_face_locations", "language": "python"}, {"code": "def face_landmarks(face_image, face_locations=None, model=\"large\"):\n    \"\"\"\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\n    \"\"\"\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            \"chin\": points[0:17],\n            \"left_eyebrow\": points[17:22],\n            \"right_eyebrow\": points[22:27],\n            \"nose_bridge\": points[27:31],\n            \"nose_tip\": points[31:36],\n            \"left_eye\": points[36:42],\n            \"right_eye\": points[42:48],\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            \"nose_tip\": [points[4]],\n            \"left_eye\": points[2:4],\n            \"right_eye\": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")", "docstring": "Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)", "func_name": "face_landmarks", "language": "python"}, {"code": "def face_encodings(face_image, known_face_locations=None, num_jitters=1):\n    \"\"\"\n    Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\n    \"\"\"\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=\"small\")\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]", "docstring": "Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)", "func_name": "face_encodings", "language": "python"}, {"code": "def _parse_datatype_string(s):\n    \"\"\"\n    Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>> _parse_datatype_string(\"int \")\n    IntegerType\n    >>> _parse_datatype_string(\"INT \")\n    IntegerType\n    >>> _parse_datatype_string(\"a: byte, b: decimal(  16 , 8   ) \")\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\n    >>> _parse_datatype_string(\"a DOUBLE, b STRING\")\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\n    >>> _parse_datatype_string(\"a: array< short>\")\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\n    >>> _parse_datatype_string(\" map<string , string > \")\n    MapType(StringType,StringType,true)\n\n    >>> # Error cases\n    >>> _parse_datatype_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    \"\"\"\n    sc = SparkContext._active_spark_context\n\n    def from_ddl_schema(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(ty", "docstring": "Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>>", "func_name": "_parse_datatype_string", "language": "python"}, {"code": "def _int_size_to_type(size):\n    \"\"\"\n    Return the Catalyst datatype from the size of integers.\n    \"\"\"\n    if size <= 8:\n        return ByteType\n    if size <= 16:\n        return ShortType\n    if size <= 32:\n        return IntegerType\n    if size <= 64:\n        return LongType", "docstring": "Return the Catalyst datatype from the size of integers.", "func_name": "_int_size_to_type", "language": "python"}, {"code": "def _infer_type(obj):\n    \"\"\"Infer the DataType from obj\n    \"\"\"\n    if obj is None:\n        return NullType()\n\n    if hasattr(obj, '__UDT__'):\n        return obj.__UDT__\n\n    dataType = _type_mappings.get(type(obj))\n    if dataType is DecimalType:\n        # the precision and scale of `obj` may be different from row to row.\n        return DecimalType(38, 18)\n    elif dataType is not None:\n        return dataType()\n\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            if key is not None and value is not None:\n                return MapType(_infer_type(key), _infer_type(value), True)\n        return MapType(NullType(), NullType(), True)\n    elif isinstance(obj, list):\n        for v in obj:\n            if v is not None:\n                return ArrayType(_infer_type(obj[0]), True)\n        return ArrayType(NullType(), True)\n    elif isinstance(obj, array):\n        if obj.typecode in _array_type_mappings:\n            return ArrayType(_array_type_mappings[obj.typecode](), False)\n        else:\n            raise TypeError(\"not supported type: array(%s)\" % obj.typecode)\n    else:\n        try:\n            return _infer_schema(obj)\n        except TypeError:\n            raise TypeError(\"not supported type: %s\" % type(obj))", "docstring": "Infer the DataType from obj", "func_name": "_infer_type", "language": "python"}, {"code": "def _infer_schema(row, names=None):\n    \"\"\"Infer the schema from dict/namedtuple/object\"\"\"\n    if isinstance(row, dict):\n        items = sorted(row.items())\n\n    elif isinstance(row, (tuple, list)):\n        if hasattr(row, \"__fields__\"):  # Row\n            items = zip(row.__fields__, tuple(row))\n        elif hasattr(row, \"_fields\"):  # namedtuple\n            items = zip(row._fields, tuple(row))\n        else:\n            if names is None:\n                names = ['_%d' % i for i in range(1, len(row) + 1)]\n            elif len(names) < len(row):\n                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))\n            items = zip(names, row)\n\n    elif hasattr(row, \"__dict__\"):  # object\n        items = sorted(row.__dict__.items())\n\n    else:\n        raise TypeError(\"Can not infer schema for type: %s\" % type(row))\n\n    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n    return StructType(fields)", "docstring": "Infer the schema from dict/namedtuple/object", "func_name": "_infer_schema", "language": "python"}, {"code": "def _has_nulltype(dt):\n    \"\"\" Return whether there is NullType in `dt` or not \"\"\"\n    if isinstance(dt, StructType):\n        return any(_has_nulltype(f.dataType) for f in dt.fields)\n    elif isinstance(dt, ArrayType):\n        return _has_nulltype((dt.elementType))\n    elif isinstance(dt, MapType):\n        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)\n    else:\n        return isinstance(dt, NullType)", "docstring": "Return whether there is NullType in `dt` or not", "func_name": "_has_nulltype", "language": "python"}, {"code": "def _create_converter(dataType):\n    \"\"\"Create a converter to drop the names of fields in obj \"\"\"\n    if not _need_converter(dataType):\n        return lambda x: x\n\n    if isinstance(dataType, ArrayType):\n        conv = _create_converter(dataType.elementType)\n        return lambda row: [conv(v) for v in row]\n\n    elif isinstance(dataType, MapType):\n        kconv = _create_converter(dataType.keyType)\n        vconv = _create_converter(dataType.valueType)\n        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())\n\n    elif isinstance(dataType, NullType):\n        return lambda x: None\n\n    elif not isinstance(dataType, StructType):\n        return lambda x: x\n\n    # dataType must be StructType\n    names = [f.name for f in dataType.fields]\n    converters = [_create_converter(f.dataType) for f in dataType.fields]\n    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)\n\n    def convert_struct(obj):\n        if obj is None:\n            return\n\n        if isinstance(obj, (tuple, list)):\n            if convert_fields:\n                return tuple(conv(v) for v, conv in zip(obj, converters))\n            else:\n                return tuple(obj)\n\n        if isinstance(obj, dict):\n            d = obj\n        elif hasattr(obj, \"__dict__\"):  # object\n            d = obj.__dict__\n        else:\n            raise TypeError(\"Unexpected obj type: %s\" % type(obj))\n\n        if convert_fields:\n            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])\n        else:\n            return tuple([d.get(name) for name in names])\n\n    return convert_struct", "docstring": "Create a converter to drop the names of fields in obj", "func_name": "_create_converter", "language": "python"}, {"code": "def _make_type_verifier(dataType, nullable=True, name=None):\n    \"\"\"\n    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())(\"\")\n    >>> _make_type_verifier(LongType())(0)\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\n    >>> _make_type_verifier(StructType([]))(())\n    >>> _make_type_verifier(StructType([]))([])\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> # Check if numeric values are within the allowed range.\n    >>> _make_type_verifier(ByteType())(12)\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> schema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType(), False)", "docstring": "Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())(\"\")\n    >>> ", "func_name": "_make_type_verifier", "language": "python"}, {"code": "def to_arrow_type(dt):\n    \"\"\" Convert Spark data type to pyarrow type\n    \"\"\"\n    import pyarrow as pa\n    if type(dt) == BooleanType:\n        arrow_type = pa.bool_()\n    elif type(dt) == ByteType:\n        arrow_type = pa.int8()\n    elif type(dt) == ShortType:\n        arrow_type = pa.int16()\n    elif type(dt) == IntegerType:\n        arrow_type = pa.int32()\n    elif type(dt) == LongType:\n        arrow_type = pa.int64()\n    elif type(dt) == FloatType:\n        arrow_type = pa.float32()\n    elif type(dt) == DoubleType:\n        arrow_type = pa.float64()\n    elif type(dt) == DecimalType:\n        arrow_type = pa.decimal128(dt.precision, dt.scale)\n    elif type(dt) == StringType:\n        arrow_type = pa.string()\n    elif type(dt) == BinaryType:\n        arrow_type = pa.binary()\n    elif type(dt) == DateType:\n        arrow_type = pa.date32()\n    elif type(dt) == TimestampType:\n        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read\n        arrow_type = pa.timestamp('us', tz='UTC')\n    elif type(dt) == ArrayType:\n        if type(dt.elementType) in [StructType, TimestampType]:\n            raise TypeError(\"Unsupported type in conversion to Arrow: \" + str(dt))\n        arrow_type = pa.list_(to_arrow_type(dt.elementType))\n    elif type(dt) == StructType:\n        if any(type(field.dataType) == StructType for field in dt):\n            raise TypeError(\"Nested StructType not supported in conversion to Arrow\")\n        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)\n                  for field in dt]\n        arrow_type = pa.struct(fields)\n    else:\n        raise TypeError(\"Unsupported type in conversion to Arrow: \" + str(dt))\n    return arrow_type", "docstring": "Convert Spark data type to pyarrow type", "func_name": "to_arrow_type", "language": "python"}, {"code": "def to_arrow_schema(schema):\n    \"\"\" Convert a schema from Spark to Arrow\n    \"\"\"\n    import pyarrow as pa\n    fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)\n              for field in schema]\n    return pa.schema(fields)", "docstring": "Convert a schema from Spark to Arrow", "func_name": "to_arrow_schema", "language": "python"}, {"code": "def from_arrow_type(at):\n    \"\"\" Convert pyarrow type to Spark data type.\n    \"\"\"\n    import pyarrow.types as types\n    if types.is_boolean(at):\n        spark_type = BooleanType()\n    elif types.is_int8(at):\n        spark_type = ByteType()\n    elif types.is_int16(at):\n        spark_type = ShortType()\n    elif types.is_int32(at):\n        spark_type = IntegerType()\n    elif types.is_int64(at):\n        spark_type = LongType()\n    elif types.is_float32(at):\n        spark_type = FloatType()\n    elif types.is_float64(at):\n        spark_type = DoubleType()\n    elif types.is_decimal(at):\n        spark_type = DecimalType(precision=at.precision, scale=at.scale)\n    elif types.is_string(at):\n        spark_type = StringType()\n    elif types.is_binary(at):\n        spark_type = BinaryType()\n    elif types.is_date32(at):\n        spark_type = DateType()\n    elif types.is_timestamp(at):\n        spark_type = TimestampType()\n    elif types.is_list(at):\n        if types.is_timestamp(at.value_type):\n            raise TypeError(\"Unsupported type in conversion from Arrow: \" + str(at))\n        spark_type = ArrayType(from_arrow_type(at.value_type))\n    elif types.is_struct(at):\n        if any(types.is_struct(field.type) for field in at):\n            raise TypeError(\"Nested StructType not supported in conversion from Arrow: \" + str(at))\n        return StructType(\n            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)\n             for field in at])\n    else:\n        raise TypeError(\"Unsupported type in conversion from Arrow: \" + str(at))\n    return spark_type", "docstring": "Convert pyarrow type to Spark data type.", "func_name": "from_arrow_type", "language": "python"}, {"code": "def from_arrow_schema(arrow_schema):\n    \"\"\" Convert schema from Arrow to Spark.\n    \"\"\"\n    return StructType(\n        [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)\n         for field in arrow_schema])", "docstring": "Convert schema from Arrow to Spark.", "func_name": "from_arrow_schema", "language": "python"}, {"code": "def _check_series_localize_timestamps(s, timezone):\n    \"\"\"\n    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.\n\n    If the input series is not a timestamp series, then the same series is returned. If the input\n    series is a timestamp series, then a converted series is returned.\n\n    :param s: pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series that have been converted to tz-naive\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    from pandas.api.types import is_datetime64tz_dtype\n    tz = timezone or _get_local_timezone()\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert(tz).dt.tz_localize(None)\n    else:\n        return s", "docstring": "Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.\n\n    If the input series is not a timestamp series, then the same series is returned. If the input\n    series is a timestamp series, then a converted series is returned.\n\n    :param s: pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series that have been converted to tz-naive", "func_name": "_check_series_localize_timestamps", "language": "python"}, {"code": "def _check_dataframe_localize_timestamps(pdf, timezone):\n    \"\"\"\n    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone\n\n    :param pdf: pandas.DataFrame\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    for column, series in pdf.iteritems():\n        pdf[column] = _check_series_localize_timestamps(series, timezone)\n    return pdf", "docstring": "Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone\n\n    :param pdf: pandas.DataFrame\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive", "func_name": "_check_dataframe_localize_timestamps", "language": "python"}, {"code": "def _check_series_convert_timestamps_internal(s, timezone):\n    \"\"\"\n    Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for\n    Spark internal storage\n\n    :param s: a pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64_dtype(s.dtype):\n        # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive\n        # timestamp is during the hour when the clock is adjusted backward during due to\n        # daylight saving time (dst).\n        # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to\n        # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize\n        # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either\n        # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).\n        #\n        # Here we explicit choose to use standard time. This matches the default behavior of\n        # pytz.\n        #\n        # Here are some code to help understand this behavior:\n        # >>> import datetime\n        # >>> import pandas as pd\n        # >>> import pytz\n        # >>>\n        # >>> t = datetime.datetime(2015, 11, 1, 1, 30)\n        # >>> ts = pd.Series([t])\n        # >>> tz = pytz.timezone('America/New_York')\n        # >>>\n        # >>> ts.dt.tz_localize(tz, ambiguous=True)\n        # 0   2015-11-01 01:30:00-04:00\n        # dtype: datetime64[ns, America/New_York]\n        # >>>\n        # >>> ts.dt.tz_localize(tz, ambiguous=False)\n        # 0   2015-11-01 01:30:00-05:00\n        # dtype: dateti", "docstring": "Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for\n    Spark internal storage\n\n    :param s: a pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone", "func_name": "_check_series_convert_timestamps_internal", "language": "python"}, {"code": "def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):\n    \"\"\"\n    Convert timestamp to timezone-naive in the specified timezone or local timezone\n\n    :param s: a pandas.Series\n    :param from_timezone: the timezone to convert from. if None then use local timezone\n    :param to_timezone: the timezone to convert to. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been converted to tz-naive\n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    import pandas as pd\n    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype\n    from_tz = from_timezone or _get_local_timezone()\n    to_tz = to_timezone or _get_local_timezone()\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert(to_tz).dt.tz_localize(None)\n    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:\n        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.\n        return s.apply(\n            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)\n            if ts is not pd.NaT else pd.NaT)\n    else:\n        return s", "docstring": "Convert timestamp to timezone-naive in the specified timezone or local timezone\n\n    :param s: a pandas.Series\n    :param from_timezone: the timezone to convert from. if None then use local timezone\n    :param to_timezone: the timezone to convert to. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been converted to tz-naive", "func_name": "_check_series_convert_timestamps_localize", "language": "python"}, {"code": "def add(self, field, data_type=None, nullable=True, metadata=None):\n        \"\"\"\n        Construct a StructType by adding new elements to it to define the schema. The method accepts\n        either:\n\n            a) A single parameter which is a StructField object.\n            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n               metadata(optional). The data_type parameter may be either a String or a\n               DataType object.\n\n        >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\\\n        ...     StructField(\"f2\", StringType(), True, None)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add(\"f1\", \"string\", True)\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n        >>> struct1 == struct2\n        True\n\n        :param field: Either the name of the field or a StructField object\n        :param data_type: If present, the DataType of the StructField to create\n        :param nullable: Whether the field to add should be nullable (default True)\n        :param metadata: Any additional metadata (default None)\n        :return: a new updated StructType\n        \"\"\"\n        if isinstance(field, StructField):\n            self.fields.append(field)\n            self.names.append(field.name)\n        else:\n            if isinstance(field, str) and data_type is None:\n                raise ValueError(\"Must specify DataType if passing name of struct_field to create.\")\n\n            if isinstance(data_type, str):\n                data_type_f = _parse_datatype_json_value(data_type)\n            else:\n                data_type_f = data_type\n            self.fields.append(StructField(field", "docstring": "Construct a StructType by adding new elements to it to define the schema. The method accepts\n        either:\n\n            a) A single parameter which is a StructField object.\n            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n               metadata(optional). The data_type parameter may be either a String or a\n               DataType object.\n\n        >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n        >>> struct2 ", "func_name": "StructType.add", "language": "python"}, {"code": "def _cachedSqlType(cls):\n        \"\"\"\n        Cache the sqlType() into class, because it's heavy used in `toInternal`.\n        \"\"\"\n        if not hasattr(cls, \"_cached_sql_type\"):\n            cls._cached_sql_type = cls.sqlType()\n        return cls._cached_sql_type", "docstring": "Cache the sqlType() into class, because it's heavy used in `toInternal`.", "func_name": "UserDefinedType._cachedSqlType", "language": "python"}, {"code": "def asDict(self, recursive=False):\n        \"\"\"\n        Return as an dict\n\n        :param recursive: turns the nested Row as dict (default: False).\n\n        >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n        True\n        >>> row = Row(key=1, value=Row(name='a', age=2))\n        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n        True\n        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n        True\n        \"\"\"\n        if not hasattr(self, \"__fields__\"):\n            raise TypeError(\"Cannot convert a Row class into dict\")\n\n        if recursive:\n            def conv(obj):\n                if isinstance(obj, Row):\n                    return obj.asDict(True)\n                elif isinstance(obj, list):\n                    return [conv(o) for o in obj]\n                elif isinstance(obj, dict):\n                    return dict((k, conv(v)) for k, v in obj.items())\n                else:\n                    return obj\n            return dict(zip(self.__fields__, (conv(o) for o in self)))\n        else:\n            return dict(zip(self.__fields__, self))", "docstring": "Return as an dict\n\n        :param recursive: turns the nested Row as dict (default: False).\n\n        >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n        True\n        >>> row = Row(key=1, value=Row(name='a', age=2))\n        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n        True\n        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n        True", "func_name": "Row.asDict", "language": "python"}, {"code": "def summary(self):\n        \"\"\"\n        Gets summary (e.g. residuals, mse, r-squared ) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.\n        \"\"\"\n        if self.hasSummary:\n            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "docstring": "Gets summary (e.g. residuals, mse, r-squared ) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.", "func_name": "LinearRegressionModel.summary", "language": "python"}, {"code": "def evaluate(self, dataset):\n        \"\"\"\n        Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`\n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise ValueError(\"dataset must be a DataFrame but got %s.\" % type(dataset))\n        java_lr_summary = self._call_java(\"evaluate\", dataset)\n        return LinearRegressionSummary(java_lr_summary)", "docstring": "Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`", "func_name": "LinearRegressionModel.evaluate", "language": "python"}, {"code": "def summary(self):\n        \"\"\"\n        Gets summary (e.g. residuals, deviance, pValues) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.\n        \"\"\"\n        if self.hasSummary:\n            return GeneralizedLinearRegressionTrainingSummary(\n                super(GeneralizedLinearRegressionModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "docstring": "Gets summary (e.g. residuals, deviance, pValues) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.", "func_name": "GeneralizedLinearRegressionModel.summary", "language": "python"}, {"code": "def evaluate(self, dataset):\n        \"\"\"\n        Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`\n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise ValueError(\"dataset must be a DataFrame but got %s.\" % type(dataset))\n        java_glr_summary = self._call_java(\"evaluate\", dataset)\n        return GeneralizedLinearRegressionSummary(java_glr_summary)", "docstring": "Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`", "func_name": "GeneralizedLinearRegressionModel.evaluate", "language": "python"}, {"code": "def _get_local_dirs(sub):\n    \"\"\" Get all the directories \"\"\"\n    path = os.environ.get(\"SPARK_LOCAL_DIRS\", \"/tmp\")\n    dirs = path.split(\",\")\n    if len(dirs) > 1:\n        # different order in different processes and instances\n        rnd = random.Random(os.getpid() + id(dirs))\n        random.shuffle(dirs, rnd.random)\n    return [os.path.join(d, \"python\", str(os.getpid()), sub) for d in dirs]", "docstring": "Get all the directories", "func_name": "_get_local_dirs", "language": "python"}, {"code": "def _get_spill_dir(self, n):\n        \"\"\" Choose one directory for spill by number n \"\"\"\n        return os.path.join(self.localdirs[n % len(self.localdirs)], str(n))", "docstring": "Choose one directory for spill by number n", "func_name": "ExternalMerger._get_spill_dir", "language": "python"}, {"code": "def mergeValues(self, iterator):\n        \"\"\" Combine the items by creator and combiner \"\"\"\n        # speedup attribute lookup\n        creator, comb = self.agg.createCombiner, self.agg.mergeValue\n        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch\n        limit = self.memory_limit\n\n        for k, v in iterator:\n            d = pdata[hfun(k)] if pdata else data\n            d[k] = comb(d[k], v) if k in d else creator(v)\n\n            c += 1\n            if c >= batch:\n                if get_used_memory() >= limit:\n                    self._spill()\n                    limit = self._next_limit()\n                    batch /= 2\n                    c = 0\n                else:\n                    batch *= 1.5\n\n        if get_used_memory() >= limit:\n            self._spill()", "docstring": "Combine the items by creator and combiner", "func_name": "ExternalMerger.mergeValues", "language": "python"}, {"code": "def mergeCombiners(self, iterator, limit=None):\n        \"\"\" Merge (K,V) pair by mergeCombiner \"\"\"\n        if limit is None:\n            limit = self.memory_limit\n        # speedup attribute lookup\n        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size\n        c, data, pdata, batch = 0, self.data, self.pdata, self.batch\n        for k, v in iterator:\n            d = pdata[hfun(k)] if pdata else data\n            d[k] = comb(d[k], v) if k in d else v\n            if not limit:\n                continue\n\n            c += objsize(v)\n            if c > batch:\n                if get_used_memory() > limit:\n                    self._spill()\n                    limit = self._next_limit()\n                    batch /= 2\n                    c = 0\n                else:\n                    batch *= 1.5\n\n        if limit and get_used_memory() >= limit:\n            self._spill()", "docstring": "Merge (K,V) pair by mergeCombiner", "func_name": "ExternalMerger.mergeCombiners", "language": "python"}, {"code": "def _spill(self):\n        \"\"\"\n        dump already partitioned data into disks.\n\n        It will dump the data in batch for better performance.\n        \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        path = self._get_spill_dir(self.spills)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        used_memory = get_used_memory()\n        if not self.pdata:\n            # The data has not been partitioned, it will iterator the\n            # dataset once, write them into different files, has no\n            # additional memory. It only called when the memory goes\n            # above limit at the first time.\n\n            # open all the files for writing\n            streams = [open(os.path.join(path, str(i)), 'wb')\n                       for i in range(self.partitions)]\n\n            for k, v in self.data.items():\n                h = self._partition(k)\n                # put one item in batch, make it compatible with load_stream\n                # it will increase the memory if dump them in batch\n                self.serializer.dump_stream([(k, v)], streams[h])\n\n            for s in streams:\n                DiskBytesSpilled += s.tell()\n                s.close()\n\n            self.data.clear()\n            self.pdata.extend([{} for i in range(self.partitions)])\n\n        else:\n            for i in range(self.partitions):\n                p = os.path.join(path, str(i))\n                with open(p, \"wb\") as f:\n                    # dump items in batch\n                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)\n                self.pdata[i].clear()\n                DiskBytesSpilled += os.path.getsize(p)\n\n        self.spills += 1\n        gc.collect()  # release the memory as much as possible\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20", "docstring": "dump already partitioned data into disks.\n\n        It will dump the data in batch for better performance.", "func_name": "ExternalMerger._spill", "language": "python"}, {"code": "def items(self):\n        \"\"\" Return all merged items as iterator \"\"\"\n        if not self.pdata and not self.spills:\n            return iter(self.data.items())\n        return self._external_items()", "docstring": "Return all merged items as iterator", "func_name": "ExternalMerger.items", "language": "python"}, {"code": "def _external_items(self):\n        \"\"\" Return all partitioned items as iterator \"\"\"\n        assert not self.data\n        if any(self.pdata):\n            self._spill()\n        # disable partitioning and spilling when merge combiners from disk\n        self.pdata = []\n\n        try:\n            for i in range(self.partitions):\n                for v in self._merged_items(i):\n                    yield v\n                self.data.clear()\n\n                # remove the merged partition\n                for j in range(self.spills):\n                    path = self._get_spill_dir(j)\n                    os.remove(os.path.join(path, str(i)))\n        finally:\n            self._cleanup()", "docstring": "Return all partitioned items as iterator", "func_name": "ExternalMerger._external_items", "language": "python"}, {"code": "def _recursive_merged_items(self, index):\n        \"\"\"\n        merge the partitioned items and return the as iterator\n\n        If one partition can not be fit in memory, then them will be\n        partitioned and merged recursively.\n        \"\"\"\n        subdirs = [os.path.join(d, \"parts\", str(index)) for d in self.localdirs]\n        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,\n                           self.scale * self.partitions, self.partitions, self.batch)\n        m.pdata = [{} for _ in range(self.partitions)]\n        limit = self._next_limit()\n\n        for j in range(self.spills):\n            path = self._get_spill_dir(j)\n            p = os.path.join(path, str(index))\n            with open(p, 'rb') as f:\n                m.mergeCombiners(self.serializer.load_stream(f), 0)\n\n            if get_used_memory() > limit:\n                m._spill()\n                limit = self._next_limit()\n\n        return m._external_items()", "docstring": "merge the partitioned items and return the as iterator\n\n        If one partition can not be fit in memory, then them will be\n        partitioned and merged recursively.", "func_name": "ExternalMerger._recursive_merged_items", "language": "python"}, {"code": "def _get_path(self, n):\n        \"\"\" Choose one directory for spill by number n \"\"\"\n        d = self.local_dirs[n % len(self.local_dirs)]\n        if not os.path.exists(d):\n            os.makedirs(d)\n        return os.path.join(d, str(n))", "docstring": "Choose one directory for spill by number n", "func_name": "ExternalSorter._get_path", "language": "python"}, {"code": "def sorted(self, iterator, key=None, reverse=False):\n        \"\"\"\n        Sort the elements in iterator, do external sort when the memory\n        goes above the limit.\n        \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        batch, limit = 100, self._next_limit()\n        chunks, current_chunk = [], []\n        iterator = iter(iterator)\n        while True:\n            # pick elements in batch\n            chunk = list(itertools.islice(iterator, batch))\n            current_chunk.extend(chunk)\n            if len(chunk) < batch:\n                break\n\n            used_memory = get_used_memory()\n            if used_memory > limit:\n                # sort them inplace will save memory\n                current_chunk.sort(key=key, reverse=reverse)\n                path = self._get_path(len(chunks))\n                with open(path, 'wb') as f:\n                    self.serializer.dump_stream(current_chunk, f)\n\n                def load(f):\n                    for v in self.serializer.load_stream(f):\n                        yield v\n                    # close the file explicit once we consume all the items\n                    # to avoid ResourceWarning in Python3\n                    f.close()\n                chunks.append(load(open(path, 'rb')))\n                current_chunk = []\n                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20\n                DiskBytesSpilled += os.path.getsize(path)\n                os.unlink(path)  # data will be deleted after close\n\n            elif not chunks:\n                batch = min(int(batch * 1.5), 10000)\n\n        current_chunk.sort(key=key, reverse=reverse)\n        if not chunks:\n            return current_chunk\n\n        if current_chunk:\n            chunks.append(iter(current_chunk))\n\n        return heapq.merge(chunks, key=key, reverse=reverse)", "docstring": "Sort the elements in iterator, do external sort when the memory\n        goes above the limit.", "func_name": "ExternalSorter.sorted", "language": "python"}, {"code": "def _spill(self):\n        \"\"\" dump the values into disk \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        if self._file is None:\n            self._open_file()\n\n        used_memory = get_used_memory()\n        pos = self._file.tell()\n        self._ser.dump_stream(self.values, self._file)\n        self.values = []\n        gc.collect()\n        DiskBytesSpilled += self._file.tell() - pos\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20", "docstring": "dump the values into disk", "func_name": "ExternalList._spill", "language": "python"}, {"code": "def _spill(self):\n        \"\"\"\n        dump already partitioned data into disks.\n        \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        path = self._get_spill_dir(self.spills)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        used_memory = get_used_memory()\n        if not self.pdata:\n            # The data has not been partitioned, it will iterator the\n            # data once, write them into different files, has no\n            # additional memory. It only called when the memory goes\n            # above limit at the first time.\n\n            # open all the files for writing\n            streams = [open(os.path.join(path, str(i)), 'wb')\n                       for i in range(self.partitions)]\n\n            # If the number of keys is small, then the overhead of sort is small\n            # sort them before dumping into disks\n            self._sorted = len(self.data) < self.SORT_KEY_LIMIT\n            if self._sorted:\n                self.serializer = self.flattened_serializer()\n                for k in sorted(self.data.keys()):\n                    h = self._partition(k)\n                    self.serializer.dump_stream([(k, self.data[k])], streams[h])\n            else:\n                for k, v in self.data.items():\n                    h = self._partition(k)\n                    self.serializer.dump_stream([(k, v)], streams[h])\n\n            for s in streams:\n                DiskBytesSpilled += s.tell()\n                s.close()\n\n            self.data.clear()\n            # self.pdata is cached in `mergeValues` and `mergeCombiners`\n            self.pdata.extend([{} for i in range(self.partitions)])\n\n        else:\n            for i in range(self.partitions):\n                p = os.path.join(path, str(i))\n                with open(p, \"wb\") as f:\n                    # dump items in batch\n                    if self._sorted:\n                        # sort by key only (stable)\n                        sorted_items = sorted(self.pdata[i].items()", "docstring": "dump already partitioned data into disks.", "func_name": "ExternalGroupBy._spill", "language": "python"}, {"code": "def _merge_sorted_items(self, index):\n        \"\"\" load a partition from disk, then sort and group by key \"\"\"\n        def load_partition(j):\n            path = self._get_spill_dir(j)\n            p = os.path.join(path, str(index))\n            with open(p, 'rb', 65536) as f:\n                for v in self.serializer.load_stream(f):\n                    yield v\n\n        disk_items = [load_partition(j) for j in range(self.spills)]\n\n        if self._sorted:\n            # all the partitions are already sorted\n            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))\n\n        else:\n            # Flatten the combined values, so it will not consume huge\n            # memory during merging sort.\n            ser = self.flattened_serializer()\n            sorter = ExternalSorter(self.memory_limit, ser)\n            sorted_items = sorter.sorted(itertools.chain(*disk_items),\n                                         key=operator.itemgetter(0))\n        return ((k, vs) for k, vs in GroupByKey(sorted_items))", "docstring": "load a partition from disk, then sort and group by key", "func_name": "ExternalGroupBy._merge_sorted_items", "language": "python"}, {"code": "def worker(sock, authenticated):\n    \"\"\"\n    Called by a worker process after the fork().\n    \"\"\"\n    signal.signal(SIGHUP, SIG_DFL)\n    signal.signal(SIGCHLD, SIG_DFL)\n    signal.signal(SIGTERM, SIG_DFL)\n    # restore the handler for SIGINT,\n    # it's useful for debugging (show the stacktrace before exit)\n    signal.signal(SIGINT, signal.default_int_handler)\n\n    # Read the socket using fdopen instead of socket.makefile() because the latter\n    # seems to be very slow; note that we need to dup() the file descriptor because\n    # otherwise writes also cause a seek that makes us miss data on the read side.\n    infile = os.fdopen(os.dup(sock.fileno()), \"rb\", 65536)\n    outfile = os.fdopen(os.dup(sock.fileno()), \"wb\", 65536)\n\n    if not authenticated:\n        client_secret = UTF8Deserializer().loads(infile)\n        if os.environ[\"PYTHON_WORKER_FACTORY_SECRET\"] == client_secret:\n            write_with_length(\"ok\".encode(\"utf-8\"), outfile)\n            outfile.flush()\n        else:\n            write_with_length(\"err\".encode(\"utf-8\"), outfile)\n            outfile.flush()\n            sock.close()\n            return 1\n\n    exit_code = 0\n    try:\n        worker_main(infile, outfile)\n    except SystemExit as exc:\n        exit_code = compute_real_exit_code(exc.code)\n    finally:\n        try:\n            outfile.flush()\n        except Exception:\n            pass\n    return exit_code", "docstring": "Called by a worker process after the fork().", "func_name": "worker", "language": "python"}, {"code": "def portable_hash(x):\n    \"\"\"\n    This function returns consistent hash code for builtin types, especially\n    for None and tuple with None.\n\n    The algorithm is similar to that one used by CPython 2.7\n\n    >>> portable_hash(None)\n    0\n    >>> portable_hash((None, 1)) & 0xffffffff\n    219750521\n    \"\"\"\n\n    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:\n        raise Exception(\"Randomness of hash of string should be disabled via PYTHONHASHSEED\")\n\n    if x is None:\n        return 0\n    if isinstance(x, tuple):\n        h = 0x345678\n        for i in x:\n            h ^= portable_hash(i)\n            h *= 1000003\n            h &= sys.maxsize\n        h ^= len(x)\n        if h == -1:\n            h = -2\n        return int(h)\n    return hash(x)", "docstring": "This function returns consistent hash code for builtin types, especially\n    for None and tuple with None.\n\n    The algorithm is similar to that one used by CPython 2.7\n\n    >>> portable_hash(None)\n    0\n    >>> portable_hash((None, 1)) & 0xffffffff\n    219750521", "func_name": "portable_hash", "language": "python"}, {"code": "def _parse_memory(s):\n    \"\"\"\n    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and\n    return the value in MiB\n\n    >>> _parse_memory(\"256m\")\n    256\n    >>> _parse_memory(\"2g\")\n    2048\n    \"\"\"\n    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}\n    if s[-1].lower() not in units:\n        raise ValueError(\"invalid format: \" + s)\n    return int(float(s[:-1]) * units[s[-1].lower()])", "docstring": "Parse a memory string in the format supported by Java (e.g. 1g, 200m) and\n    return the value in MiB\n\n    >>> _parse_memory(\"256m\")\n    256\n    >>> _parse_memory(\"2g\")\n    2048", "func_name": "_parse_memory", "language": "python"}, {"code": "def ignore_unicode_prefix(f):\n    \"\"\"\n    Ignore the 'u' prefix of string in doc tests, to make it works\n    in both python 2 and 3\n    \"\"\"\n    if sys.version >= '3':\n        # the representation of unicode string in Python 3 does not have prefix 'u',\n        # so remove the prefix 'u' for doc tests\n        literal_re = re.compile(r\"(\\W|^)[uU](['])\", re.UNICODE)\n        f.__doc__ = literal_re.sub(r'\\1\\2', f.__doc__)\n    return f", "docstring": "Ignore the 'u' prefix of string in doc tests, to make it works\n    in both python 2 and 3", "func_name": "ignore_unicode_prefix", "language": "python"}, {"code": "def cache(self):\n        \"\"\"\n        Persist this RDD with the default storage level (C{MEMORY_ONLY}).\n        \"\"\"\n        self.is_cached = True\n        self.persist(StorageLevel.MEMORY_ONLY)\n        return self", "docstring": "Persist this RDD with the default storage level (C{MEMORY_ONLY}).", "func_name": "RDD.cache", "language": "python"}, {"code": "def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):\n        \"\"\"\n        Set this RDD's storage level to persist its values across operations\n        after the first time it is computed. This can only be used to assign\n        a new storage level if the RDD does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_ONLY}).\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> rdd.persist().is_cached\n        True\n        \"\"\"\n        self.is_cached = True\n        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)\n        self._jrdd.persist(javaStorageLevel)\n        return self", "docstring": "Set this RDD's storage level to persist its values across operations\n        after the first time it is computed. This can only be used to assign\n        a new storage level if the RDD does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_ONLY}).\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> rdd.persist().is_cached\n        True", "func_name": "RDD.persist", "language": "python"}, {"code": "def unpersist(self, blocking=False):\n        \"\"\"\n        Mark the RDD as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. versionchanged:: 3.0.0\n           Added optional argument `blocking` to specify whether to block until all\n           blocks are deleted.\n        \"\"\"\n        self.is_cached = False\n        self._jrdd.unpersist(blocking)\n        return self", "docstring": "Mark the RDD as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. versionchanged:: 3.0.0\n           Added optional argument `blocking` to specify whether to block until all\n           blocks are deleted.", "func_name": "RDD.unpersist", "language": "python"}, {"code": "def getCheckpointFile(self):\n        \"\"\"\n        Gets the name of the file to which this RDD was checkpointed\n\n        Not defined if RDD is checkpointed locally.\n        \"\"\"\n        checkpointFile = self._jrdd.rdd().getCheckpointFile()\n        if checkpointFile.isDefined():\n            return checkpointFile.get()", "docstring": "Gets the name of the file to which this RDD was checkpointed\n\n        Not defined if RDD is checkpointed locally.", "func_name": "RDD.getCheckpointFile", "language": "python"}, {"code": "def map(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD by applying a function to each element of this RDD.\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n        [('a', 1), ('b', 1), ('c', 1)]\n        \"\"\"\n        def func(_, iterator):\n            return map(fail_on_stopiteration(f), iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "docstring": "Return a new RDD by applying a function to each element of this RDD.\n\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n        [('a', 1), ('b', 1), ('c', 1)]", "func_name": "RDD.map", "language": "python"}, {"code": "def flatMap(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD by first applying a function to all elements of this\n        RDD, and then flattening the results.\n\n        >>> rdd = sc.parallelize([2, 3, 4])\n        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n        [1, 1, 1, 2, 2, 3]\n        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n        \"\"\"\n        def func(s, iterator):\n            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "docstring": "Return a new RDD by first applying a function to all elements of this\n        RDD, and then flattening the results.\n\n        >>> rdd = sc.parallelize([2, 3, 4])\n        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n        [1, 1, 1, 2, 2, 3]\n        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]", "func_name": "RDD.flatMap", "language": "python"}, {"code": "def mapPartitions(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]\n        \"\"\"\n        def func(s, iterator):\n            return f(iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "docstring": "Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]", "func_name": "RDD.mapPartitions", "language": "python"}, {"code": "def mapPartitionsWithSplit(self, f, preservesPartitioning=False):\n        \"\"\"\n        Deprecated: use mapPartitionsWithIndex instead.\n\n        Return a new RDD by applying a function to each partition of this RDD,\n        while tracking the index of the original partition.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        >>> rdd.mapPartitionsWithSplit(f).sum()\n        6\n        \"\"\"\n        warnings.warn(\"mapPartitionsWithSplit is deprecated; \"\n                      \"use mapPartitionsWithIndex instead\", DeprecationWarning, stacklevel=2)\n        return self.mapPartitionsWithIndex(f, preservesPartitioning)", "docstring": "Deprecated: use mapPartitionsWithIndex instead.\n\n        Return a new RDD by applying a function to each partition of this RDD,\n        while tracking the index of the original partition.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        >>> rdd.mapPartitionsWithSplit(f).sum()\n        6", "func_name": "RDD.mapPartitionsWithSplit", "language": "python"}, {"code": "def distinct(self, numPartitions=None):\n        \"\"\"\n        Return a new RDD containing the distinct elements in this RDD.\n\n        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n        [1, 2, 3]\n        \"\"\"\n        return self.map(lambda x: (x, None)) \\\n                   .reduceByKey(lambda x, _: x, numPartitions) \\\n                   .map(lambda x: x[0])", "docstring": "Return a new RDD containing the distinct elements in this RDD.\n\n        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n        [1, 2, 3]", "func_name": "RDD.distinct", "language": "python"}, {"code": "def sample(self, withReplacement, fraction, seed=None):\n        \"\"\"\n        Return a sampled subset of this RDD.\n\n        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n        :param fraction: expected size of the sample as a fraction of this RDD's size\n            without replacement: probability that each element is chosen; fraction must be [0, 1]\n            with replacement: expected number of times each element is chosen; fraction must be >= 0\n        :param seed: seed for the random number generator\n\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n            count of the given :class:`DataFrame`.\n\n        >>> rdd = sc.parallelize(range(100), 4)\n        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n        True\n        \"\"\"\n        assert fraction >= 0.0, \"Negative fraction value: %s\" % fraction\n        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)", "docstring": "Return a sampled subset of this RDD.\n\n        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n        :param fraction: expected size of the sample as a fraction of this RDD's size\n            without replacement: probability that each element is chosen; fraction must be [0, 1]\n            with replacement: expected number of times each element is chosen; fraction must be >= 0\n        :param seed: seed for the random number generator\n\n        .. note:: T", "func_name": "RDD.sample", "language": "python"}, {"code": "def randomSplit(self, weights, seed=None):\n        \"\"\"\n        Randomly splits this RDD with the provided weights.\n\n        :param weights: weights for splits, will be normalized if they don't sum to 1\n        :param seed: random seed\n        :return: split RDDs in a list\n\n        >>> rdd = sc.parallelize(range(500), 1)\n        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n        >>> len(rdd1.collect() + rdd2.collect())\n        500\n        >>> 150 < rdd1.count() < 250\n        True\n        >>> 250 < rdd2.count() < 350\n        True\n        \"\"\"\n        s = float(sum(weights))\n        cweights = [0.0]\n        for w in weights:\n            cweights.append(cweights[-1] + w / s)\n        if seed is None:\n            seed = random.randint(0, 2 ** 32 - 1)\n        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)\n                for lb, ub in zip(cweights, cweights[1:])]", "docstring": "Randomly splits this RDD with the provided weights.\n\n        :param weights: weights for splits, will be normalized if they don't sum to 1\n        :param seed: random seed\n        :return: split RDDs in a list\n\n        >>> rdd = sc.parallelize(range(500), 1)\n        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n        >>> len(rdd1.collect() + rdd2.collect())\n        500\n        >>> 150 < rdd1.count() < 250\n        True\n        >>> 250 < rdd2.count() < 350\n        True", "func_name": "RDD.randomSplit", "language": "python"}, {"code": "def takeSample(self, withReplacement, num, seed=None):\n        \"\"\"\n        Return a fixed-size sampled subset of this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> rdd = sc.parallelize(range(0, 10))\n        >>> len(rdd.takeSample(True, 20, 1))\n        20\n        >>> len(rdd.takeSample(False, 5, 2))\n        5\n        >>> len(rdd.takeSample(False, 15, 3))\n        10\n        \"\"\"\n        numStDev = 10.0\n\n        if num < 0:\n            raise ValueError(\"Sample size cannot be negative.\")\n        elif num == 0:\n            return []\n\n        initialCount = self.count()\n        if initialCount == 0:\n            return []\n\n        rand = random.Random(seed)\n\n        if (not withReplacement) and num >= initialCount:\n            # shuffle current RDD and return\n            samples = self.collect()\n            rand.shuffle(samples)\n            return samples\n\n        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))\n        if num > maxSampleSize:\n            raise ValueError(\n                \"Sample size cannot be greater than %d.\" % maxSampleSize)\n\n        fraction = RDD._computeFractionForSampleSize(\n            num, initialCount, withReplacement)\n        samples = self.sample(withReplacement, fraction, seed).collect()\n\n        # If the first sample didn't turn out large enough, keep trying to take samples;\n        # this shouldn't happen often because we use a big multiplier for their initial size.\n        # See: scala/spark/RDD.scala\n        while len(samples) < num:\n            # TODO: add log warning for when more than one iteration was run\n            seed = rand.randint(0, sys.maxsize)\n            samples = self.sample(withReplacement, fraction, seed).collect()\n\n        rand.shuffle(samples)\n\n        return samples[0:num]", "docstring": "Return a fixed-size sampled subset of this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> rdd = sc.parallelize(range(0, 10))\n        >>> len(rdd.takeSample(True, 20, 1))\n        20\n        >>> len(rdd.takeSample(False, 5, 2))\n        5\n        >>> len(rdd.takeSample(False, 15, 3))\n        10", "func_name": "RDD.takeSample", "language": "python"}, {"code": "def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):\n        \"\"\"\n        Returns a sampling rate that guarantees a sample of\n        size >= sampleSizeLowerBound 99.99% of the time.\n\n        How the sampling rate is determined:\n        Let p = num / total, where num is the sample size and total is the\n        total number of data points in the RDD. We're trying to compute\n        q > p such that\n          - when sampling with replacement, we're drawing each data point\n            with prob_i ~ Pois(q), where we want to guarantee\n            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\n            total), i.e. the failure rate of not having a sufficiently large\n            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\n            to guarantee 0.9999 success rate for num > 12, but we need a\n            slightly larger q (9 empirically determined).\n          - when sampling without replacement, we're drawing each data point\n            with prob_i ~ Binomial(total, fraction) and our choice of q\n            guarantees 1-delta, or 0.9999 success rate, where success rate is\n            defined the same as in sampling with replacement.\n        \"\"\"\n        fraction = float(sampleSizeLowerBound) / total\n        if withReplacement:\n            numStDev = 5\n            if (sampleSizeLowerBound < 12):\n                numStDev = 9\n            return fraction + numStDev * sqrt(fraction / total)\n        else:\n            delta = 0.00005\n            gamma = - log(delta) / total\n            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))", "docstring": "Returns a sampling rate that guarantees a sample of\n        size >= sampleSizeLowerBound 99.99% of the time.\n\n        How the sampling rate is determined:\n        Let p = num / total, where num is the sample size and total is the\n        total number of data points in the RDD. We're trying to compute\n        q > p such that\n          - when sampling with replacement, we're drawing each data point\n            with prob_i ~ Pois(q), where we want to guarantee\n            Pr[s < num] < 0.0001 for s", "func_name": "RDD._computeFractionForSampleSize", "language": "python"}, {"code": "def union(self, other):\n        \"\"\"\n        Return the union of this RDD and another one.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> rdd.union(rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]\n        \"\"\"\n        if self._jrdd_deserializer == other._jrdd_deserializer:\n            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,\n                      self._jrdd_deserializer)\n        else:\n            # These RDDs contain data in different serialized formats, so we\n            # must normalize them to the default serializer.\n            self_copy = self._reserialize()\n            other_copy = other._reserialize()\n            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,\n                      self.ctx.serializer)\n        if (self.partitioner == other.partitioner and\n                self.getNumPartitions() == rdd.getNumPartitions()):\n            rdd.partitioner = self.partitioner\n        return rdd", "docstring": "Return the union of this RDD and another one.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> rdd.union(rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]", "func_name": "RDD.union", "language": "python"}, {"code": "def intersection(self, other):\n        \"\"\"\n        Return the intersection of this RDD and another one. The output will\n        not contain any duplicate elements, even if the input RDDs did.\n\n        .. note:: This method performs a shuffle internally.\n\n        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n        >>> rdd1.intersection(rdd2).collect()\n        [1, 2, 3]\n        \"\"\"\n        return self.map(lambda v: (v, None)) \\\n            .cogroup(other.map(lambda v: (v, None))) \\\n            .filter(lambda k_vs: all(k_vs[1])) \\\n            .keys()", "docstring": "Return the intersection of this RDD and another one. The output will\n        not contain any duplicate elements, even if the input RDDs did.\n\n        .. note:: This method performs a shuffle internally.\n\n        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n        >>> rdd1.intersection(rdd2).collect()\n        [1, 2, 3]", "func_name": "RDD.intersection", "language": "python"}, {"code": "def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,\n                                           ascending=True, keyfunc=lambda x: x):\n        \"\"\"\n        Repartition the RDD according to the given partitioner and, within each resulting partition,\n        sort records by their keys.\n\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n        >>> rdd2.glom().collect()\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        memory = _parse_memory(self.ctx._conf.get(\"spark.python.worker.memory\", \"512m\"))\n        serializer = self._jrdd_deserializer\n\n        def sortPartition(iterator):\n            sort = ExternalSorter(memory * 0.9, serializer).sorted\n            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))\n\n        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)", "docstring": "Repartition the RDD according to the given partitioner and, within each resulting partition,\n        sort records by their keys.\n\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n        >>> rdd2.glom().collect()\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]", "func_name": "RDD.repartitionAndSortWithinPartitions", "language": "python"}, {"code": "def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n        \"\"\"\n        Sorts this RDD, which is assumed to consist of (key, value) pairs.\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortByKey().first()\n        ('1', 3)\n        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        memory = self._memory_limit()\n        serializer = self._jrdd_deserializer\n\n        def sortPartition(iterator):\n            sort = ExternalSorter(memory * 0.9, serializer).sorted\n            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n\n        if numPartitions == 1:\n            if self.getNumPartitions() > 1:\n                self = self.coalesce(1)\n            return self.mapPartitions(sortPartition, True)\n\n        # first compute the boundary of each part via sampling: we want to partition\n        # the key-space into bins such that the bins have roughly the same\n        # number of (key, value) pairs falling into them\n        rddSize = self.count()\n        if not rddSize:\n            return self  # empty RDD\n        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner\n        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)\n        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()\n      ", "docstring": "Sorts this RDD, which is assumed to consist of (key, value) pairs.\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortByKey().first()\n        ('1', 3)\n        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3),", "func_name": "RDD.sortByKey", "language": "python"}, {"code": "def sortBy(self, keyfunc, ascending=True, numPartitions=None):\n        \"\"\"\n        Sorts this RDD by the given keyfunc\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        \"\"\"\n        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()", "docstring": "Sorts this RDD by the given keyfunc\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]", "func_name": "RDD.sortBy", "language": "python"}, {"code": "def cartesian(self, other):\n        \"\"\"\n        Return the Cartesian product of this RDD and another one, that is, the\n        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n        C{b} is in C{other}.\n\n        >>> rdd = sc.parallelize([1, 2])\n        >>> sorted(rdd.cartesian(rdd).collect())\n        [(1, 1), (1, 2), (2, 1), (2, 2)]\n        \"\"\"\n        # Due to batching, we can't use the Java cartesian method.\n        deserializer = CartesianDeserializer(self._jrdd_deserializer,\n                                             other._jrdd_deserializer)\n        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)", "docstring": "Return the Cartesian product of this RDD and another one, that is, the\n        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n        C{b} is in C{other}.\n\n        >>> rdd = sc.parallelize([1, 2])\n        >>> sorted(rdd.cartesian(rdd).collect())\n        [(1, 1), (1, 2), (2, 1), (2, 2)]", "func_name": "RDD.cartesian", "language": "python"}, {"code": "def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Return an RDD of grouped items.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n        >>> sorted([(x, sorted(y)) for (x, y) in result])\n        [(0, [2, 8]), (1, [1, 1, 3, 5])]\n        \"\"\"\n        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)", "docstring": "Return an RDD of grouped items.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n        >>> sorted([(x, sorted(y)) for (x, y) in result])\n        [(0, [2, 8]), (1, [1, 1, 3, 5])]", "func_name": "RDD.groupBy", "language": "python"}, {"code": "def pipe(self, command, env=None, checkCode=False):\n        \"\"\"\n        Return an RDD created by piping elements to a forked external process.\n\n        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n        [u'1', u'2', u'', u'3']\n\n        :param checkCode: whether or not to check the return value of the shell command.\n        \"\"\"\n        if env is None:\n            env = dict()\n\n        def func(iterator):\n            pipe = Popen(\n                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n\n            def pipe_objs(out):\n                for obj in iterator:\n                    s = unicode(obj).rstrip('\\n') + '\\n'\n                    out.write(s.encode('utf-8'))\n                out.close()\n            Thread(target=pipe_objs, args=[pipe.stdin]).start()\n\n            def check_return_code():\n                pipe.wait()\n                if checkCode and pipe.returncode:\n                    raise Exception(\"Pipe function `%s' exited \"\n                                    \"with error code %d\" % (command, pipe.returncode))\n                else:\n                    for i in range(0):\n                        yield i\n            return (x.rstrip(b'\\n').decode('utf-8') for x in\n                    chain(iter(pipe.stdout.readline, b''), check_return_code()))\n        return self.mapPartitions(func)", "docstring": "Return an RDD created by piping elements to a forked external process.\n\n        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n        [u'1', u'2', u'', u'3']\n\n        :param checkCode: whether or not to check the return value of the shell command.", "func_name": "RDD.pipe", "language": "python"}, {"code": "def foreach(self, f):\n        \"\"\"\n        Applies a function to all elements of this RDD.\n\n        >>> def f(x): print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n        \"\"\"\n        f = fail_on_stopiteration(f)\n\n        def processPartition(iterator):\n            for x in iterator:\n                f(x)\n            return iter([])\n        self.mapPartitions(processPartition).count()", "docstring": "Applies a function to all elements of this RDD.\n\n        >>> def f(x): print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)", "func_name": "RDD.foreach", "language": "python"}, {"code": "def foreachPartition(self, f):\n        \"\"\"\n        Applies a function to each partition of this RDD.\n\n        >>> def f(iterator):\n        ...     for x in iterator:\n        ...          print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n        \"\"\"\n        def func(it):\n            r = f(it)\n            try:\n                return iter(r)\n            except TypeError:\n                return iter([])\n        self.mapPartitions(func).count()", "docstring": "Applies a function to each partition of this RDD.\n\n        >>> def f(iterator):\n        ...     for x in iterator:\n        ...          print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)", "func_name": "RDD.foreachPartition", "language": "python"}, {"code": "def collect(self):\n        \"\"\"\n        Return a list that contains all of the elements in this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n        \"\"\"\n        with SCCallSiteSync(self.context) as css:\n            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n        return list(_load_from_socket(sock_info, self._jrdd_deserializer))", "docstring": "Return a list that contains all of the elements in this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.", "func_name": "RDD.collect", "language": "python"}, {"code": "def reduce(self, f):\n        \"\"\"\n        Reduces the elements of this RDD using the specified commutative and\n        associative binary operator. Currently reduces partitions locally.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n        15\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n        10\n        >>> sc.parallelize([]).reduce(add)\n        Traceback (most recent call last):\n            ...\n        ValueError: Can not reduce() empty RDD\n        \"\"\"\n        f = fail_on_stopiteration(f)\n\n        def func(iterator):\n            iterator = iter(iterator)\n            try:\n                initial = next(iterator)\n            except StopIteration:\n                return\n            yield reduce(f, iterator, initial)\n\n        vals = self.mapPartitions(func).collect()\n        if vals:\n            return reduce(f, vals)\n        raise ValueError(\"Can not reduce() empty RDD\")", "docstring": "Reduces the elements of this RDD using the specified commutative and\n        associative binary operator. Currently reduces partitions locally.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n        15\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n        10\n        >>> sc.parallelize([]).reduce(add)\n        Traceback (most recent call last):\n            ...\n        ValueError: Can not reduce() empty RDD", "func_name": "RDD.reduce", "language": "python"}, {"code": "def treeReduce(self, f, depth=2):\n        \"\"\"\n        Reduces the elements of this RDD in a multi-level tree pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeReduce(add)\n        -5\n        >>> rdd.treeReduce(add, 1)\n        -5\n        >>> rdd.treeReduce(add, 2)\n        -5\n        >>> rdd.treeReduce(add, 5)\n        -5\n        >>> rdd.treeReduce(add, 10)\n        -5\n        \"\"\"\n        if depth < 1:\n            raise ValueError(\"Depth cannot be smaller than 1 but got %d.\" % depth)\n\n        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.\n\n        def op(x, y):\n            if x[1]:\n                return y\n            elif y[1]:\n                return x\n            else:\n                return f(x[0], y[0]), False\n\n        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)\n        if reduced[1]:\n            raise ValueError(\"Cannot reduce empty RDD.\")\n        return reduced[0]", "docstring": "Reduces the elements of this RDD in a multi-level tree pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeReduce(add)\n        -5\n        >>> rdd.treeReduce(add, 1)\n        -5\n        >>> rdd.treeReduce(add, 2)\n        -5\n        >>> rdd.treeReduce(add, 5)\n        -5\n        >>> rdd.treeReduce(add, 10)\n        -5", "func_name": "RDD.treeReduce", "language": "python"}, {"code": "def fold(self, zeroValue, op):\n        \"\"\"\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given associative function and a neutral \"zero value.\"\n\n        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        This behaves somewhat differently from fold operations implemented\n        for non-distributed collections in functional languages like Scala.\n        This fold operation may be applied to partitions individually, and then\n        fold those results into the final result, rather than apply the fold\n        to each element sequentially in some defined ordering. For functions\n        that are not commutative, the result may differ from that of a fold\n        applied to a non-distributed collection.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n        15\n        \"\"\"\n        op = fail_on_stopiteration(op)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = op(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(op, vals, zeroValue)", "docstring": "Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given associative function and a neutral \"zero value.\"\n\n        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        This behaves somewhat differently from fold operations implemented\n        for non-distributed collections in functional languages like Scala.\n        This", "func_name": "RDD.fold", "language": "python"}, {"code": "def aggregate(self, zeroValue, seqOp, combOp):\n        \"\"\"\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given combine functions and a neutral \"zero\n        value.\"\n\n        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        The first function (seqOp) can return a different result type, U, than\n        the type of this RDD. Thus, we need one operation for merging a T into\n        an U and one operation for merging two U\n\n        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n        (10, 4)\n        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n        (0, 0)\n        \"\"\"\n        seqOp = fail_on_stopiteration(seqOp)\n        combOp = fail_on_stopiteration(combOp)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = seqOp(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(combOp, vals, zeroValue)", "docstring": "Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given combine functions and a neutral \"zero\n        value.\"\n\n        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        The first function (seqOp) can return a different result type, U, than\n        the type of this RDD. Thus, we need one operation for merging a T into", "func_name": "RDD.aggregate", "language": "python"}, {"code": "def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):\n        \"\"\"\n        Aggregates the elements of this RDD in a multi-level tree\n        pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeAggregate(0, add, add)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 1)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 2)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 5)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 10)\n        -5\n        \"\"\"\n        if depth < 1:\n            raise ValueError(\"Depth cannot be smaller than 1 but got %d.\" % depth)\n\n        if self.getNumPartitions() == 0:\n            return zeroValue\n\n        def aggregatePartition(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = seqOp(acc, obj)\n            yield acc\n\n        partiallyAggregated = self.mapPartitions(aggregatePartition)\n        numPartitions = partiallyAggregated.getNumPartitions()\n        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)\n        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree\n        # aggregation.\n        while numPartitions > scale + numPartitions / scale:\n            numPartitions /= scale\n            curNumPartitions = int(numPartitions)\n\n            def mapPartition(i, iterator):\n                for obj in iterator:\n                    yield (i % curNumPartitions, obj)\n\n            partiallyAggregated = partiallyAggregated \\\n                .mapPartitionsWithIndex(mapPartition) \\\n                .reduceByKey(combOp, curNumPartitions) \\\n                .values()\n\n        return partiallyAggregated.reduce(combOp)", "docstring": "Aggregates the elements of this RDD in a multi-level tree\n        pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeAggregate(0, add, add)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 1)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 2)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 5)\n        -5\n        >>> rdd.treeAggrega", "func_name": "RDD.treeAggregate", "language": "python"}, {"code": "def max(self, key=None):\n        \"\"\"\n        Find the maximum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n        >>> rdd.max()\n        43.0\n        >>> rdd.max(key=str)\n        5.0\n        \"\"\"\n        if key is None:\n            return self.reduce(max)\n        return self.reduce(lambda a, b: max(a, b, key=key))", "docstring": "Find the maximum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n        >>> rdd.max()\n        43.0\n        >>> rdd.max(key=str)\n        5.0", "func_name": "RDD.max", "language": "python"}, {"code": "def min(self, key=None):\n        \"\"\"\n        Find the minimum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n        >>> rdd.min()\n        2.0\n        >>> rdd.min(key=str)\n        10.0\n        \"\"\"\n        if key is None:\n            return self.reduce(min)\n        return self.reduce(lambda a, b: min(a, b, key=key))", "docstring": "Find the minimum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n        >>> rdd.min()\n        2.0\n        >>> rdd.min(key=str)\n        10.0", "func_name": "RDD.min", "language": "python"}, {"code": "def sum(self):\n        \"\"\"\n        Add up the elements in this RDD.\n\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n        6.0\n        \"\"\"\n        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)", "docstring": "Add up the elements in this RDD.\n\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n        6.0", "func_name": "RDD.sum", "language": "python"}, {"code": "def stats(self):\n        \"\"\"\n        Return a L{StatCounter} object that captures the mean, variance\n        and count of the RDD's elements in one operation.\n        \"\"\"\n        def redFunc(left_counter, right_counter):\n            return left_counter.mergeStats(right_counter)\n\n        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)", "docstring": "Return a L{StatCounter} object that captures the mean, variance\n        and count of the RDD's elements in one operation.", "func_name": "RDD.stats", "language": "python"}, {"code": "def histogram(self, buckets):\n        \"\"\"\n        Compute a histogram using the provided buckets. The buckets\n        are all open to the right except for the last which is closed.\n        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n        and 50 we would have a histogram of 1,0,1.\n\n        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n        this can be switched from an O(log n) inseration to O(1) per\n        element (where n is the number of buckets).\n\n        Buckets must be sorted, not contain any duplicates, and have\n        at least two elements.\n\n        If `buckets` is a number, it will generate buckets which are\n        evenly spaced between the minimum and maximum of the RDD. For\n        example, if the min value is 0 and the max is 100, given `buckets`\n        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n        be at least 1. An exception is raised if the RDD contains infinity.\n        If the elements in the RDD do not vary (max == min), a single bucket\n        will be used.\n\n        The return value is a tuple of buckets and histogram.\n\n        >>> rdd = sc.parallelize(range(51))\n        >>> rdd.histogram(2)\n        ([0, 25, 50], [25, 26])\n        >>> rdd.histogram([0, 5, 25, 50])\n        ([0, 5, 25, 50], [5, 20, 26])\n        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n        >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n        >>> rdd.histogram((\"a\", \"b\", \"c\"))\n        (('a', 'b', 'c'), [2, 2])\n        \"\"\"\n\n        if isinstance(buckets, int):\n            if buckets < 1:\n                raise ValueError(\"number of buckets must be >= 1\")\n\n            # filter out non-comparable elements\n            def comparable(x):\n                if x is None:\n                    return False\n                if type(x) is float and isnan(x):\n                    ", "docstring": "Compute a histogram using the provided buckets. The buckets\n        are all open to the right except for the last which is closed.\n        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n        and 50 we would have a histogram of 1,0,1.\n\n        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n        this can be switched from an O(log n) inseration to O(1) per\n        element (where n is the numbe", "func_name": "RDD.histogram", "language": "python"}, {"code": "def countByValue(self):\n        \"\"\"\n        Return the count of each unique value in this RDD as a dictionary of\n        (value, count) pairs.\n\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n        [(1, 2), (2, 3)]\n        \"\"\"\n        def countPartition(iterator):\n            counts = defaultdict(int)\n            for obj in iterator:\n                counts[obj] += 1\n            yield counts\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] += v\n            return m1\n        return self.mapPartitions(countPartition).reduce(mergeMaps)", "docstring": "Return the count of each unique value in this RDD as a dictionary of\n        (value, count) pairs.\n\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n        [(1, 2), (2, 3)]", "func_name": "RDD.countByValue", "language": "python"}, {"code": "def top(self, num, key=None):\n        \"\"\"\n        Get the top N elements from an RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: It returns the list sorted in descending order.\n\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n        [12]\n        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n        [6, 5]\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n        [4, 3, 2]\n        \"\"\"\n        def topIterator(iterator):\n            yield heapq.nlargest(num, iterator, key=key)\n\n        def merge(a, b):\n            return heapq.nlargest(num, a + b, key=key)\n\n        return self.mapPartitions(topIterator).reduce(merge)", "docstring": "Get the top N elements from an RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: It returns the list sorted in descending order.\n\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n        [12]\n        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n        [6, 5]\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n        [4, 3, 2]", "func_name": "RDD.top", "language": "python"}, {"code": "def takeOrdered(self, num, key=None):\n        \"\"\"\n        Get the N elements from an RDD ordered in ascending order or as\n        specified by the optional key function.\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n        [1, 2, 3, 4, 5, 6]\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n        [10, 9, 7, 6, 5, 4]\n        \"\"\"\n\n        def merge(a, b):\n            return heapq.nsmallest(num, a + b, key)\n\n        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)", "docstring": "Get the N elements from an RDD ordered in ascending order or as\n        specified by the optional key function.\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n        [1, 2, 3, 4, 5, 6]\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n        [10, 9, 7, 6, 5, 4]", "func_name": "RDD.takeOrdered", "language": "python"}, {"code": "def take(self, num):\n        \"\"\"\n        Take the first num elements of the RDD.\n\n        It works by first scanning one partition, and use the results from\n        that partition to estimate the number of additional partitions needed\n        to satisfy the limit.\n\n        Translated from the Scala implementation in RDD#take().\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n        [2, 3]\n        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n        [2, 3, 4, 5, 6]\n        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n        [91, 92, 93]\n        \"\"\"\n        items = []\n        totalParts = self.getNumPartitions()\n        partsScanned = 0\n\n        while len(items) < num and partsScanned < totalParts:\n            # The number of partitions to try in this iteration.\n            # It is ok for this number to be greater than totalParts because\n            # we actually cap it at totalParts in runJob.\n            numPartsToTry = 1\n            if partsScanned > 0:\n                # If we didn't find any rows after the previous iteration,\n                # quadruple and retry.  Otherwise, interpolate the number of\n                # partitions we need to try, but overestimate it by 50%.\n                # We also cap the estimation in the end.\n                if len(items) == 0:\n                    numPartsToTry = partsScanned * 4\n                else:\n                    # the first parameter of max is >=1 whenever partsScanned >= 2\n                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned\n                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)\n\n            left = num - len(items)\n\n            def takeUpToNumLeft(iterator):\n                iterator = iter(iterator)\n                taken = 0\n                while taken <", "docstring": "Take the first num elements of the RDD.\n\n        It works by first scanning one partition, and use the results from\n        that partition to estimate the number of additional partitions needed\n        to satisfy the limit.\n\n        Translated from the Scala implementation in RDD#take().\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([2, 3, 4, 5, 6]).cac", "func_name": "RDD.take", "language": "python"}, {"code": "def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):\n        \"\"\"\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        L{org.apache.spark.api.python.JavaToWritableConverter}.\n\n        :param conf: Hadoop job configuration, passed in as a dict\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)\n        \"\"\"\n        jconf = self.ctx._dictToJavaMap(conf)\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,\n                                                    keyConverter, valueConverter, True)", "docstring": "Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        L{org.apache.spark.api.python.JavaToWritableConverter}.\n\n        :param conf: Hadoop job configuration, passed in as a dict\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)", "func_name": "RDD.saveAsNewAPIHadoopDataset", "language": "python"}, {"code": "def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,\n                               keyConverter=None, valueConverter=None, conf=None):\n        \"\"\"\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n        will be inferred if not specified. Keys and values are converted for output using either\n        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n\n        :param path: path to Hadoop file\n        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n               (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n        :param keyClass: fully qualified classname of key Writable class\n               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n        :param valueClass: fully qualified classname of value Writable class\n               (e.g. \"org.apache.hadoop.io.Text\", None by default)\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)\n        :param conf: Hadoop job configuration, passed in as a dict (None by default)\n        \"\"\"\n        jconf = self.ctx._dictToJavaMap(conf)\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,\n                                                       outputFormatClass,\n                                                       keyClass, valueClass,\n                                                       keyConverter, valueConverter, jconf)", "docstring": "Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n        will be inferred if not specified. Keys and values are converted for output using either\n        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n        of this RDD to create a merge", "func_name": "RDD.saveAsNewAPIHadoopFile", "language": "python"}, {"code": "def saveAsSequenceFile(self, path, compressionCodecClass=None):\n        \"\"\"\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n        RDD's key and value types. The mechanism is as follows:\n\n            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n            2. Keys and values of this Java RDD are converted to Writables and written out.\n\n        :param path: path to sequence file\n        :param compressionCodecClass: (None by default)\n        \"\"\"\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,\n                                                   path, compressionCodecClass)", "docstring": "Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n        RDD's key and value types. The mechanism is as follows:\n\n            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n            2. Keys and values of this Java RDD are converted to Writables and written out.\n\n        :param path: path to sequence file\n        :param compressionCodecClass: ", "func_name": "RDD.saveAsSequenceFile", "language": "python"}, {"code": "def saveAsPickleFile(self, path, batchSize=10):\n        \"\"\"\n        Save this RDD as a SequenceFile of serialized objects. The serializer\n        used is L{pyspark.serializers.PickleSerializer}, default batch size\n        is 10.\n\n        >>> tmpFile = NamedTemporaryFile(delete=True)\n        >>> tmpFile.close()\n        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n        ['1', '2', 'rdd', 'spark']\n        \"\"\"\n        if batchSize == 0:\n            ser = AutoBatchedSerializer(PickleSerializer())\n        else:\n            ser = BatchedSerializer(PickleSerializer(), batchSize)\n        self._reserialize(ser)._jrdd.saveAsObjectFile(path)", "docstring": "Save this RDD as a SequenceFile of serialized objects. The serializer\n        used is L{pyspark.serializers.PickleSerializer}, default batch size\n        is 10.\n\n        >>> tmpFile = NamedTemporaryFile(delete=True)\n        >>> tmpFile.close()\n        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n        ['1', '2', 'rdd', 'spark']", "func_name": "RDD.saveAsPickleFile", "language": "python"}, {"code": "def saveAsTextFile(self, path, compressionCodecClass=None):\n        \"\"\"\n        Save this RDD as a text file, using string representations of elements.\n\n        @param path: path to text file\n        @param compressionCodecClass: (None by default) string i.e.\n            \"org.apache.hadoop.io.compress.GzipCodec\"\n\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n        >>> from fileinput import input\n        >>> from glob import glob\n        >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n        '0\\\\n1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\\\n'\n\n        Empty lines are tolerated when saving to text files.\n\n        >>> tempFile2 = NamedTemporaryFile(delete=True)\n        >>> tempFile2.close()\n        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n        >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n        '\\\\n\\\\n\\\\nbar\\\\nfoo\\\\n'\n\n        Using compressionCodecClass\n\n        >>> tempFile3 = NamedTemporaryFile(delete=True)\n        >>> tempFile3.close()\n        >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n        >>> from fileinput import input, hook_compressed\n        >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n        >>> b''.join(result).decode('utf-8')\n        u'bar\\\\nfoo\\\\n'\n        \"\"\"\n        def func(split, iterator):\n            for x in iterator:\n                if not isinstance(x, (unicode, bytes)):\n                    x = unicode(x)\n                if isinstance(x, unicode):\n                    x = x.encode(\"utf-8\")\n                yield x\n        keyed = self.mapPartitionsWithIndex(func)\n        keyed._bypass_serializer = True\n        if compressionCodecClass:\n            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)\n            ", "docstring": "Save this RDD as a text file, using string representations of elements.\n\n        @param path: path to text file\n        @param compressionCodecClass: (None by default) string i.e.\n            \"org.apache.hadoop.io.compress.GzipCodec\"\n\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n        >>> from fileinput import input\n        >>> from glob import glob\n        >>> ''.join(sorted(input(glob(t", "func_name": "RDD.saveAsTextFile", "language": "python"}, {"code": "def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Merge the values for each key using an associative and commutative reduce function.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        Output will be partitioned with C{numPartitions} partitions, or\n        the default parallelism level if C{numPartitions} is not specified.\n        Default partitioner is hash-partition.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKey(add).collect())\n        [('a', 2), ('b', 1)]\n        \"\"\"\n        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)", "docstring": "Merge the values for each key using an associative and commutative reduce function.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        Output will be partitioned with C{numPartitions} partitions, or\n        the default parallelism level if C{numPartitions} is not specified.\n        Default partitioner is hash-partition.\n\n        >>> from operator import add\n        >>> rdd = sc.paralleli", "func_name": "RDD.reduceByKey", "language": "python"}, {"code": "def reduceByKeyLocally(self, func):\n        \"\"\"\n        Merge the values for each key using an associative and commutative reduce function, but\n        return the results immediately to the master as a dictionary.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\n        [('a', 2), ('b', 1)]\n        \"\"\"\n        func = fail_on_stopiteration(func)\n\n        def reducePartition(iterator):\n            m = {}\n            for k, v in iterator:\n                m[k] = func(m[k], v) if k in m else v\n            yield m\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] = func(m1[k], v) if k in m1 else v\n            return m1\n        return self.mapPartitions(reducePartition).reduce(mergeMaps)", "docstring": "Merge the values for each key using an associative and commutative reduce function, but\n        return the results immediately to the master as a dictionary.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\n        [('a', 2), ('b', 1)]", "func_name": "RDD.reduceByKeyLocally", "language": "python"}, {"code": "def partitionBy(self, numPartitions, partitionFunc=portable_hash):\n        \"\"\"\n        Return a copy of the RDD partitioned using the specified partitioner.\n\n        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n        >>> sets = pairs.partitionBy(2).glom().collect()\n        >>> len(set(sets[0]).intersection(set(sets[1])))\n        0\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n        partitioner = Partitioner(numPartitions, partitionFunc)\n        if self.partitioner == partitioner:\n            return self\n\n        # Transferring O(n) objects to Java is too expensive.\n        # Instead, we'll form the hash buckets in Python,\n        # transferring O(numPartitions) objects to Java.\n        # Each object is a (splitNumber, [objects]) pair.\n        # In order to avoid too huge objects, the objects are\n        # grouped into chunks.\n        outputSerializer = self.ctx._unbatched_serializer\n\n        limit = (_parse_memory(self.ctx._conf.get(\n            \"spark.python.worker.memory\", \"512m\")) / 2)\n\n        def add_shuffle_key(split, iterator):\n\n            buckets = defaultdict(list)\n            c, batch = 0, min(10 * numPartitions, 1000)\n\n            for k, v in iterator:\n                buckets[partitionFunc(k) % numPartitions].append((k, v))\n                c += 1\n\n                # check used memory and avg size of chunk of objects\n                if (c % 1000 == 0 and get_used_memory() > limit\n                        or c > batch):\n                    n, size = len(buckets), 0\n                    for split in list(buckets.keys()):\n                        yield pack_long(split)\n                        d = outputSerializer.dumps(buckets[split])\n                        del buckets[split]\n                        yield d\n                        size += len(d)\n\n                    avg = int(size / n) >> 20\n                    # let 1M < avg < 10M\n                    if avg < 1:\n        ", "docstring": "Return a copy of the RDD partitioned using the specified partitioner.\n\n        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n        >>> sets = pairs.partitionBy(2).glom().collect()\n        >>> len(set(sets[0]).intersection(set(sets[1])))\n        0", "func_name": "RDD.partitionBy", "language": "python"}, {"code": "def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n                     numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Generic function to combine the elements for each key using a custom\n        set of aggregation functions.\n\n        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n        type\" C.\n\n        Users provide three functions:\n\n            - C{createCombiner}, which turns a V into a C (e.g., creates\n              a one-element list)\n            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n              a list)\n            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges\n              the lists)\n\n        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n        modify and return their first argument instead of creating a new C.\n\n        In addition, users can control the partitioning of the output RDD.\n\n        .. note:: V and C can be different -- for example, one might group an RDD of type\n            (Int, Int) into an RDD of type (Int, List[Int]).\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n        >>> def to_list(a):\n        ...     return [a]\n        ...\n        >>> def append(a, b):\n        ...     a.append(b)\n        ...     return a\n        ...\n        >>> def extend(a, b):\n        ...     a.extend(b)\n        ...     return a\n        ...\n        >>> sorted(x.combineByKey(to_list, append, extend).collect())\n        [('a', [1, 2]), ('b', [1])]\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        serializer = self.ctx.serializer\n        memory = self._memory_limit()\n        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n\n        def combineLocally(iterator):\n            merger = ExternalMerger(agg, memory * 0.9, serializer)\n            merger.mergeValues(iterator)\n            return merger.items()\n\n        locally_combine", "docstring": "Generic function to combine the elements for each key using a custom\n        set of aggregation functions.\n\n        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n        type\" C.\n\n        Users provide three functions:\n\n            - C{createCombiner}, which turns a V into a C (e.g., creates\n              a one-element list)\n            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n              a list)\n            - C{mergeCombiners}, to combine t", "func_name": "RDD.combineByKey", "language": "python"}, {"code": "def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,\n                       partitionFunc=portable_hash):\n        \"\"\"\n        Aggregate the values of each key, using given combine functions and a neutral\n        \"zero value\". This function can return a different result type, U, than the type\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\n        a U and one operation for merging two U's, The former operation is used for merging\n        values within a partition, and the latter is used for merging values between\n        partitions. To avoid memory allocation, both of these functions are\n        allowed to modify and return their first argument instead of creating a new U.\n        \"\"\"\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(\n            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)", "docstring": "Aggregate the values of each key, using given combine functions and a neutral\n        \"zero value\". This function can return a different result type, U, than the type\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\n        a U and one operation for merging two U's, The former operation is used for merging\n        values within a partition, and the latter is used for merging values between\n        partitions. To avoid memory allocation, both of these functio", "func_name": "RDD.aggregateByKey", "language": "python"}, {"code": "def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Merge the values for each key using an associative function \"func\"\n        and a neutral \"zeroValue\" which may be added to the result an\n        arbitrary number of times, and must not change the result\n        (e.g., 0 for addition, or 1 for multiplication.).\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> from operator import add\n        >>> sorted(rdd.foldByKey(0, add).collect())\n        [('a', 2), ('b', 1)]\n        \"\"\"\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,\n                                 partitionFunc)", "docstring": "Merge the values for each key using an associative function \"func\"\n        and a neutral \"zeroValue\" which may be added to the result an\n        arbitrary number of times, and must not change the result\n        (e.g., 0 for addition, or 1 for multiplication.).\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> from operator import add\n        >>> sorted(rdd.foldByKey(0, add).collect())\n        [('a', 2), ('b', 1)]", "func_name": "RDD.foldByKey", "language": "python"}, {"code": "def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        Group the values for each key in the RDD into a single sequence.\n        Hash-partitions the resulting RDD with numPartitions partitions.\n\n        .. note:: If you are grouping in order to perform an aggregation (such as a\n            sum or average) over each key, using reduceByKey or aggregateByKey will\n            provide much better performance.\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.groupByKey().mapValues(len).collect())\n        [('a', 2), ('b', 1)]\n        >>> sorted(rdd.groupByKey().mapValues(list).collect())\n        [('a', [1, 1]), ('b', [1])]\n        \"\"\"\n        def createCombiner(x):\n            return [x]\n\n        def mergeValue(xs, x):\n            xs.append(x)\n            return xs\n\n        def mergeCombiners(a, b):\n            a.extend(b)\n            return a\n\n        memory = self._memory_limit()\n        serializer = self._jrdd_deserializer\n        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n\n        def combine(iterator):\n            merger = ExternalMerger(agg, memory * 0.9, serializer)\n            merger.mergeValues(iterator)\n            return merger.items()\n\n        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)\n        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n\n        def groupByKey(it):\n            merger = ExternalGroupBy(agg, memory, serializer)\n            merger.mergeCombiners(it)\n            return merger.items()\n\n        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)", "docstring": "Group the values for each key in the RDD into a single sequence.\n        Hash-partitions the resulting RDD with numPartitions partitions.\n\n        .. note:: If you are grouping in order to perform an aggregation (such as a\n            sum or average) over each key, using reduceByKey or aggregateByKey will\n            provide much better performance.\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.groupByKey().mapValues(len).collect())\n        [('a', 2), (", "func_name": "RDD.groupByKey", "language": "python"}, {"code": "def flatMapValues(self, f):\n        \"\"\"\n        Pass each value in the key-value pair RDD through a flatMap function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n        >>> def f(x): return x\n        >>> x.flatMapValues(f).collect()\n        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n        \"\"\"\n        flat_map_fn = lambda kv: ((kv[0], x) for x in f(kv[1]))\n        return self.flatMap(flat_map_fn, preservesPartitioning=True)", "docstring": "Pass each value in the key-value pair RDD through a flatMap function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n        >>> def f(x): return x\n        >>> x.flatMapValues(f).collect()\n        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]", "func_name": "RDD.flatMapValues", "language": "python"}, {"code": "def mapValues(self, f):\n        \"\"\"\n        Pass each value in the key-value pair RDD through a map function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n        >>> def f(x): return len(x)\n        >>> x.mapValues(f).collect()\n        [('a', 3), ('b', 1)]\n        \"\"\"\n        map_values_fn = lambda kv: (kv[0], f(kv[1]))\n        return self.map(map_values_fn, preservesPartitioning=True)", "docstring": "Pass each value in the key-value pair RDD through a map function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n        >>> def f(x): return len(x)\n        >>> x.mapValues(f).collect()\n        [('a', 3), ('b', 1)]", "func_name": "RDD.mapValues", "language": "python"}, {"code": "def sampleByKey(self, withReplacement, fractions, seed=None):\n        \"\"\"\n        Return a subset of this RDD sampled by key (via stratified sampling).\n        Create a sample of this RDD using variable sampling rates for\n        different keys as specified by fractions, a key to sampling rate map.\n\n        >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n        >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n        True\n        >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n        True\n        >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n        True\n        \"\"\"\n        for fraction in fractions.values():\n            assert fraction >= 0.0, \"Negative fraction value: %s\" % fraction\n        return self.mapPartitionsWithIndex(\n            RDDStratifiedSampler(withReplacement, fractions, seed).func, True)", "docstring": "Return a subset of this RDD sampled by key (via stratified sampling).\n        Create a sample of this RDD using variable sampling rates for\n        different keys as specified by fractions, a key to sampling rate map.\n\n        >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n        >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sa", "func_name": "RDD.sampleByKey", "language": "python"}, {"code": "def subtractByKey(self, other, numPartitions=None):\n        \"\"\"\n        Return each (key, value) pair in C{self} that has no pair with matching\n        key in C{other}.\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n        >>> sorted(x.subtractByKey(y).collect())\n        [('b', 4), ('b', 5)]\n        \"\"\"\n        def filter_func(pair):\n            key, (val1, val2) = pair\n            return val1 and not val2\n        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])", "docstring": "Return each (key, value) pair in C{self} that has no pair with matching\n        key in C{other}.\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n        >>> sorted(x.subtractByKey(y).collect())\n        [('b', 4), ('b', 5)]", "func_name": "RDD.subtractByKey", "language": "python"}, {"code": "def subtract(self, other, numPartitions=None):\n        \"\"\"\n        Return each value in C{self} that is not contained in C{other}.\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n        >>> sorted(x.subtract(y).collect())\n        [('a', 1), ('b', 4), ('b', 5)]\n        \"\"\"\n        # note: here 'True' is just a placeholder\n        rdd = other.map(lambda x: (x, True))\n        return self.map(lambda x: (x, True)).subtractByKey(rdd, numPartitions).keys()", "docstring": "Return each value in C{self} that is not contained in C{other}.\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n        >>> sorted(x.subtract(y).collect())\n        [('a', 1), ('b', 4), ('b', 5)]", "func_name": "RDD.subtract", "language": "python"}, {"code": "def coalesce(self, numPartitions, shuffle=False):\n        \"\"\"\n        Return a new RDD that is reduced into `numPartitions` partitions.\n\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n        [[1], [2, 3], [4, 5]]\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n        [[1, 2, 3, 4, 5]]\n        \"\"\"\n        if shuffle:\n            # Decrease the batch size in order to distribute evenly the elements across output\n            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.\n            batchSize = min(10, self.ctx._batchSize or 1024)\n            ser = BatchedSerializer(PickleSerializer(), batchSize)\n            selfCopy = self._reserialize(ser)\n            jrdd_deserializer = selfCopy._jrdd_deserializer\n            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)\n        else:\n            jrdd_deserializer = self._jrdd_deserializer\n            jrdd = self._jrdd.coalesce(numPartitions, shuffle)\n        return RDD(jrdd, self.ctx, jrdd_deserializer)", "docstring": "Return a new RDD that is reduced into `numPartitions` partitions.\n\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n        [[1], [2, 3], [4, 5]]\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n        [[1, 2, 3, 4, 5]]", "func_name": "RDD.coalesce", "language": "python"}, {"code": "def zip(self, other):\n        \"\"\"\n        Zips this RDD with another one, returning key-value pairs with the\n        first element in each RDD second element in each RDD, etc. Assumes\n        that the two RDDs have the same number of partitions and the same\n        number of elements in each partition (e.g. one was made through\n        a map on the other).\n\n        >>> x = sc.parallelize(range(0,5))\n        >>> y = sc.parallelize(range(1000, 1005))\n        >>> x.zip(y).collect()\n        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n        \"\"\"\n        def get_batch_size(ser):\n            if isinstance(ser, BatchedSerializer):\n                return ser.batchSize\n            return 1  # not batched\n\n        def batch_as(rdd, batchSize):\n            return rdd._reserialize(BatchedSerializer(PickleSerializer(), batchSize))\n\n        my_batch = get_batch_size(self._jrdd_deserializer)\n        other_batch = get_batch_size(other._jrdd_deserializer)\n        if my_batch != other_batch or not my_batch:\n            # use the smallest batchSize for both of them\n            batchSize = min(my_batch, other_batch)\n            if batchSize <= 0:\n                # auto batched or unlimited\n                batchSize = 100\n            other = batch_as(other, batchSize)\n            self = batch_as(self, batchSize)\n\n        if self.getNumPartitions() != other.getNumPartitions():\n            raise ValueError(\"Can only zip with RDD which has the same number of partitions\")\n\n        # There will be an Exception in JVM if there are different number\n        # of items in each partitions.\n        pairRDD = self._jrdd.zip(other._jrdd)\n        deserializer = PairDeserializer(self._jrdd_deserializer,\n                                        other._jrdd_deserializer)\n        return RDD(pairRDD, self.ctx, deserializer)", "docstring": "Zips this RDD with another one, returning key-value pairs with the\n        first element in each RDD second element in each RDD, etc. Assumes\n        that the two RDDs have the same number of partitions and the same\n        number of elements in each partition (e.g. one was made through\n        a map on the other).\n\n        >>> x = sc.parallelize(range(0,5))\n        >>> y = sc.parallelize(range(1000, 1005))\n        >>> x.zip(y).collect()\n        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1", "func_name": "RDD.zip", "language": "python"}, {"code": "def zipWithIndex(self):\n        \"\"\"\n        Zips this RDD with its element indices.\n\n        The ordering is first based on the partition index and then the\n        ordering of items within each partition. So the first item in\n        the first partition gets index 0, and the last item in the last\n        partition receives the largest index.\n\n        This method needs to trigger a spark job when this RDD contains\n        more than one partitions.\n\n        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n        \"\"\"\n        starts = [0]\n        if self.getNumPartitions() > 1:\n            nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n            for i in range(len(nums) - 1):\n                starts.append(starts[-1] + nums[i])\n\n        def func(k, it):\n            for i, v in enumerate(it, starts[k]):\n                yield v, i\n\n        return self.mapPartitionsWithIndex(func)", "docstring": "Zips this RDD with its element indices.\n\n        The ordering is first based on the partition index and then the\n        ordering of items within each partition. So the first item in\n        the first partition gets index 0, and the last item in the last\n        partition receives the largest index.\n\n        This method needs to trigger a spark job when this RDD contains\n        more than one partitions.\n\n        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n        [('a',", "func_name": "RDD.zipWithIndex", "language": "python"}, {"code": "def zipWithUniqueId(self):\n        \"\"\"\n        Zips this RDD with generated unique Long ids.\n\n        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n        n is the number of partitions. So there may exist gaps, but this\n        method won't trigger a spark job, which is different from\n        L{zipWithIndex}\n\n        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n        \"\"\"\n        n = self.getNumPartitions()\n\n        def func(k, it):\n            for i, v in enumerate(it):\n                yield v, i * n + k\n\n        return self.mapPartitionsWithIndex(func)", "docstring": "Zips this RDD with generated unique Long ids.\n\n        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n        n is the number of partitions. So there may exist gaps, but this\n        method won't trigger a spark job, which is different from\n        L{zipWithIndex}\n\n        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]", "func_name": "RDD.zipWithUniqueId", "language": "python"}, {"code": "def getStorageLevel(self):\n        \"\"\"\n        Get the RDD's current storage level.\n\n        >>> rdd1 = sc.parallelize([1,2])\n        >>> rdd1.getStorageLevel()\n        StorageLevel(False, False, False, False, 1)\n        >>> print(rdd1.getStorageLevel())\n        Serialized 1x Replicated\n        \"\"\"\n        java_storage_level = self._jrdd.getStorageLevel()\n        storage_level = StorageLevel(java_storage_level.useDisk(),\n                                     java_storage_level.useMemory(),\n                                     java_storage_level.useOffHeap(),\n                                     java_storage_level.deserialized(),\n                                     java_storage_level.replication())\n        return storage_level", "docstring": "Get the RDD's current storage level.\n\n        >>> rdd1 = sc.parallelize([1,2])\n        >>> rdd1.getStorageLevel()\n        StorageLevel(False, False, False, False, 1)\n        >>> print(rdd1.getStorageLevel())\n        Serialized 1x Replicated", "func_name": "RDD.getStorageLevel", "language": "python"}, {"code": "def _defaultReducePartitions(self):\n        \"\"\"\n        Returns the default number of partitions to use during reduce tasks (e.g., groupBy).\n        If spark.default.parallelism is set, then we'll use the value from SparkContext\n        defaultParallelism, otherwise we'll use the number of partitions in this RDD.\n\n        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce\n        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will\n        be inherent.\n        \"\"\"\n        if self.ctx._conf.contains(\"spark.default.parallelism\"):\n            return self.ctx.defaultParallelism\n        else:\n            return self.getNumPartitions()", "docstring": "Returns the default number of partitions to use during reduce tasks (e.g., groupBy).\n        If spark.default.parallelism is set, then we'll use the value from SparkContext\n        defaultParallelism, otherwise we'll use the number of partitions in this RDD.\n\n        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce\n        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will\n        be inherent.", "func_name": "RDD._defaultReducePartitions", "language": "python"}, {"code": "def lookup(self, key):\n        \"\"\"\n        Return the list of values in the RDD for key `key`. This operation\n        is done efficiently if the RDD has a known partitioner by only\n        searching the partition that the key maps to.\n\n        >>> l = range(1000)\n        >>> rdd = sc.parallelize(zip(l, l), 10)\n        >>> rdd.lookup(42)  # slow\n        [42]\n        >>> sorted = rdd.sortByKey()\n        >>> sorted.lookup(42)  # fast\n        [42]\n        >>> sorted.lookup(1024)\n        []\n        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n        >>> list(rdd2.lookup(('a', 'b'))[0])\n        ['c']\n        \"\"\"\n        values = self.filter(lambda kv: kv[0] == key).values()\n\n        if self.partitioner is not None:\n            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])\n\n        return values.collect()", "docstring": "Return the list of values in the RDD for key `key`. This operation\n        is done efficiently if the RDD has a known partitioner by only\n        searching the partition that the key maps to.\n\n        >>> l = range(1000)\n        >>> rdd = sc.parallelize(zip(l, l), 10)\n        >>> rdd.lookup(42)  # slow\n        [42]\n        >>> sorted = rdd.sortByKey()\n        >>> sorted.lookup(42)  # fast\n        [42]\n        >>> sorted.lookup(1024)\n        []\n        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')", "func_name": "RDD.lookup", "language": "python"}, {"code": "def _to_java_object_rdd(self):\n        \"\"\" Return a JavaRDD of Object by unpickling\n\n        It will convert each Python object into Java object by Pyrolite, whenever the\n        RDD is serialized in batch or not.\n        \"\"\"\n        rdd = self._pickled()\n        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)", "docstring": "Return a JavaRDD of Object by unpickling\n\n        It will convert each Python object into Java object by Pyrolite, whenever the\n        RDD is serialized in batch or not.", "func_name": "RDD._to_java_object_rdd", "language": "python"}, {"code": "def countApprox(self, timeout, confidence=0.95):\n        \"\"\"\n        .. note:: Experimental\n\n        Approximate version of count() that returns a potentially incomplete\n        result within a timeout, even if not all tasks have finished.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> rdd.countApprox(1000, 1.0)\n        1000\n        \"\"\"\n        drdd = self.mapPartitions(lambda it: [float(sum(1 for i in it))])\n        return int(drdd.sumApprox(timeout, confidence))", "docstring": ".. note:: Experimental\n\n        Approximate version of count() that returns a potentially incomplete\n        result within a timeout, even if not all tasks have finished.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> rdd.countApprox(1000, 1.0)\n        1000", "func_name": "RDD.countApprox", "language": "python"}, {"code": "def sumApprox(self, timeout, confidence=0.95):\n        \"\"\"\n        .. note:: Experimental\n\n        Approximate operation to return the sum within a timeout\n        or meet the confidence.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000))\n        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n        True\n        \"\"\"\n        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()\n        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())\n        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()\n        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())", "docstring": ".. note:: Experimental\n\n        Approximate operation to return the sum within a timeout\n        or meet the confidence.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000))\n        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n        True", "func_name": "RDD.sumApprox", "language": "python"}, {"code": "def meanApprox(self, timeout, confidence=0.95):\n        \"\"\"\n        .. note:: Experimental\n\n        Approximate operation to return the mean within a timeout\n        or meet the confidence.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000)) / 1000.0\n        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n        True\n        \"\"\"\n        jrdd = self.map(float)._to_java_object_rdd()\n        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())\n        r = jdrdd.meanApprox(timeout, confidence).getFinalValue()\n        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())", "docstring": ".. note:: Experimental\n\n        Approximate operation to return the mean within a timeout\n        or meet the confidence.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000)) / 1000.0\n        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n        True", "func_name": "RDD.meanApprox", "language": "python"}, {"code": "def countApproxDistinct(self, relativeSD=0.05):\n        \"\"\"\n        .. note:: Experimental\n\n        Return approximate number of distinct elements in the RDD.\n\n        The algorithm used is based on streamlib's implementation of\n        `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n        of The Art Cardinality Estimation Algorithm\", available here\n        <https://doi.org/10.1145/2452376.2452456>`_.\n\n        :param relativeSD: Relative accuracy. Smaller values create\n                           counters that require more space.\n                           It must be greater than 0.000017.\n\n        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n        >>> 900 < n < 1100\n        True\n        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n        >>> 16 < n < 24\n        True\n        \"\"\"\n        if relativeSD < 0.000017:\n            raise ValueError(\"relativeSD should be greater than 0.000017\")\n        # the hash space in Java is 2^32\n        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)\n        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)", "docstring": ".. note:: Experimental\n\n        Return approximate number of distinct elements in the RDD.\n\n        The algorithm used is based on streamlib's implementation of\n        `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n        of The Art Cardinality Estimation Algorithm\", available here\n        <https://doi.org/10.1145/2452376.2452456>`_.\n\n        :param relativeSD: Relative accuracy. Smaller values create\n                           counters that require more space.\n                 ", "func_name": "RDD.countApproxDistinct", "language": "python"}, {"code": "def toLocalIterator(self):\n        \"\"\"\n        Return an iterator that contains all of the elements in this RDD.\n        The iterator will consume as much memory as the largest partition in this RDD.\n\n        >>> rdd = sc.parallelize(range(10))\n        >>> [x for x in rdd.toLocalIterator()]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        \"\"\"\n        with SCCallSiteSync(self.context) as css:\n            sock_info = self.ctx._jvm.PythonRDD.toLocalIteratorAndServe(self._jrdd.rdd())\n        return _load_from_socket(sock_info, self._jrdd_deserializer)", "docstring": "Return an iterator that contains all of the elements in this RDD.\n        The iterator will consume as much memory as the largest partition in this RDD.\n\n        >>> rdd = sc.parallelize(range(10))\n        >>> [x for x in rdd.toLocalIterator()]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]", "func_name": "RDD.toLocalIterator", "language": "python"}, {"code": "def mapPartitions(self, f, preservesPartitioning=False):\n        \"\"\"\n        .. note:: Experimental\n\n        Returns a new RDD by applying a function to each partition of the wrapped RDD,\n        where tasks are launched together in a barrier stage.\n        The interface is the same as :func:`RDD.mapPartitions`.\n        Please see the API doc there.\n\n        .. versionadded:: 2.4.0\n        \"\"\"\n        def func(s, iterator):\n            return f(iterator)\n        return PipelinedRDD(self.rdd, func, preservesPartitioning, isFromBarrier=True)", "docstring": ".. note:: Experimental\n\n        Returns a new RDD by applying a function to each partition of the wrapped RDD,\n        where tasks are launched together in a barrier stage.\n        The interface is the same as :func:`RDD.mapPartitions`.\n        Please see the API doc there.\n\n        .. versionadded:: 2.4.0", "func_name": "RDDBarrier.mapPartitions", "language": "python"}, {"code": "def _to_seq(sc, cols, converter=None):\n    \"\"\"\n    Convert a list of Column (or names) into a JVM Seq of Column.\n\n    An optional `converter` could be used to convert items in `cols`\n    into JVM Column objects.\n    \"\"\"\n    if converter:\n        cols = [converter(c) for c in cols]\n    return sc._jvm.PythonUtils.toSeq(cols)", "docstring": "Convert a list of Column (or names) into a JVM Seq of Column.\n\n    An optional `converter` could be used to convert items in `cols`\n    into JVM Column objects.", "func_name": "_to_seq", "language": "python"}, {"code": "def _to_list(sc, cols, converter=None):\n    \"\"\"\n    Convert a list of Column (or names) into a JVM (Scala) List of Column.\n\n    An optional `converter` could be used to convert items in `cols`\n    into JVM Column objects.\n    \"\"\"\n    if converter:\n        cols = [converter(c) for c in cols]\n    return sc._jvm.PythonUtils.toList(cols)", "docstring": "Convert a list of Column (or names) into a JVM (Scala) List of Column.\n\n    An optional `converter` could be used to convert items in `cols`\n    into JVM Column objects.", "func_name": "_to_list", "language": "python"}, {"code": "def _unary_op(name, doc=\"unary operator\"):\n    \"\"\" Create a method for given unary operator \"\"\"\n    def _(self):\n        jc = getattr(self._jc, name)()\n        return Column(jc)\n    _.__doc__ = doc\n    return _", "docstring": "Create a method for given unary operator", "func_name": "_unary_op", "language": "python"}, {"code": "def _bin_op(name, doc=\"binary operator\"):\n    \"\"\" Create a method for given binary operator\n    \"\"\"\n    def _(self, other):\n        jc = other._jc if isinstance(other, Column) else other\n        njc = getattr(self._jc, name)(jc)\n        return Column(njc)\n    _.__doc__ = doc\n    return _", "docstring": "Create a method for given binary operator", "func_name": "_bin_op", "language": "python"}, {"code": "def _reverse_op(name, doc=\"binary operator\"):\n    \"\"\" Create a method for binary operator (this object is on right side)\n    \"\"\"\n    def _(self, other):\n        jother = _create_column_from_literal(other)\n        jc = getattr(jother, name)(self._jc)\n        return Column(jc)\n    _.__doc__ = doc\n    return _", "docstring": "Create a method for binary operator (this object is on right side)", "func_name": "_reverse_op", "language": "python"}, {"code": "def substr(self, startPos, length):\n        \"\"\"\n        Return a :class:`Column` which is a substring of the column.\n\n        :param startPos: start position (int or Column)\n        :param length:  length of the substring (int or Column)\n\n        >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n        [Row(col=u'Ali'), Row(col=u'Bob')]\n        \"\"\"\n        if type(startPos) != type(length):\n            raise TypeError(\n                \"startPos and length must be the same type. \"\n                \"Got {startPos_t} and {length_t}, respectively.\"\n                .format(\n                    startPos_t=type(startPos),\n                    length_t=type(length),\n                ))\n        if isinstance(startPos, int):\n            jc = self._jc.substr(startPos, length)\n        elif isinstance(startPos, Column):\n            jc = self._jc.substr(startPos._jc, length._jc)\n        else:\n            raise TypeError(\"Unexpected type: %s\" % type(startPos))\n        return Column(jc)", "docstring": "Return a :class:`Column` which is a substring of the column.\n\n        :param startPos: start position (int or Column)\n        :param length:  length of the substring (int or Column)\n\n        >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n        [Row(col=u'Ali'), Row(col=u'Bob')]", "func_name": "Column.substr", "language": "python"}, {"code": "def isin(self, *cols):\n        \"\"\"\n        A boolean expression that is evaluated to true if the value of this\n        expression is contained by the evaluated values of the arguments.\n\n        >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df[df.age.isin([1, 2, 3])].collect()\n        [Row(age=2, name=u'Alice')]\n        \"\"\"\n        if len(cols) == 1 and isinstance(cols[0], (list, set)):\n            cols = cols[0]\n        cols = [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols]\n        sc = SparkContext._active_spark_context\n        jc = getattr(self._jc, \"isin\")(_to_seq(sc, cols))\n        return Column(jc)", "docstring": "A boolean expression that is evaluated to true if the value of this\n        expression is contained by the evaluated values of the arguments.\n\n        >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df[df.age.isin([1, 2, 3])].collect()\n        [Row(age=2, name=u'Alice')]", "func_name": "Column.isin", "language": "python"}, {"code": "def alias(self, *alias, **kwargs):\n        \"\"\"\n        Returns this column aliased with a new name or names (in the case of expressions that\n        return more than one column, such as explode).\n\n        :param alias: strings of desired column names (collects all positional arguments passed)\n        :param metadata: a dict of information to be stored in ``metadata`` attribute of the\n            corresponding :class: `StructField` (optional, keyword only argument)\n\n        .. versionchanged:: 2.2\n           Added optional ``metadata`` argument.\n\n        >>> df.select(df.age.alias(\"age2\")).collect()\n        [Row(age2=2), Row(age2=5)]\n        >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n        99\n        \"\"\"\n\n        metadata = kwargs.pop('metadata', None)\n        assert not kwargs, 'Unexpected kwargs where passed: %s' % kwargs\n\n        sc = SparkContext._active_spark_context\n        if len(alias) == 1:\n            if metadata:\n                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(\n                    json.dumps(metadata))\n                return Column(getattr(self._jc, \"as\")(alias[0], jmeta))\n            else:\n                return Column(getattr(self._jc, \"as\")(alias[0]))\n        else:\n            if metadata:\n                raise ValueError('metadata can only be provided for a single column')\n            return Column(getattr(self._jc, \"as\")(_to_seq(sc, list(alias))))", "docstring": "Returns this column aliased with a new name or names (in the case of expressions that\n        return more than one column, such as explode).\n\n        :param alias: strings of desired column names (collects all positional arguments passed)\n        :param metadata: a dict of information to be stored in ``metadata`` attribute of the\n            corresponding :class: `StructField` (optional, keyword only argument)\n\n        .. versionchanged:: 2.2\n           Added optional ``metadata`` argument.\n\n   ", "func_name": "Column.alias", "language": "python"}, {"code": "def cast(self, dataType):\n        \"\"\" Convert the column into type ``dataType``.\n\n        >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]\n        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]\n        \"\"\"\n        if isinstance(dataType, basestring):\n            jc = self._jc.cast(dataType)\n        elif isinstance(dataType, DataType):\n            from pyspark.sql import SparkSession\n            spark = SparkSession.builder.getOrCreate()\n            jdt = spark._jsparkSession.parseDataType(dataType.json())\n            jc = self._jc.cast(jdt)\n        else:\n            raise TypeError(\"unexpected type: %s\" % type(dataType))\n        return Column(jc)", "docstring": "Convert the column into type ``dataType``.\n\n        >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]\n        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]", "func_name": "Column.cast", "language": "python"}, {"code": "def when(self, condition, value):\n        \"\"\"\n        Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n        See :func:`pyspark.sql.functions.when` for example usage.\n\n        :param condition: a boolean :class:`Column` expression.\n        :param value: a literal value, or a :class:`Column` expression.\n\n        >>> from pyspark.sql import functions as F\n        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n        +-----+------------------------------------------------------------+\n        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n        +-----+------------------------------------------------------------+\n        |Alice|                                                          -1|\n        |  Bob|                                                           1|\n        +-----+------------------------------------------------------------+\n        \"\"\"\n        if not isinstance(condition, Column):\n            raise TypeError(\"condition should be a Column\")\n        v = value._jc if isinstance(value, Column) else value\n        jc = self._jc.when(condition._jc, v)\n        return Column(jc)", "docstring": "Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n        See :func:`pyspark.sql.functions.when` for example usage.\n\n        :param condition: a boolean :class:`Column` expression.\n        :param value: a literal value, or a :class:`Column` expression.\n\n        >>> from pyspark.sql import functions as F\n        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.", "func_name": "Column.when", "language": "python"}, {"code": "def otherwise(self, value):\n        \"\"\"\n        Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n        See :func:`pyspark.sql.functions.when` for example usage.\n\n        :param value: a literal value, or a :class:`Column` expression.\n\n        >>> from pyspark.sql import functions as F\n        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n        +-----+-------------------------------------+\n        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n        +-----+-------------------------------------+\n        |Alice|                                    0|\n        |  Bob|                                    1|\n        +-----+-------------------------------------+\n        \"\"\"\n        v = value._jc if isinstance(value, Column) else value\n        jc = self._jc.otherwise(v)\n        return Column(jc)", "docstring": "Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n        See :func:`pyspark.sql.functions.when` for example usage.\n\n        :param value: a literal value, or a :class:`Column` expression.\n\n        >>> from pyspark.sql import functions as F\n        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n        +-----+------------------------------------", "func_name": "Column.otherwise", "language": "python"}, {"code": "def over(self, window):\n        \"\"\"\n        Define a windowing column.\n\n        :param window: a :class:`WindowSpec`\n        :return: a Column\n\n        >>> from pyspark.sql import Window\n        >>> window = Window.partitionBy(\"name\").orderBy(\"age\").rowsBetween(-1, 1)\n        >>> from pyspark.sql.functions import rank, min\n        >>> # df.select(rank().over(window), min('age').over(window))\n        \"\"\"\n        from pyspark.sql.window import WindowSpec\n        if not isinstance(window, WindowSpec):\n            raise TypeError(\"window should be WindowSpec\")\n        jc = self._jc.over(window._jspec)\n        return Column(jc)", "docstring": "Define a windowing column.\n\n        :param window: a :class:`WindowSpec`\n        :return: a Column\n\n        >>> from pyspark.sql import Window\n        >>> window = Window.partitionBy(\"name\").orderBy(\"age\").rowsBetween(-1, 1)\n        >>> from pyspark.sql.functions import rank, min\n        >>> # df.select(rank().over(window), min('age').over(window))", "func_name": "Column.over", "language": "python"}, {"code": "def transform(self, vector):\n        \"\"\"\n        Applies transformation on a vector or an RDD[Vector].\n\n        .. note:: In Python, transform cannot currently be used within\n            an RDD transformation or action.\n            Call transform directly on the RDD instead.\n\n        :param vector: Vector or RDD of Vector to be transformed.\n        \"\"\"\n        if isinstance(vector, RDD):\n            vector = vector.map(_convert_to_vector)\n        else:\n            vector = _convert_to_vector(vector)\n        return self.call(\"transform\", vector)", "docstring": "Applies transformation on a vector or an RDD[Vector].\n\n        .. note:: In Python, transform cannot currently be used within\n            an RDD transformation or action.\n            Call transform directly on the RDD instead.\n\n        :param vector: Vector or RDD of Vector to be transformed.", "func_name": "JavaVectorTransformer.transform", "language": "python"}, {"code": "def fit(self, dataset):\n        \"\"\"\n        Computes the mean and variance and stores as a model to be used\n        for later scaling.\n\n        :param dataset: The data used to compute the mean and variance\n                     to build the transformation model.\n        :return: a StandardScalarModel\n        \"\"\"\n        dataset = dataset.map(_convert_to_vector)\n        jmodel = callMLlibFunc(\"fitStandardScaler\", self.withMean, self.withStd, dataset)\n        return StandardScalerModel(jmodel)", "docstring": "Computes the mean and variance and stores as a model to be used\n        for later scaling.\n\n        :param dataset: The data used to compute the mean and variance\n                     to build the transformation model.\n        :return: a StandardScalarModel", "func_name": "StandardScaler.fit", "language": "python"}, {"code": "def fit(self, data):\n        \"\"\"\n        Returns a ChiSquared feature selector.\n\n        :param data: an `RDD[LabeledPoint]` containing the labeled dataset\n                     with categorical features. Real-valued features will be\n                     treated as categorical for each distinct value.\n                     Apply feature discretizer before using this function.\n        \"\"\"\n        jmodel = callMLlibFunc(\"fitChiSqSelector\", self.selectorType, self.numTopFeatures,\n                               self.percentile, self.fpr, self.fdr, self.fwe, data)\n        return ChiSqSelectorModel(jmodel)", "docstring": "Returns a ChiSquared feature selector.\n\n        :param data: an `RDD[LabeledPoint]` containing the labeled dataset\n                     with categorical features. Real-valued features will be\n                     treated as categorical for each distinct value.\n                     Apply feature discretizer before using this function.", "func_name": "ChiSqSelector.fit", "language": "python"}, {"code": "def fit(self, data):\n        \"\"\"\n        Computes a [[PCAModel]] that contains the principal components of the input vectors.\n        :param data: source vectors\n        \"\"\"\n        jmodel = callMLlibFunc(\"fitPCA\", self.k, data)\n        return PCAModel(jmodel)", "docstring": "Computes a [[PCAModel]] that contains the principal components of the input vectors.\n        :param data: source vectors", "func_name": "PCA.fit", "language": "python"}, {"code": "def transform(self, document):\n        \"\"\"\n        Transforms the input document (list of terms) to term frequency\n        vectors, or transform the RDD of document to RDD of term\n        frequency vectors.\n        \"\"\"\n        if isinstance(document, RDD):\n            return document.map(self.transform)\n\n        freq = {}\n        for term in document:\n            i = self.indexOf(term)\n            freq[i] = 1.0 if self.binary else freq.get(i, 0) + 1.0\n        return Vectors.sparse(self.numFeatures, freq.items())", "docstring": "Transforms the input document (list of terms) to term frequency\n        vectors, or transform the RDD of document to RDD of term\n        frequency vectors.", "func_name": "HashingTF.transform", "language": "python"}, {"code": "def fit(self, dataset):\n        \"\"\"\n        Computes the inverse document frequency.\n\n        :param dataset: an RDD of term frequency vectors\n        \"\"\"\n        if not isinstance(dataset, RDD):\n            raise TypeError(\"dataset should be an RDD of term frequency vectors\")\n        jmodel = callMLlibFunc(\"fitIDF\", self.minDocFreq, dataset.map(_convert_to_vector))\n        return IDFModel(jmodel)", "docstring": "Computes the inverse document frequency.\n\n        :param dataset: an RDD of term frequency vectors", "func_name": "IDF.fit", "language": "python"}, {"code": "def findSynonyms(self, word, num):\n        \"\"\"\n        Find synonyms of a word\n\n        :param word: a word or a vector representation of word\n        :param num: number of synonyms to find\n        :return: array of (word, cosineSimilarity)\n\n        .. note:: Local use only\n        \"\"\"\n        if not isinstance(word, basestring):\n            word = _convert_to_vector(word)\n        words, similarity = self.call(\"findSynonyms\", word, num)\n        return zip(words, similarity)", "docstring": "Find synonyms of a word\n\n        :param word: a word or a vector representation of word\n        :param num: number of synonyms to find\n        :return: array of (word, cosineSimilarity)\n\n        .. note:: Local use only", "func_name": "Word2VecModel.findSynonyms", "language": "python"}, {"code": "def load(cls, sc, path):\n        \"\"\"\n        Load a model from the given path.\n        \"\"\"\n        jmodel = sc._jvm.org.apache.spark.mllib.feature \\\n            .Word2VecModel.load(sc._jsc.sc(), path)\n        model = sc._jvm.org.apache.spark.mllib.api.python.Word2VecModelWrapper(jmodel)\n        return Word2VecModel(model)", "docstring": "Load a model from the given path.", "func_name": "Word2VecModel.load", "language": "python"}, {"code": "def transform(self, vector):\n        \"\"\"\n        Computes the Hadamard product of the vector.\n        \"\"\"\n        if isinstance(vector, RDD):\n            vector = vector.map(_convert_to_vector)\n\n        else:\n            vector = _convert_to_vector(vector)\n        return callMLlibFunc(\"elementwiseProductVector\", self.scalingVector, vector)", "docstring": "Computes the Hadamard product of the vector.", "func_name": "ElementwiseProduct.transform", "language": "python"}, {"code": "def predict(self, x):\n        \"\"\"\n        Predict values for a single data point or an RDD of points using\n        the model trained.\n\n        .. note:: In Python, predict cannot currently be used within an RDD\n            transformation or action.\n            Call predict directly on the RDD instead.\n        \"\"\"\n        if isinstance(x, RDD):\n            return self.call(\"predict\", x.map(_convert_to_vector))\n\n        else:\n            return self.call(\"predict\", _convert_to_vector(x))", "docstring": "Predict values for a single data point or an RDD of points using\n        the model trained.\n\n        .. note:: In Python, predict cannot currently be used within an RDD\n            transformation or action.\n            Call predict directly on the RDD instead.", "func_name": "TreeEnsembleModel.predict", "language": "python"}, {"code": "def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,\n                        impurity=\"gini\", maxDepth=5, maxBins=32, minInstancesPerNode=1,\n                        minInfoGain=0.0):\n        \"\"\"\n        Train a decision tree model for classification.\n\n        :param data:\n          Training data: RDD of LabeledPoint. Labels should take values\n          {0, 1, ..., numClasses-1}.\n        :param numClasses:\n          Number of classes for classification.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param impurity:\n          Criterion used for information gain calculation.\n          Supported values: \"gini\" or \"entropy\".\n          (default: \"gini\")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 5)\n        :param maxBins:\n          Number of bins used for finding splits at each node.\n          (default: 32)\n        :param minInstancesPerNode:\n          Minimum number of instances required at child nodes to create\n          the parent split.\n          (default: 1)\n        :param minInfoGain:\n          Minimum info gain required to create a split.\n          (default: 0.0)\n        :return:\n          DecisionTreeModel.\n\n        Example usage:\n\n        >>> from numpy import array\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import DecisionTree\n        >>>\n        >>> data = [\n        ...     LabeledPoint(0.0, [0.0]),\n        ...     LabeledPoint(1.0, [1.0]),\n        ...     LabeledPoint(1.0, [2.0]),\n        ...     LabeledPoint(1.0, [3.0])\n        ... ]\n        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})\n        >>> print(model)\n        DecisionTreeModel classifier of depth 1 with ", "docstring": "Train a decision tree model for classification.\n\n        :param data:\n          Training data: RDD of LabeledPoint. Labels should take values\n          {0, 1, ..., numClasses-1}.\n        :param numClasses:\n          Number of classes for classification.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param impurity", "func_name": "DecisionTree.trainClassifier", "language": "python"}, {"code": "def trainRegressor(cls, data, categoricalFeaturesInfo,\n                       impurity=\"variance\", maxDepth=5, maxBins=32, minInstancesPerNode=1,\n                       minInfoGain=0.0):\n        \"\"\"\n        Train a decision tree model for regression.\n\n        :param data:\n          Training data: RDD of LabeledPoint. Labels are real numbers.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param impurity:\n          Criterion used for information gain calculation.\n          The only supported value for regression is \"variance\".\n          (default: \"variance\")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 5)\n        :param maxBins:\n          Number of bins used for finding splits at each node.\n          (default: 32)\n        :param minInstancesPerNode:\n          Minimum number of instances required at child nodes to create\n          the parent split.\n          (default: 1)\n        :param minInfoGain:\n          Minimum info gain required to create a split.\n          (default: 0.0)\n        :return:\n          DecisionTreeModel.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import DecisionTree\n        >>> from pyspark.mllib.linalg import SparseVector\n        >>>\n        >>> sparse_data = [\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n        ... ]\n        >>>\n        >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\n        >>> model.predict(SparseVector(2, {1: 1.0}))\n        1", "docstring": "Train a decision tree model for regression.\n\n        :param data:\n          Training data: RDD of LabeledPoint. Labels are real numbers.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param impurity:\n          Criterion used for information gain calculation.\n          The only supported value for regression is \"va", "func_name": "DecisionTree.trainRegressor", "language": "python"}, {"code": "def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,\n                        featureSubsetStrategy=\"auto\", impurity=\"gini\", maxDepth=4, maxBins=32,\n                        seed=None):\n        \"\"\"\n        Train a random forest model for binary or multiclass\n        classification.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels should take values\n          {0, 1, ..., numClasses-1}.\n        :param numClasses:\n          Number of classes for classification.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param numTrees:\n          Number of trees in the random forest.\n        :param featureSubsetStrategy:\n          Number of features to consider for splits at each node.\n          Supported values: \"auto\", \"all\", \"sqrt\", \"log2\", \"onethird\".\n          If \"auto\" is set, this parameter is set based on numTrees:\n          if numTrees == 1, set to \"all\";\n          if numTrees > 1 (forest) set to \"sqrt\".\n          (default: \"auto\")\n        :param impurity:\n          Criterion used for information gain calculation.\n          Supported values: \"gini\" or \"entropy\".\n          (default: \"gini\")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 4)\n        :param maxBins:\n          Maximum number of bins used for splitting features.\n          (default: 32)\n        :param seed:\n          Random seed for bootstrapping and choosing feature subsets.\n          Set as None to generate seed based on system time.\n          (default: None)\n        :return:\n          RandomForestModel that can be used for prediction.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree impor", "docstring": "Train a random forest model for binary or multiclass\n        classification.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels should take values\n          {0, 1, ..., numClasses-1}.\n        :param numClasses:\n          Number of classes for classification.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..", "func_name": "RandomForest.trainClassifier", "language": "python"}, {"code": "def trainRegressor(cls, data, categoricalFeaturesInfo, numTrees, featureSubsetStrategy=\"auto\",\n                       impurity=\"variance\", maxDepth=4, maxBins=32, seed=None):\n        \"\"\"\n        Train a random forest model for regression.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels are real numbers.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param numTrees:\n          Number of trees in the random forest.\n        :param featureSubsetStrategy:\n          Number of features to consider for splits at each node.\n          Supported values: \"auto\", \"all\", \"sqrt\", \"log2\", \"onethird\".\n          If \"auto\" is set, this parameter is set based on numTrees:\n          if numTrees == 1, set to \"all\";\n          if numTrees > 1 (forest) set to \"onethird\" for regression.\n          (default: \"auto\")\n        :param impurity:\n          Criterion used for information gain calculation.\n          The only supported value for regression is \"variance\".\n          (default: \"variance\")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 4)\n        :param maxBins:\n          Maximum number of bins used for splitting features.\n          (default: 32)\n        :param seed:\n          Random seed for bootstrapping and choosing feature subsets.\n          Set as None to generate seed based on system time.\n          (default: None)\n        :return:\n          RandomForestModel that can be used for prediction.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import RandomForest\n        >>> from pyspark.mllib.linalg import SparseVector\n        >>>\n        >>> sparse_data = [\n        ...     LabeledPoint", "docstring": "Train a random forest model for regression.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels are real numbers.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param numTrees:\n          Number of trees in the random forest.\n        :param featureSubsetStrategy:\n          Number of featur", "func_name": "RandomForest.trainRegressor", "language": "python"}, {"code": "def trainClassifier(cls, data, categoricalFeaturesInfo,\n                        loss=\"logLoss\", numIterations=100, learningRate=0.1, maxDepth=3,\n                        maxBins=32):\n        \"\"\"\n        Train a gradient-boosted trees model for classification.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels should take values\n          {0, 1}.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param loss:\n          Loss function used for minimization during gradient boosting.\n          Supported values: \"logLoss\", \"leastSquaresError\",\n          \"leastAbsoluteError\".\n          (default: \"logLoss\")\n        :param numIterations:\n          Number of iterations of boosting.\n          (default: 100)\n        :param learningRate:\n          Learning rate for shrinking the contribution of each estimator.\n          The learning rate should be between in the interval (0, 1].\n          (default: 0.1)\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 3)\n        :param maxBins:\n          Maximum number of bins used for splitting features. DecisionTree\n          requires maxBins >= max categories.\n          (default: 32)\n        :return:\n          GradientBoostedTreesModel that can be used for prediction.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import GradientBoostedTrees\n        >>>\n        >>> data = [\n        ...     LabeledPoint(0.0, [0.0]),\n        ...     LabeledPoint(0.0, [1.0]),\n        ...     LabeledPoint(1.0, [2.0]),\n        ...     LabeledPoint(1.0, [3.0])\n        ... ]\n        >>>\n        >>> model = GradientBoostedTrees.trainClassifier(sc.parallelize(data), {}, numIt", "docstring": "Train a gradient-boosted trees model for classification.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels should take values\n          {0, 1}.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param loss:\n          Loss function used for minimization during gradient boosting.\n          Su", "func_name": "GradientBoostedTrees.trainClassifier", "language": "python"}, {"code": "def set(self, key, value):\n        \"\"\"Set a configuration property.\"\"\"\n        # Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.\n        if self._jconf is not None:\n            self._jconf.set(key, unicode(value))\n        else:\n            self._conf[key] = unicode(value)\n        return self", "docstring": "Set a configuration property.", "func_name": "SparkConf.set", "language": "python"}, {"code": "def setIfMissing(self, key, value):\n        \"\"\"Set a configuration property, if not already set.\"\"\"\n        if self.get(key) is None:\n            self.set(key, value)\n        return self", "docstring": "Set a configuration property, if not already set.", "func_name": "SparkConf.setIfMissing", "language": "python"}, {"code": "def setExecutorEnv(self, key=None, value=None, pairs=None):\n        \"\"\"Set an environment variable to be passed to executors.\"\"\"\n        if (key is not None and pairs is not None) or (key is None and pairs is None):\n            raise Exception(\"Either pass one key-value pair or a list of pairs\")\n        elif key is not None:\n            self.set(\"spark.executorEnv.\" + key, value)\n        elif pairs is not None:\n            for (k, v) in pairs:\n                self.set(\"spark.executorEnv.\" + k, v)\n        return self", "docstring": "Set an environment variable to be passed to executors.", "func_name": "SparkConf.setExecutorEnv", "language": "python"}, {"code": "def setAll(self, pairs):\n        \"\"\"\n        Set multiple parameters, passed as a list of key-value pairs.\n\n        :param pairs: list of key-value pairs to set\n        \"\"\"\n        for (k, v) in pairs:\n            self.set(k, v)\n        return self", "docstring": "Set multiple parameters, passed as a list of key-value pairs.\n\n        :param pairs: list of key-value pairs to set", "func_name": "SparkConf.setAll", "language": "python"}, {"code": "def get(self, key, defaultValue=None):\n        \"\"\"Get the configured value for some key, or return a default otherwise.\"\"\"\n        if defaultValue is None:   # Py4J doesn't call the right get() if we pass None\n            if self._jconf is not None:\n                if not self._jconf.contains(key):\n                    return None\n                return self._jconf.get(key)\n            else:\n                if key not in self._conf:\n                    return None\n                return self._conf[key]\n        else:\n            if self._jconf is not None:\n                return self._jconf.get(key, defaultValue)\n            else:\n                return self._conf.get(key, defaultValue)", "docstring": "Get the configured value for some key, or return a default otherwise.", "func_name": "SparkConf.get", "language": "python"}, {"code": "def getAll(self):\n        \"\"\"Get all values as a list of key-value pairs.\"\"\"\n        if self._jconf is not None:\n            return [(elem._1(), elem._2()) for elem in self._jconf.getAll()]\n        else:\n            return self._conf.items()", "docstring": "Get all values as a list of key-value pairs.", "func_name": "SparkConf.getAll", "language": "python"}, {"code": "def contains(self, key):\n        \"\"\"Does this configuration contain a given key?\"\"\"\n        if self._jconf is not None:\n            return self._jconf.contains(key)\n        else:\n            return key in self._conf", "docstring": "Does this configuration contain a given key?", "func_name": "SparkConf.contains", "language": "python"}, {"code": "def toDebugString(self):\n        \"\"\"\n        Returns a printable version of the configuration, as a list of\n        key=value pairs, one per line.\n        \"\"\"\n        if self._jconf is not None:\n            return self._jconf.toDebugString()\n        else:\n            return '\\n'.join('%s=%s' % (k, v) for k, v in self._conf.items())", "docstring": "Returns a printable version of the configuration, as a list of\n        key=value pairs, one per line.", "func_name": "SparkConf.toDebugString", "language": "python"}, {"code": "def listDatabases(self):\n        \"\"\"Returns a list of databases available across all sessions.\"\"\"\n        iter = self._jcatalog.listDatabases().toLocalIterator()\n        databases = []\n        while iter.hasNext():\n            jdb = iter.next()\n            databases.append(Database(\n                name=jdb.name(),\n                description=jdb.description(),\n                locationUri=jdb.locationUri()))\n        return databases", "docstring": "Returns a list of databases available across all sessions.", "func_name": "Catalog.listDatabases", "language": "python"}, {"code": "def listTables(self, dbName=None):\n        \"\"\"Returns a list of tables/views in the specified database.\n\n        If no database is specified, the current database is used.\n        This includes all temporary views.\n        \"\"\"\n        if dbName is None:\n            dbName = self.currentDatabase()\n        iter = self._jcatalog.listTables(dbName).toLocalIterator()\n        tables = []\n        while iter.hasNext():\n            jtable = iter.next()\n            tables.append(Table(\n                name=jtable.name(),\n                database=jtable.database(),\n                description=jtable.description(),\n                tableType=jtable.tableType(),\n                isTemporary=jtable.isTemporary()))\n        return tables", "docstring": "Returns a list of tables/views in the specified database.\n\n        If no database is specified, the current database is used.\n        This includes all temporary views.", "func_name": "Catalog.listTables", "language": "python"}, {"code": "def listFunctions(self, dbName=None):\n        \"\"\"Returns a list of functions registered in the specified database.\n\n        If no database is specified, the current database is used.\n        This includes all temporary functions.\n        \"\"\"\n        if dbName is None:\n            dbName = self.currentDatabase()\n        iter = self._jcatalog.listFunctions(dbName).toLocalIterator()\n        functions = []\n        while iter.hasNext():\n            jfunction = iter.next()\n            functions.append(Function(\n                name=jfunction.name(),\n                description=jfunction.description(),\n                className=jfunction.className(),\n                isTemporary=jfunction.isTemporary()))\n        return functions", "docstring": "Returns a list of functions registered in the specified database.\n\n        If no database is specified, the current database is used.\n        This includes all temporary functions.", "func_name": "Catalog.listFunctions", "language": "python"}, {"code": "def listColumns(self, tableName, dbName=None):\n        \"\"\"Returns a list of columns for the given table/view in the specified database.\n\n        If no database is specified, the current database is used.\n\n        Note: the order of arguments here is different from that of its JVM counterpart\n        because Python does not support method overloading.\n        \"\"\"\n        if dbName is None:\n            dbName = self.currentDatabase()\n        iter = self._jcatalog.listColumns(dbName, tableName).toLocalIterator()\n        columns = []\n        while iter.hasNext():\n            jcolumn = iter.next()\n            columns.append(Column(\n                name=jcolumn.name(),\n                description=jcolumn.description(),\n                dataType=jcolumn.dataType(),\n                nullable=jcolumn.nullable(),\n                isPartition=jcolumn.isPartition(),\n                isBucket=jcolumn.isBucket()))\n        return columns", "docstring": "Returns a list of columns for the given table/view in the specified database.\n\n        If no database is specified, the current database is used.\n\n        Note: the order of arguments here is different from that of its JVM counterpart\n        because Python does not support method overloading.", "func_name": "Catalog.listColumns", "language": "python"}, {"code": "def createExternalTable(self, tableName, path=None, source=None, schema=None, **options):\n        \"\"\"Creates a table based on the dataset in a data source.\n\n        It returns the DataFrame associated with the external table.\n\n        The data source is specified by the ``source`` and a set of ``options``.\n        If ``source`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used.\n\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n        created external table.\n\n        :return: :class:`DataFrame`\n        \"\"\"\n        warnings.warn(\n            \"createExternalTable is deprecated since Spark 2.2, please use createTable instead.\",\n            DeprecationWarning)\n        return self.createTable(tableName, path, source, schema, **options)", "docstring": "Creates a table based on the dataset in a data source.\n\n        It returns the DataFrame associated with the external table.\n\n        The data source is specified by the ``source`` and a set of ``options``.\n        If ``source`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used.\n\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n        created external table.\n\n        :return: :class:`DataFr", "func_name": "Catalog.createExternalTable", "language": "python"}, {"code": "def createTable(self, tableName, path=None, source=None, schema=None, **options):\n        \"\"\"Creates a table based on the dataset in a data source.\n\n        It returns the DataFrame associated with the table.\n\n        The data source is specified by the ``source`` and a set of ``options``.\n        If ``source`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n        created from the data at the given path. Otherwise a managed table is created.\n\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n        created table.\n\n        :return: :class:`DataFrame`\n        \"\"\"\n        if path is not None:\n            options[\"path\"] = path\n        if source is None:\n            source = self._sparkSession._wrapped._conf.defaultDataSourceName()\n        if schema is None:\n            df = self._jcatalog.createTable(tableName, source, options)\n        else:\n            if not isinstance(schema, StructType):\n                raise TypeError(\"schema should be StructType\")\n            scala_datatype = self._jsparkSession.parseDataType(schema.json())\n            df = self._jcatalog.createTable(tableName, source, scala_datatype, options)\n        return DataFrame(df, self._sparkSession._wrapped)", "docstring": "Creates a table based on the dataset in a data source.\n\n        It returns the DataFrame associated with the table.\n\n        The data source is specified by the ``source`` and a set of ``options``.\n        If ``source`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n        created from the data at the given path. Otherwise a managed table is created.\n\n        Optionally, a schema can be", "func_name": "Catalog.createTable", "language": "python"}, {"code": "def _load_from_socket(port, auth_secret):\n    \"\"\"\n    Load data from a given socket, this is a blocking method thus only return when the socket\n    connection has been closed.\n    \"\"\"\n    (sockfile, sock) = local_connect_and_auth(port, auth_secret)\n    # The barrier() call may block forever, so no timeout\n    sock.settimeout(None)\n    # Make a barrier() function call.\n    write_int(BARRIER_FUNCTION, sockfile)\n    sockfile.flush()\n\n    # Collect result.\n    res = UTF8Deserializer().loads(sockfile)\n\n    # Release resources.\n    sockfile.close()\n    sock.close()\n\n    return res", "docstring": "Load data from a given socket, this is a blocking method thus only return when the socket\n    connection has been closed.", "func_name": "_load_from_socket", "language": "python"}, {"code": "def _getOrCreate(cls):\n        \"\"\"\n        Internal function to get or create global BarrierTaskContext. We need to make sure\n        BarrierTaskContext is returned from here because it is needed in python worker reuse\n        scenario, see SPARK-25921 for more details.\n        \"\"\"\n        if not isinstance(cls._taskContext, BarrierTaskContext):\n            cls._taskContext = object.__new__(cls)\n        return cls._taskContext", "docstring": "Internal function to get or create global BarrierTaskContext. We need to make sure\n        BarrierTaskContext is returned from here because it is needed in python worker reuse\n        scenario, see SPARK-25921 for more details.", "func_name": "BarrierTaskContext._getOrCreate", "language": "python"}, {"code": "def _initialize(cls, port, secret):\n        \"\"\"\n        Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called\n        after BarrierTaskContext is initialized.\n        \"\"\"\n        cls._port = port\n        cls._secret = secret", "docstring": "Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called\n        after BarrierTaskContext is initialized.", "func_name": "BarrierTaskContext._initialize", "language": "python"}, {"code": "def barrier(self):\n        \"\"\"\n        .. note:: Experimental\n\n        Sets a global barrier and waits until all tasks in this stage hit this barrier.\n        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks\n        in the same stage have reached this routine.\n\n        .. warning:: In a barrier stage, each task much have the same number of `barrier()`\n            calls, in all possible code branches.\n            Otherwise, you may get the job hanging or a SparkException after timeout.\n\n        .. versionadded:: 2.4.0\n        \"\"\"\n        if self._port is None or self._secret is None:\n            raise Exception(\"Not supported to call barrier() before initialize \" +\n                            \"BarrierTaskContext.\")\n        else:\n            _load_from_socket(self._port, self._secret)", "docstring": ".. note:: Experimental\n\n        Sets a global barrier and waits until all tasks in this stage hit this barrier.\n        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks\n        in the same stage have reached this routine.\n\n        .. warning:: In a barrier stage, each task much have the same number of `barrier()`\n            calls, in all possible code branches.\n            Otherwise, you may get the job hanging or a SparkException after timeout.\n\n        .. version", "func_name": "BarrierTaskContext.barrier", "language": "python"}, {"code": "def getTaskInfos(self):\n        \"\"\"\n        .. note:: Experimental\n\n        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,\n        ordered by partition ID.\n\n        .. versionadded:: 2.4.0\n        \"\"\"\n        if self._port is None or self._secret is None:\n            raise Exception(\"Not supported to call getTaskInfos() before initialize \" +\n                            \"BarrierTaskContext.\")\n        else:\n            addresses = self._localProperties.get(\"addresses\", \"\")\n            return [BarrierTaskInfo(h.strip()) for h in addresses.split(\",\")]", "docstring": ".. note:: Experimental\n\n        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,\n        ordered by partition ID.\n\n        .. versionadded:: 2.4.0", "func_name": "BarrierTaskContext.getTaskInfos", "language": "python"}, {"code": "def since(version):\n    \"\"\"\n    A decorator that annotates a function to append the version of Spark the function was added.\n    \"\"\"\n    import re\n    indent_p = re.compile(r'\\n( +)')\n\n    def deco(f):\n        indents = indent_p.findall(f.__doc__)\n        indent = ' ' * (min(len(m) for m in indents) if indents else 0)\n        f.__doc__ = f.__doc__.rstrip() + \"\\n\\n%s.. versionadded:: %s\" % (indent, version)\n        return f\n    return deco", "docstring": "A decorator that annotates a function to append the version of Spark the function was added.", "func_name": "since", "language": "python"}, {"code": "def copy_func(f, name=None, sinceversion=None, doc=None):\n    \"\"\"\n    Returns a function with same code, globals, defaults, closure, and\n    name (or provide a new name).\n    \"\"\"\n    # See\n    # http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python\n    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,\n                            f.__closure__)\n    # in case f was given attrs (note this dict is a shallow copy):\n    fn.__dict__.update(f.__dict__)\n    if doc is not None:\n        fn.__doc__ = doc\n    if sinceversion is not None:\n        fn = since(sinceversion)(fn)\n    return fn", "docstring": "Returns a function with same code, globals, defaults, closure, and\n    name (or provide a new name).", "func_name": "copy_func", "language": "python"}, {"code": "def keyword_only(func):\n    \"\"\"\n    A decorator that forces keyword arguments in the wrapped method\n    and saves actual input keyword arguments in `_input_kwargs`.\n\n    .. note:: Should only be used to wrap a method where first arg is `self`\n    \"\"\"\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        if len(args) > 0:\n            raise TypeError(\"Method %s forces keyword arguments.\" % func.__name__)\n        self._input_kwargs = kwargs\n        return func(self, **kwargs)\n    return wrapper", "docstring": "A decorator that forces keyword arguments in the wrapped method\n    and saves actual input keyword arguments in `_input_kwargs`.\n\n    .. note:: Should only be used to wrap a method where first arg is `self`", "func_name": "keyword_only", "language": "python"}, {"code": "def _gen_param_header(name, doc, defaultValueStr, typeConverter):\n    \"\"\"\n    Generates the header part for shared variables\n\n    :param name: param name\n    :param doc: param doc\n    \"\"\"\n    template = '''class Has$Name(Params):\n    \"\"\"\n    Mixin for param $name: $doc\n    \"\"\"\n\n    $name = Param(Params._dummy(), \"$name\", \"$doc\", typeConverter=$typeConverter)\n\n    def __init__(self):\n        super(Has$Name, self).__init__()'''\n\n    if defaultValueStr is not None:\n        template += '''\n        self._setDefault($name=$defaultValueStr)'''\n\n    Name = name[0].upper() + name[1:]\n    if typeConverter is None:\n        typeConverter = str(None)\n    return template \\\n        .replace(\"$name\", name) \\\n        .replace(\"$Name\", Name) \\\n        .replace(\"$doc\", doc) \\\n        .replace(\"$defaultValueStr\", str(defaultValueStr)) \\\n        .replace(\"$typeConverter\", typeConverter)", "docstring": "Generates the header part for shared variables\n\n    :param name: param name\n    :param doc: param doc", "func_name": "_gen_param_header", "language": "python"}, {"code": "def _gen_param_code(name, doc, defaultValueStr):\n    \"\"\"\n    Generates Python code for a shared param class.\n\n    :param name: param name\n    :param doc: param doc\n    :param defaultValueStr: string representation of the default value\n    :return: code string\n    \"\"\"\n    # TODO: How to correctly inherit instance attributes?\n    template = '''\n    def set$Name(self, value):\n        \"\"\"\n        Sets the value of :py:attr:`$name`.\n        \"\"\"\n        return self._set($name=value)\n\n    def get$Name(self):\n        \"\"\"\n        Gets the value of $name or its default value.\n        \"\"\"\n        return self.getOrDefault(self.$name)'''\n\n    Name = name[0].upper() + name[1:]\n    return template \\\n        .replace(\"$name\", name) \\\n        .replace(\"$Name\", Name) \\\n        .replace(\"$doc\", doc) \\\n        .replace(\"$defaultValueStr\", str(defaultValueStr))", "docstring": "Generates Python code for a shared param class.\n\n    :param name: param name\n    :param doc: param doc\n    :param defaultValueStr: string representation of the default value\n    :return: code string", "func_name": "_gen_param_code", "language": "python"}, {"code": "def train(self, rdd, k=4, maxIterations=20, minDivisibleClusterSize=1.0, seed=-1888008604):\n        \"\"\"\n        Runs the bisecting k-means algorithm return the model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          The desired number of leaf clusters. The actual number could\n          be smaller if there are no divisible leaf clusters.\n          (default: 4)\n        :param maxIterations:\n          Maximum number of iterations allowed to split clusters.\n          (default: 20)\n        :param minDivisibleClusterSize:\n          Minimum number of points (if >= 1.0) or the minimum proportion\n          of points (if < 1.0) of a divisible cluster.\n          (default: 1)\n        :param seed:\n          Random seed value for cluster initialization.\n          (default: -1888008604 from classOf[BisectingKMeans].getName.##)\n        \"\"\"\n        java_model = callMLlibFunc(\n            \"trainBisectingKMeans\", rdd.map(_convert_to_vector),\n            k, maxIterations, minDivisibleClusterSize, seed)\n        return BisectingKMeansModel(java_model)", "docstring": "Runs the bisecting k-means algorithm return the model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          The desired number of leaf clusters. The actual number could\n          be smaller if there are no divisible leaf clusters.\n          (default: 4)\n        :param maxIterations:\n          Maximum number of iterations allowed to split clusters.\n          (default: 20)\n        :param minDivisibleClusterSize:", "func_name": "BisectingKMeans.train", "language": "python"}, {"code": "def train(cls, rdd, k, maxIterations=100, runs=1, initializationMode=\"k-means||\",\n              seed=None, initializationSteps=2, epsilon=1e-4, initialModel=None):\n        \"\"\"\n        Train a k-means clustering model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          Number of clusters to create.\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 100)\n        :param runs:\n          This param has no effect since Spark 2.0.0.\n        :param initializationMode:\n          The initialization algorithm. This can be either \"random\" or\n          \"k-means||\".\n          (default: \"k-means||\")\n        :param seed:\n          Random seed value for cluster initialization. Set as None to\n          generate seed based on system time.\n          (default: None)\n        :param initializationSteps:\n          Number of steps for the k-means|| initialization mode.\n          This is an advanced setting -- the default of 2 is almost\n          always enough.\n          (default: 2)\n        :param epsilon:\n          Distance threshold within which a center will be considered to\n          have converged. If all centers move less than this Euclidean\n          distance, iterations are stopped.\n          (default: 1e-4)\n        :param initialModel:\n          Initial cluster centers can be provided as a KMeansModel object\n          rather than using the random or k-means|| initializationModel.\n          (default: None)\n        \"\"\"\n        if runs != 1:\n            warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n        clusterInitialModel = []\n        if initialModel is not None:\n            if not isinstance(initialModel, KMeansModel):\n                raise Exception(\"initialModel is of \"+str(type(initialModel))+\". It needs \"\n                                \"to be of <type 'KMeansModel'>\")\n            clusterInitialModel = [_convert_to_vector", "docstring": "Train a k-means clustering model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          Number of clusters to create.\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 100)\n        :param runs:\n          This param has no effect since Spark 2.0.0.\n        :param initializationMode:\n          The initialization algorithm. This can be either \"random\" or\n          \"k", "func_name": "KMeans.train", "language": "python"}, {"code": "def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None):\n        \"\"\"\n        Train a Gaussian Mixture clustering model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          Number of independent Gaussians in the mixture model.\n        :param convergenceTol:\n          Maximum change in log-likelihood at which convergence is\n          considered to have occurred.\n          (default: 1e-3)\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 100)\n        :param seed:\n          Random seed for initial Gaussian distribution. Set as None to\n          generate seed based on system time.\n          (default: None)\n        :param initialModel:\n          Initial GMM starting point, bypassing the random\n          initialization.\n          (default: None)\n        \"\"\"\n        initialModelWeights = None\n        initialModelMu = None\n        initialModelSigma = None\n        if initialModel is not None:\n            if initialModel.k != k:\n                raise Exception(\"Mismatched cluster count, initialModel.k = %s, however k = %s\"\n                                % (initialModel.k, k))\n            initialModelWeights = list(initialModel.weights)\n            initialModelMu = [initialModel.gaussians[i].mu for i in range(initialModel.k)]\n            initialModelSigma = [initialModel.gaussians[i].sigma for i in range(initialModel.k)]\n        java_model = callMLlibFunc(\"trainGaussianMixtureModel\", rdd.map(_convert_to_vector),\n                                   k, convergenceTol, maxIterations, seed,\n                                   initialModelWeights, initialModelMu, initialModelSigma)\n        return GaussianMixtureModel(java_model)", "docstring": "Train a Gaussian Mixture clustering model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          Number of independent Gaussians in the mixture model.\n        :param convergenceTol:\n          Maximum change in log-likelihood at which convergence is\n          considered to have occurred.\n          (default: 1e-3)\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 10", "func_name": "GaussianMixture.train", "language": "python"}, {"code": "def load(cls, sc, path):\n        \"\"\"\n        Load a model from the given path.\n        \"\"\"\n        model = cls._load_java(sc, path)\n        wrapper =\\\n            sc._jvm.org.apache.spark.mllib.api.python.PowerIterationClusteringModelWrapper(model)\n        return PowerIterationClusteringModel(wrapper)", "docstring": "Load a model from the given path.", "func_name": "PowerIterationClusteringModel.load", "language": "python"}, {"code": "def train(cls, rdd, k, maxIterations=100, initMode=\"random\"):\n        r\"\"\"\n        :param rdd:\n          An RDD of (i, j, s\\ :sub:`ij`\\) tuples representing the\n          affinity matrix, which is the matrix A in the PIC paper.  The\n          similarity s\\ :sub:`ij`\\ must be nonnegative.  This is a symmetric\n          matrix and hence s\\ :sub:`ij`\\ = s\\ :sub:`ji`\\  For any (i, j) with\n          nonzero similarity, there should be either (i, j, s\\ :sub:`ij`\\) or\n          (j, i, s\\ :sub:`ji`\\) in the input.  Tuples with i = j are ignored,\n          because it is assumed s\\ :sub:`ij`\\ = 0.0.\n        :param k:\n          Number of clusters.\n        :param maxIterations:\n          Maximum number of iterations of the PIC algorithm.\n          (default: 100)\n        :param initMode:\n          Initialization mode. This can be either \"random\" to use\n          a random vector as vertex properties, or \"degree\" to use\n          normalized sum similarities.\n          (default: \"random\")\n        \"\"\"\n        model = callMLlibFunc(\"trainPowerIterationClusteringModel\",\n                              rdd.map(_convert_to_vector), int(k), int(maxIterations), initMode)\n        return PowerIterationClusteringModel(model)", "docstring": "r\"\"\"\n        :param rdd:\n          An RDD of (i, j, s\\ :sub:`ij`\\) tuples representing the\n          affinity matrix, which is the matrix A in the PIC paper.  The\n          similarity s\\ :sub:`ij`\\ must be nonnegative.  This is a symmetric\n          matrix and hence s\\ :sub:`ij`\\ = s\\ :sub:`ji`\\  For any (i, j) with\n          nonzero similarity, there should be either (i, j, s\\ :sub:`ij`\\) or\n          (j, i, s\\ :sub:`ji`\\) in the input.  Tuples with i = j are ignored,\n          because it is as", "func_name": "PowerIterationClustering.train", "language": "python"}, {"code": "def update(self, data, decayFactor, timeUnit):\n        \"\"\"Update the centroids, according to data\n\n        :param data:\n          RDD with new data for the model update.\n        :param decayFactor:\n          Forgetfulness of the previous centroids.\n        :param timeUnit:\n          Can be \"batches\" or \"points\". If points, then the decay factor\n          is raised to the power of number of new points and if batches,\n          then decay factor will be used as is.\n        \"\"\"\n        if not isinstance(data, RDD):\n            raise TypeError(\"Data should be of an RDD, got %s.\" % type(data))\n        data = data.map(_convert_to_vector)\n        decayFactor = float(decayFactor)\n        if timeUnit not in [\"batches\", \"points\"]:\n            raise ValueError(\n                \"timeUnit should be 'batches' or 'points', got %s.\" % timeUnit)\n        vectorCenters = [_convert_to_vector(center) for center in self.centers]\n        updatedModel = callMLlibFunc(\n            \"updateStreamingKMeansModel\", vectorCenters, self._clusterWeights,\n            data, decayFactor, timeUnit)\n        self.centers = array(updatedModel[0])\n        self._clusterWeights = list(updatedModel[1])\n        return self", "docstring": "Update the centroids, according to data\n\n        :param data:\n          RDD with new data for the model update.\n        :param decayFactor:\n          Forgetfulness of the previous centroids.\n        :param timeUnit:\n          Can be \"batches\" or \"points\". If points, then the decay factor\n          is raised to the power of number of new points and if batches,\n          then decay factor will be used as is.", "func_name": "StreamingKMeansModel.update", "language": "python"}, {"code": "def setHalfLife(self, halfLife, timeUnit):\n        \"\"\"\n        Set number of batches after which the centroids of that\n        particular batch has half the weightage.\n        \"\"\"\n        self._timeUnit = timeUnit\n        self._decayFactor = exp(log(0.5) / halfLife)\n        return self", "docstring": "Set number of batches after which the centroids of that\n        particular batch has half the weightage.", "func_name": "StreamingKMeans.setHalfLife", "language": "python"}, {"code": "def setInitialCenters(self, centers, weights):\n        \"\"\"\n        Set initial centers. Should be set before calling trainOn.\n        \"\"\"\n        self._model = StreamingKMeansModel(centers, weights)\n        return self", "docstring": "Set initial centers. Should be set before calling trainOn.", "func_name": "StreamingKMeans.setInitialCenters", "language": "python"}, {"code": "def setRandomCenters(self, dim, weight, seed):\n        \"\"\"\n        Set the initial centres to be random samples from\n        a gaussian population with constant weights.\n        \"\"\"\n        rng = random.RandomState(seed)\n        clusterCenters = rng.randn(self._k, dim)\n        clusterWeights = tile(weight, self._k)\n        self._model = StreamingKMeansModel(clusterCenters, clusterWeights)\n        return self", "docstring": "Set the initial centres to be random samples from\n        a gaussian population with constant weights.", "func_name": "StreamingKMeans.setRandomCenters", "language": "python"}, {"code": "def trainOn(self, dstream):\n        \"\"\"Train the model on the incoming dstream.\"\"\"\n        self._validate(dstream)\n\n        def update(rdd):\n            self._model.update(rdd, self._decayFactor, self._timeUnit)\n\n        dstream.foreachRDD(update)", "docstring": "Train the model on the incoming dstream.", "func_name": "StreamingKMeans.trainOn", "language": "python"}, {"code": "def predictOn(self, dstream):\n        \"\"\"\n        Make predictions on a dstream.\n        Returns a transformed dstream object\n        \"\"\"\n        self._validate(dstream)\n        return dstream.map(lambda x: self._model.predict(x))", "docstring": "Make predictions on a dstream.\n        Returns a transformed dstream object", "func_name": "StreamingKMeans.predictOn", "language": "python"}, {"code": "def predictOnValues(self, dstream):\n        \"\"\"\n        Make predictions on a keyed dstream.\n        Returns a transformed dstream object.\n        \"\"\"\n        self._validate(dstream)\n        return dstream.mapValues(lambda x: self._model.predict(x))", "docstring": "Make predictions on a keyed dstream.\n        Returns a transformed dstream object.", "func_name": "StreamingKMeans.predictOnValues", "language": "python"}, {"code": "def describeTopics(self, maxTermsPerTopic=None):\n        \"\"\"Return the topics described by weighted terms.\n\n        WARNING: If vocabSize and k are large, this can return a large object!\n\n        :param maxTermsPerTopic:\n          Maximum number of terms to collect for each topic.\n          (default: vocabulary size)\n        :return:\n          Array over topics. Each topic is represented as a pair of\n          matching arrays: (term indices, term weights in topic).\n          Each topic's terms are sorted in order of decreasing weight.\n        \"\"\"\n        if maxTermsPerTopic is None:\n            topics = self.call(\"describeTopics\")\n        else:\n            topics = self.call(\"describeTopics\", maxTermsPerTopic)\n        return topics", "docstring": "Return the topics described by weighted terms.\n\n        WARNING: If vocabSize and k are large, this can return a large object!\n\n        :param maxTermsPerTopic:\n          Maximum number of terms to collect for each topic.\n          (default: vocabulary size)\n        :return:\n          Array over topics. Each topic is represented as a pair of\n          matching arrays: (term indices, term weights in topic).\n          Each topic's terms are sorted in order of decreasing weight.", "func_name": "LDAModel.describeTopics", "language": "python"}, {"code": "def load(cls, sc, path):\n        \"\"\"Load the LDAModel from disk.\n\n        :param sc:\n          SparkContext.\n        :param path:\n          Path to where the model is stored.\n        \"\"\"\n        if not isinstance(sc, SparkContext):\n            raise TypeError(\"sc should be a SparkContext, got type %s\" % type(sc))\n        if not isinstance(path, basestring):\n            raise TypeError(\"path should be a basestring, got type %s\" % type(path))\n        model = callMLlibFunc(\"loadLDAModel\", sc, path)\n        return LDAModel(model)", "docstring": "Load the LDAModel from disk.\n\n        :param sc:\n          SparkContext.\n        :param path:\n          Path to where the model is stored.", "func_name": "LDAModel.load", "language": "python"}, {"code": "def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,\n              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer=\"em\"):\n        \"\"\"Train a LDA model.\n\n        :param rdd:\n          RDD of documents, which are tuples of document IDs and term\n          (word) count vectors. The term count vectors are \"bags of\n          words\" with a fixed-size vocabulary (where the vocabulary size\n          is the length of the vector). Document IDs must be unique\n          and >= 0.\n        :param k:\n          Number of topics to infer, i.e., the number of soft cluster\n          centers.\n          (default: 10)\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 20)\n        :param docConcentration:\n          Concentration parameter (commonly named \"alpha\") for the prior\n          placed on documents' distributions over topics (\"theta\").\n          (default: -1.0)\n        :param topicConcentration:\n          Concentration parameter (commonly named \"beta\" or \"eta\") for\n          the prior placed on topics' distributions over terms.\n          (default: -1.0)\n        :param seed:\n          Random seed for cluster initialization. Set as None to generate\n          seed based on system time.\n          (default: None)\n        :param checkpointInterval:\n          Period (in iterations) between checkpoints.\n          (default: 10)\n        :param optimizer:\n          LDAOptimizer used to perform the actual calculation. Currently\n          \"em\", \"online\" are supported.\n          (default: \"em\")\n        \"\"\"\n        model = callMLlibFunc(\"trainLDAModel\", rdd, k, maxIterations,\n                              docConcentration, topicConcentration, seed,\n                              checkpointInterval, optimizer)\n        return LDAModel(model)", "docstring": "Train a LDA model.\n\n        :param rdd:\n          RDD of documents, which are tuples of document IDs and term\n          (word) count vectors. The term count vectors are \"bags of\n          words\" with a fixed-size vocabulary (where the vocabulary size\n          is the length of the vector). Document IDs must be unique\n          and >= 0.\n        :param k:\n          Number of topics to infer, i.e., the number of soft cluster\n          centers.\n          (default: 10)\n        :param maxIterations:\n", "func_name": "LDA.train", "language": "python"}, {"code": "def _to_java_object_rdd(rdd):\n    \"\"\" Return a JavaRDD of Object by unpickling\n\n    It will convert each Python object into Java object by Pyrolite, whenever the\n    RDD is serialized in batch or not.\n    \"\"\"\n    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)", "docstring": "Return a JavaRDD of Object by unpickling\n\n    It will convert each Python object into Java object by Pyrolite, whenever the\n    RDD is serialized in batch or not.", "func_name": "_to_java_object_rdd", "language": "python"}, {"code": "def _py2java(sc, obj):\n    \"\"\" Convert Python object into Java \"\"\"\n    if isinstance(obj, RDD):\n        obj = _to_java_object_rdd(obj)\n    elif isinstance(obj, DataFrame):\n        obj = obj._jdf\n    elif isinstance(obj, SparkContext):\n        obj = obj._jsc\n    elif isinstance(obj, list):\n        obj = [_py2java(sc, x) for x in obj]\n    elif isinstance(obj, JavaObject):\n        pass\n    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):\n        pass\n    else:\n        data = bytearray(PickleSerializer().dumps(obj))\n        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)\n    return obj", "docstring": "Convert Python object into Java", "func_name": "_py2java", "language": "python"}, {"code": "def callJavaFunc(sc, func, *args):\n    \"\"\" Call Java Function \"\"\"\n    args = [_py2java(sc, a) for a in args]\n    return _java2py(sc, func(*args))", "docstring": "Call Java Function", "func_name": "callJavaFunc", "language": "python"}, {"code": "def callMLlibFunc(name, *args):\n    \"\"\" Call API in PythonMLLibAPI \"\"\"\n    sc = SparkContext.getOrCreate()\n    api = getattr(sc._jvm.PythonMLLibAPI(), name)\n    return callJavaFunc(sc, api, *args)", "docstring": "Call API in PythonMLLibAPI", "func_name": "callMLlibFunc", "language": "python"}, {"code": "def inherit_doc(cls):\n    \"\"\"\n    A decorator that makes a class inherit documentation from its parents.\n    \"\"\"\n    for name, func in vars(cls).items():\n        # only inherit docstring for public functions\n        if name.startswith(\"_\"):\n            continue\n        if not func.__doc__:\n            for parent in cls.__bases__:\n                parent_func = getattr(parent, name, None)\n                if parent_func and getattr(parent_func, \"__doc__\", None):\n                    func.__doc__ = parent_func.__doc__\n                    break\n    return cls", "docstring": "A decorator that makes a class inherit documentation from its parents.", "func_name": "inherit_doc", "language": "python"}, {"code": "def call(self, name, *a):\n        \"\"\"Call method of java_model\"\"\"\n        return callJavaFunc(self._sc, getattr(self._java_model, name), *a)", "docstring": "Call method of java_model", "func_name": "JavaModelWrapper.call", "language": "python"}, {"code": "def count(self):\n        \"\"\"\n        Return a new DStream in which each RDD has a single element\n        generated by counting each RDD of this DStream.\n        \"\"\"\n        return self.mapPartitions(lambda i: [sum(1 for _ in i)]).reduce(operator.add)", "docstring": "Return a new DStream in which each RDD has a single element\n        generated by counting each RDD of this DStream.", "func_name": "DStream.count", "language": "python"}, {"code": "def filter(self, f):\n        \"\"\"\n        Return a new DStream containing only the elements that satisfy predicate.\n        \"\"\"\n        def func(iterator):\n            return filter(f, iterator)\n        return self.mapPartitions(func, True)", "docstring": "Return a new DStream containing only the elements that satisfy predicate.", "func_name": "DStream.filter", "language": "python"}, {"code": "def map(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new DStream by applying a function to each element of DStream.\n        \"\"\"\n        def func(iterator):\n            return map(f, iterator)\n        return self.mapPartitions(func, preservesPartitioning)", "docstring": "Return a new DStream by applying a function to each element of DStream.", "func_name": "DStream.map", "language": "python"}, {"code": "def mapPartitionsWithIndex(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new DStream in which each RDD is generated by applying\n        mapPartitionsWithIndex() to each RDDs of this DStream.\n        \"\"\"\n        return self.transform(lambda rdd: rdd.mapPartitionsWithIndex(f, preservesPartitioning))", "docstring": "Return a new DStream in which each RDD is generated by applying\n        mapPartitionsWithIndex() to each RDDs of this DStream.", "func_name": "DStream.mapPartitionsWithIndex", "language": "python"}, {"code": "def reduce(self, func):\n        \"\"\"\n        Return a new DStream in which each RDD has a single element\n        generated by reducing each RDD of this DStream.\n        \"\"\"\n        return self.map(lambda x: (None, x)).reduceByKey(func, 1).map(lambda x: x[1])", "docstring": "Return a new DStream in which each RDD has a single element\n        generated by reducing each RDD of this DStream.", "func_name": "DStream.reduce", "language": "python"}, {"code": "def reduceByKey(self, func, numPartitions=None):\n        \"\"\"\n        Return a new DStream by applying reduceByKey to each RDD.\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.combineByKey(lambda x: x, func, func, numPartitions)", "docstring": "Return a new DStream by applying reduceByKey to each RDD.", "func_name": "DStream.reduceByKey", "language": "python"}, {"code": "def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n                     numPartitions=None):\n        \"\"\"\n        Return a new DStream by applying combineByKey to each RDD.\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        def func(rdd):\n            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)\n        return self.transform(func)", "docstring": "Return a new DStream by applying combineByKey to each RDD.", "func_name": "DStream.combineByKey", "language": "python"}, {"code": "def partitionBy(self, numPartitions, partitionFunc=portable_hash):\n        \"\"\"\n        Return a copy of the DStream in which each RDD are partitioned\n        using the specified partitioner.\n        \"\"\"\n        return self.transform(lambda rdd: rdd.partitionBy(numPartitions, partitionFunc))", "docstring": "Return a copy of the DStream in which each RDD are partitioned\n        using the specified partitioner.", "func_name": "DStream.partitionBy", "language": "python"}, {"code": "def foreachRDD(self, func):\n        \"\"\"\n        Apply a function to each RDD in this DStream.\n        \"\"\"\n        if func.__code__.co_argcount == 1:\n            old_func = func\n            func = lambda t, rdd: old_func(rdd)\n        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer)\n        api = self._ssc._jvm.PythonDStream\n        api.callForeachRDD(self._jdstream, jfunc)", "docstring": "Apply a function to each RDD in this DStream.", "func_name": "DStream.foreachRDD", "language": "python"}, {"code": "def pprint(self, num=10):\n        \"\"\"\n        Print the first num elements of each RDD generated in this DStream.\n\n        @param num: the number of elements from the first will be printed.\n        \"\"\"\n        def takeAndPrint(time, rdd):\n            taken = rdd.take(num + 1)\n            print(\"-------------------------------------------\")\n            print(\"Time: %s\" % time)\n            print(\"-------------------------------------------\")\n            for record in taken[:num]:\n                print(record)\n            if len(taken) > num:\n                print(\"...\")\n            print(\"\")\n\n        self.foreachRDD(takeAndPrint)", "docstring": "Print the first num elements of each RDD generated in this DStream.\n\n        @param num: the number of elements from the first will be printed.", "func_name": "DStream.pprint", "language": "python"}, {"code": "def persist(self, storageLevel):\n        \"\"\"\n        Persist the RDDs of this DStream with the given storage level\n        \"\"\"\n        self.is_cached = True\n        javaStorageLevel = self._sc._getJavaStorageLevel(storageLevel)\n        self._jdstream.persist(javaStorageLevel)\n        return self", "docstring": "Persist the RDDs of this DStream with the given storage level", "func_name": "DStream.persist", "language": "python"}, {"code": "def checkpoint(self, interval):\n        \"\"\"\n        Enable periodic checkpointing of RDDs of this DStream\n\n        @param interval: time in seconds, after each period of that, generated\n                         RDD will be checkpointed\n        \"\"\"\n        self.is_checkpointed = True\n        self._jdstream.checkpoint(self._ssc._jduration(interval))\n        return self", "docstring": "Enable periodic checkpointing of RDDs of this DStream\n\n        @param interval: time in seconds, after each period of that, generated\n                         RDD will be checkpointed", "func_name": "DStream.checkpoint", "language": "python"}, {"code": "def groupByKey(self, numPartitions=None):\n        \"\"\"\n        Return a new DStream by applying groupByKey on each RDD.\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.transform(lambda rdd: rdd.groupByKey(numPartitions))", "docstring": "Return a new DStream by applying groupByKey on each RDD.", "func_name": "DStream.groupByKey", "language": "python"}, {"code": "def countByValue(self):\n        \"\"\"\n        Return a new DStream in which each RDD contains the counts of each\n        distinct value in each RDD of this DStream.\n        \"\"\"\n        return self.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)", "docstring": "Return a new DStream in which each RDD contains the counts of each\n        distinct value in each RDD of this DStream.", "func_name": "DStream.countByValue", "language": "python"}, {"code": "def saveAsTextFiles(self, prefix, suffix=None):\n        \"\"\"\n        Save each RDD in this DStream as at text file, using string\n        representation of elements.\n        \"\"\"\n        def saveAsTextFile(t, rdd):\n            path = rddToFileName(prefix, suffix, t)\n            try:\n                rdd.saveAsTextFile(path)\n            except Py4JJavaError as e:\n                # after recovered from checkpointing, the foreachRDD may\n                # be called twice\n                if 'FileAlreadyExistsException' not in str(e):\n                    raise\n        return self.foreachRDD(saveAsTextFile)", "docstring": "Save each RDD in this DStream as at text file, using string\n        representation of elements.", "func_name": "DStream.saveAsTextFiles", "language": "python"}, {"code": "def transform(self, func):\n        \"\"\"\n        Return a new DStream in which each RDD is generated by applying a function\n        on each RDD of this DStream.\n\n        `func` can have one argument of `rdd`, or have two arguments of\n        (`time`, `rdd`)\n        \"\"\"\n        if func.__code__.co_argcount == 1:\n            oldfunc = func\n            func = lambda t, rdd: oldfunc(rdd)\n        assert func.__code__.co_argcount == 2, \"func should take one or two arguments\"\n        return TransformedDStream(self, func)", "docstring": "Return a new DStream in which each RDD is generated by applying a function\n        on each RDD of this DStream.\n\n        `func` can have one argument of `rdd`, or have two arguments of\n        (`time`, `rdd`)", "func_name": "DStream.transform", "language": "python"}, {"code": "def transformWith(self, func, other, keepSerializer=False):\n        \"\"\"\n        Return a new DStream in which each RDD is generated by applying a function\n        on each RDD of this DStream and 'other' DStream.\n\n        `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three\n        arguments of (`time`, `rdd_a`, `rdd_b`)\n        \"\"\"\n        if func.__code__.co_argcount == 2:\n            oldfunc = func\n            func = lambda t, a, b: oldfunc(a, b)\n        assert func.__code__.co_argcount == 3, \"func should take two or three arguments\"\n        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer, other._jrdd_deserializer)\n        dstream = self._sc._jvm.PythonTransformed2DStream(self._jdstream.dstream(),\n                                                          other._jdstream.dstream(), jfunc)\n        jrdd_serializer = self._jrdd_deserializer if keepSerializer else self._sc.serializer\n        return DStream(dstream.asJavaDStream(), self._ssc, jrdd_serializer)", "docstring": "Return a new DStream in which each RDD is generated by applying a function\n        on each RDD of this DStream and 'other' DStream.\n\n        `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three\n        arguments of (`time`, `rdd_a`, `rdd_b`)", "func_name": "DStream.transformWith", "language": "python"}, {"code": "def union(self, other):\n        \"\"\"\n        Return a new DStream by unifying data of another DStream with this DStream.\n\n        @param other: Another DStream having the same interval (i.e., slideDuration)\n                     as this DStream.\n        \"\"\"\n        if self._slideDuration != other._slideDuration:\n            raise ValueError(\"the two DStream should have same slide duration\")\n        return self.transformWith(lambda a, b: a.union(b), other, True)", "docstring": "Return a new DStream by unifying data of another DStream with this DStream.\n\n        @param other: Another DStream having the same interval (i.e., slideDuration)\n                     as this DStream.", "func_name": "DStream.union", "language": "python"}, {"code": "def cogroup(self, other, numPartitions=None):\n        \"\"\"\n        Return a new DStream by applying 'cogroup' between RDDs of this\n        DStream and `other` DStream.\n\n        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.transformWith(lambda a, b: a.cogroup(b, numPartitions), other)", "docstring": "Return a new DStream by applying 'cogroup' between RDDs of this\n        DStream and `other` DStream.\n\n        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.", "func_name": "DStream.cogroup", "language": "python"}, {"code": "def _jtime(self, timestamp):\n        \"\"\" Convert datetime or unix_timestamp into Time\n        \"\"\"\n        if isinstance(timestamp, datetime):\n            timestamp = time.mktime(timestamp.timetuple())\n        return self._sc._jvm.Time(long(timestamp * 1000))", "docstring": "Convert datetime or unix_timestamp into Time", "func_name": "DStream._jtime", "language": "python"}, {"code": "def slice(self, begin, end):\n        \"\"\"\n        Return all the RDDs between 'begin' to 'end' (both included)\n\n        `begin`, `end` could be datetime.datetime() or unix_timestamp\n        \"\"\"\n        jrdds = self._jdstream.slice(self._jtime(begin), self._jtime(end))\n        return [RDD(jrdd, self._sc, self._jrdd_deserializer) for jrdd in jrdds]", "docstring": "Return all the RDDs between 'begin' to 'end' (both included)\n\n        `begin`, `end` could be datetime.datetime() or unix_timestamp", "func_name": "DStream.slice", "language": "python"}, {"code": "def window(self, windowDuration, slideDuration=None):\n        \"\"\"\n        Return a new DStream in which each RDD contains all the elements in seen in a\n        sliding window of time over this DStream.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        \"\"\"\n        self._validate_window_param(windowDuration, slideDuration)\n        d = self._ssc._jduration(windowDuration)\n        if slideDuration is None:\n            return DStream(self._jdstream.window(d), self._ssc, self._jrdd_deserializer)\n        s = self._ssc._jduration(slideDuration)\n        return DStream(self._jdstream.window(d, s), self._ssc, self._jrdd_deserializer)", "docstring": "Return a new DStream in which each RDD contains all the elements in seen in a\n        sliding window of time over this DStream.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batchi", "func_name": "DStream.window", "language": "python"}, {"code": "def reduceByWindow(self, reduceFunc, invReduceFunc, windowDuration, slideDuration):\n        \"\"\"\n        Return a new DStream in which each RDD has a single element generated by reducing all\n        elements in a sliding window over this DStream.\n\n        if `invReduceFunc` is not None, the reduction is done incrementally\n        using the old window's reduced value :\n\n        1. reduce the new values that entered the window (e.g., adding new counts)\n\n        2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\n        This is more efficient than `invReduceFunc` is None.\n\n        @param reduceFunc:     associative and commutative reduce function\n        @param invReduceFunc:  inverse reduce function of `reduceFunc`; such that for all y,\n                               and invertible x:\n                               `invReduceFunc(reduceFunc(x, y), x) = y`\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                               batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                               the new DStream will generate RDDs); must be a multiple of this\n                               DStream's batching interval\n        \"\"\"\n        keyed = self.map(lambda x: (1, x))\n        reduced = keyed.reduceByKeyAndWindow(reduceFunc, invReduceFunc,\n                                             windowDuration, slideDuration, 1)\n        return reduced.map(lambda kv: kv[1])", "docstring": "Return a new DStream in which each RDD has a single element generated by reducing all\n        elements in a sliding window over this DStream.\n\n        if `invReduceFunc` is not None, the reduction is done incrementally\n        using the old window's reduced value :\n\n        1. reduce the new values that entered the window (e.g., adding new counts)\n\n        2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\n        This is more efficient than `invReduceFunc` is", "func_name": "DStream.reduceByWindow", "language": "python"}, {"code": "def countByWindow(self, windowDuration, slideDuration):\n        \"\"\"\n        Return a new DStream in which each RDD has a single element generated\n        by counting the number of elements in a window over this DStream.\n        windowDuration and slideDuration are as defined in the window() operation.\n\n        This is equivalent to window(windowDuration, slideDuration).count(),\n        but will be more efficient if window is large.\n        \"\"\"\n        return self.map(lambda x: 1).reduceByWindow(operator.add, operator.sub,\n                                                    windowDuration, slideDuration)", "docstring": "Return a new DStream in which each RDD has a single element generated\n        by counting the number of elements in a window over this DStream.\n        windowDuration and slideDuration are as defined in the window() operation.\n\n        This is equivalent to window(windowDuration, slideDuration).count(),\n        but will be more efficient if window is large.", "func_name": "DStream.countByWindow", "language": "python"}, {"code": "def countByValueAndWindow(self, windowDuration, slideDuration, numPartitions=None):\n        \"\"\"\n        Return a new DStream in which each RDD contains the count of distinct elements in\n        RDDs in a sliding window over this DStream.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        @param numPartitions:  number of partitions of each RDD in the new DStream.\n        \"\"\"\n        keyed = self.map(lambda x: (x, 1))\n        counted = keyed.reduceByKeyAndWindow(operator.add, operator.sub,\n                                             windowDuration, slideDuration, numPartitions)\n        return counted.filter(lambda kv: kv[1] > 0)", "docstring": "Return a new DStream in which each RDD contains the count of distinct elements in\n        RDDs in a sliding window over this DStream.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's ", "func_name": "DStream.countByValueAndWindow", "language": "python"}, {"code": "def groupByKeyAndWindow(self, windowDuration, slideDuration, numPartitions=None):\n        \"\"\"\n        Return a new DStream by applying `groupByKey` over a sliding window.\n        Similar to `DStream.groupByKey()`, but applies it over a sliding window.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        @param numPartitions:  Number of partitions of each RDD in the new DStream.\n        \"\"\"\n        ls = self.mapValues(lambda x: [x])\n        grouped = ls.reduceByKeyAndWindow(lambda a, b: a.extend(b) or a, lambda a, b: a[len(b):],\n                                          windowDuration, slideDuration, numPartitions)\n        return grouped.mapValues(ResultIterable)", "docstring": "Return a new DStream by applying `groupByKey` over a sliding window.\n        Similar to `DStream.groupByKey()`, but applies it over a sliding window.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                        ", "func_name": "DStream.groupByKeyAndWindow", "language": "python"}, {"code": "def reduceByKeyAndWindow(self, func, invFunc, windowDuration, slideDuration=None,\n                             numPartitions=None, filterFunc=None):\n        \"\"\"\n        Return a new DStream by applying incremental `reduceByKey` over a sliding window.\n\n        The reduced value of over a new window is calculated using the old window's reduce value :\n         1. reduce the new values that entered the window (e.g., adding new counts)\n         2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\n\n        `invFunc` can be None, then it will reduce all the RDDs in window, could be slower\n        than having `invFunc`.\n\n        @param func:           associative and commutative reduce function\n        @param invFunc:        inverse function of `reduceFunc`\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        @param numPartitions:  number of partitions of each RDD in the new DStream.\n        @param filterFunc:     function to filter expired key-value pairs;\n                              only pairs that satisfy the function are retained\n                              set this to null if you do not want to filter\n        \"\"\"\n        self._validate_window_param(windowDuration, slideDuration)\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        reduced = self.reduceByKey(func, numPartitions)\n\n        if invFunc:\n            def reduceFunc(t, a, b):\n                b = b.reduceByKey(func, numPartitions)\n                r = a.union(b).reduceByKey(func, numPartitions) if a else b\n                if filterFunc:\n                    r = r.filter(filterFunc)\n                r", "docstring": "Return a new DStream by applying incremental `reduceByKey` over a sliding window.\n\n        The reduced value of over a new window is calculated using the old window's reduce value :\n         1. reduce the new values that entered the window (e.g., adding new counts)\n         2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\n\n        `invFunc` can be None, then it will reduce all the RDDs in window, could be slower\n        than having `invFunc`.\n\n        @param", "func_name": "DStream.reduceByKeyAndWindow", "language": "python"}, {"code": "def updateStateByKey(self, updateFunc, numPartitions=None, initialRDD=None):\n        \"\"\"\n        Return a new \"state\" DStream where the state for each key is updated by applying\n        the given function on the previous state of the key and the new values of the key.\n\n        @param updateFunc: State update function. If this function returns None, then\n                           corresponding state key-value pair will be eliminated.\n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        if initialRDD and not isinstance(initialRDD, RDD):\n            initialRDD = self._sc.parallelize(initialRDD)\n\n        def reduceFunc(t, a, b):\n            if a is None:\n                g = b.groupByKey(numPartitions).mapValues(lambda vs: (list(vs), None))\n            else:\n                g = a.cogroup(b.partitionBy(numPartitions), numPartitions)\n                g = g.mapValues(lambda ab: (list(ab[1]), list(ab[0])[0] if len(ab[0]) else None))\n            state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n            return state.filter(lambda k_v: k_v[1] is not None)\n\n        jreduceFunc = TransformFunction(self._sc, reduceFunc,\n                                        self._sc.serializer, self._jrdd_deserializer)\n        if initialRDD:\n            initialRDD = initialRDD._reserialize(self._jrdd_deserializer)\n            dstream = self._sc._jvm.PythonStateDStream(self._jdstream.dstream(), jreduceFunc,\n                                                       initialRDD._jrdd)\n        else:\n            dstream = self._sc._jvm.PythonStateDStream(self._jdstream.dstream(), jreduceFunc)\n\n        return DStream(dstream.asJavaDStream(), self._ssc, self._sc.serializer)", "docstring": "Return a new \"state\" DStream where the state for each key is updated by applying\n        the given function on the previous state of the key and the new values of the key.\n\n        @param updateFunc: State update function. If this function returns None, then\n                           corresponding state key-value pair will be eliminated.", "func_name": "DStream.updateStateByKey", "language": "python"}, {"code": "def setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol=\"items\",\n                  predictionCol=\"prediction\", numPartitions=None):\n        \"\"\"\n        setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol=\"items\", \\\n                  predictionCol=\"prediction\", numPartitions=None)\n        \"\"\"\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)", "docstring": "setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol=\"items\", \\\n                  predictionCol=\"prediction\", numPartitions=None)", "func_name": "FPGrowth.setParams", "language": "python"}, {"code": "def setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000,\n                  sequenceCol=\"sequence\"):\n        \"\"\"\n        setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000, \\\n                  sequenceCol=\"sequence\")\n        \"\"\"\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)", "docstring": "setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000, \\\n                  sequenceCol=\"sequence\")", "func_name": "PrefixSpan.setParams", "language": "python"}, {"code": "def findFrequentSequentialPatterns(self, dataset):\n        \"\"\"\n        .. note:: Experimental\n\n        Finds the complete set of frequent sequential patterns in the input sequences of itemsets.\n\n        :param dataset: A dataframe containing a sequence column which is\n                        `ArrayType(ArrayType(T))` type, T is the item type for the input dataset.\n        :return: A `DataFrame` that contains columns of sequence and corresponding frequency.\n                 The schema of it will be:\n                 - `sequence: ArrayType(ArrayType(T))` (T is the item type)\n                 - `freq: Long`\n\n        >>> from pyspark.ml.fpm import PrefixSpan\n        >>> from pyspark.sql import Row\n        >>> df = sc.parallelize([Row(sequence=[[1, 2], [3]]),\n        ...                      Row(sequence=[[1], [3, 2], [1, 2]]),\n        ...                      Row(sequence=[[1, 2], [5]]),\n        ...                      Row(sequence=[[6]])]).toDF()\n        >>> prefixSpan = PrefixSpan(minSupport=0.5, maxPatternLength=5)\n        >>> prefixSpan.findFrequentSequentialPatterns(df).sort(\"sequence\").show(truncate=False)\n        +----------+----+\n        |sequence  |freq|\n        +----------+----+\n        |[[1]]     |3   |\n        |[[1], [3]]|2   |\n        |[[1, 2]]  |3   |\n        |[[2]]     |3   |\n        |[[3]]     |2   |\n        +----------+----+\n\n        .. versionadded:: 2.4.0\n        \"\"\"\n        self._transfer_params_to_java()\n        jdf = self._java_obj.findFrequentSequentialPatterns(dataset._jdf)\n        return DataFrame(jdf, dataset.sql_ctx)", "docstring": ".. note:: Experimental\n\n        Finds the complete set of frequent sequential patterns in the input sequences of itemsets.\n\n        :param dataset: A dataframe containing a sequence column which is\n                        `ArrayType(ArrayType(T))` type, T is the item type for the input dataset.\n        :return: A `DataFrame` that contains columns of sequence and corresponding frequency.\n                 The schema of it will be:\n                 - `sequence: ArrayType(ArrayType(T))` (T is the it", "func_name": "PrefixSpan.findFrequentSequentialPatterns", "language": "python"}, {"code": "def first_spark_call():\n    \"\"\"\n    Return a CallSite representing the first Spark call in the current call stack.\n    \"\"\"\n    tb = traceback.extract_stack()\n    if len(tb) == 0:\n        return None\n    file, line, module, what = tb[len(tb) - 1]\n    sparkpath = os.path.dirname(file)\n    first_spark_frame = len(tb) - 1\n    for i in range(0, len(tb)):\n        file, line, fun, what = tb[i]\n        if file.startswith(sparkpath):\n            first_spark_frame = i\n            break\n    if first_spark_frame == 0:\n        file, line, fun, what = tb[0]\n        return CallSite(function=fun, file=file, linenum=line)\n    sfile, sline, sfun, swhat = tb[first_spark_frame]\n    ufile, uline, ufun, uwhat = tb[first_spark_frame - 1]\n    return CallSite(function=sfun, file=ufile, linenum=uline)", "docstring": "Return a CallSite representing the first Spark call in the current call stack.", "func_name": "first_spark_call", "language": "python"}, {"code": "def parsePoint(line):\n    \"\"\"\n    Parse a line of text into an MLlib LabeledPoint object.\n    \"\"\"\n    values = [float(s) for s in line.split(' ')]\n    if values[0] == -1:   # Convert -1 labels to 0 for MLlib\n        values[0] = 0\n    return LabeledPoint(values[0], values[1:])", "docstring": "Parse a line of text into an MLlib LabeledPoint object.", "func_name": "parsePoint", "language": "python"}, {"code": "def fMeasure(self, label, beta=None):\n        \"\"\"\n        Returns f-measure.\n        \"\"\"\n        if beta is None:\n            return self.call(\"fMeasure\", label)\n        else:\n            return self.call(\"fMeasure\", label, beta)", "docstring": "Returns f-measure.", "func_name": "MulticlassMetrics.fMeasure", "language": "python"}, {"code": "def precision(self, label=None):\n        \"\"\"\n        Returns precision or precision for a given label (category) if specified.\n        \"\"\"\n        if label is None:\n            return self.call(\"precision\")\n        else:\n            return self.call(\"precision\", float(label))", "docstring": "Returns precision or precision for a given label (category) if specified.", "func_name": "MultilabelMetrics.precision", "language": "python"}, {"code": "def recall(self, label=None):\n        \"\"\"\n        Returns recall or recall for a given label (category) if specified.\n        \"\"\"\n        if label is None:\n            return self.call(\"recall\")\n        else:\n            return self.call(\"recall\", float(label))", "docstring": "Returns recall or recall for a given label (category) if specified.", "func_name": "MultilabelMetrics.recall", "language": "python"}, {"code": "def f1Measure(self, label=None):\n        \"\"\"\n        Returns f1Measure or f1Measure for a given label (category) if specified.\n        \"\"\"\n        if label is None:\n            return self.call(\"f1Measure\")\n        else:\n            return self.call(\"f1Measure\", float(label))", "docstring": "Returns f1Measure or f1Measure for a given label (category) if specified.", "func_name": "MultilabelMetrics.f1Measure", "language": "python"}, {"code": "def _to_corrected_pandas_type(dt):\n    \"\"\"\n    When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.\n    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.\n    \"\"\"\n    import numpy as np\n    if type(dt) == ByteType:\n        return np.int8\n    elif type(dt) == ShortType:\n        return np.int16\n    elif type(dt) == IntegerType:\n        return np.int32\n    elif type(dt) == FloatType:\n        return np.float32\n    else:\n        return None", "docstring": "When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.\n    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.", "func_name": "_to_corrected_pandas_type", "language": "python"}, {"code": "def rdd(self):\n        \"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n        \"\"\"\n        if self._lazy_rdd is None:\n            jrdd = self._jdf.javaToPython()\n            self._lazy_rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(PickleSerializer()))\n        return self._lazy_rdd", "docstring": "Returns the content as an :class:`pyspark.RDD` of :class:`Row`.", "func_name": "DataFrame.rdd", "language": "python"}, {"code": "def toJSON(self, use_unicode=True):\n        \"\"\"Converts a :class:`DataFrame` into a :class:`RDD` of string.\n\n        Each row is turned into a JSON document as one element in the returned RDD.\n\n        >>> df.toJSON().first()\n        u'{\"age\":2,\"name\":\"Alice\"}'\n        \"\"\"\n        rdd = self._jdf.toJSON()\n        return RDD(rdd.toJavaRDD(), self._sc, UTF8Deserializer(use_unicode))", "docstring": "Converts a :class:`DataFrame` into a :class:`RDD` of string.\n\n        Each row is turned into a JSON document as one element in the returned RDD.\n\n        >>> df.toJSON().first()\n        u'{\"age\":2,\"name\":\"Alice\"}'", "func_name": "DataFrame.toJSON", "language": "python"}, {"code": "def schema(self):\n        \"\"\"Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n\n        >>> df.schema\n        StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n        \"\"\"\n        if self._schema is None:\n            try:\n                self._schema = _parse_datatype_json_string(self._jdf.schema().json())\n            except AttributeError as e:\n                raise Exception(\n                    \"Unable to parse datatype from schema. %s\" % e)\n        return self._schema", "docstring": "Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n\n        >>> df.schema\n        StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))", "func_name": "DataFrame.schema", "language": "python"}, {"code": "def explain(self, extended=False):\n        \"\"\"Prints the (logical and physical) plans to the console for debugging purpose.\n\n        :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n\n        >>> df.explain()\n        == Physical Plan ==\n        *(1) Scan ExistingRDD[age#0,name#1]\n\n        >>> df.explain(True)\n        == Parsed Logical Plan ==\n        ...\n        == Analyzed Logical Plan ==\n        ...\n        == Optimized Logical Plan ==\n        ...\n        == Physical Plan ==\n        ...\n        \"\"\"\n        if extended:\n            print(self._jdf.queryExecution().toString())\n        else:\n            print(self._jdf.queryExecution().simpleString())", "docstring": "Prints the (logical and physical) plans to the console for debugging purpose.\n\n        :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n\n        >>> df.explain()\n        == Physical Plan ==\n        *(1) Scan ExistingRDD[age#0,name#1]\n\n        >>> df.explain(True)\n        == Parsed Logical Plan ==\n        ...\n        == Analyzed Logical Plan ==\n        ...\n        == Optimized Logical Plan ==\n        ...\n        == Physical Plan ==\n        ...", "func_name": "DataFrame.explain", "language": "python"}, {"code": "def exceptAll(self, other):\n        \"\"\"Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n        not in another :class:`DataFrame` while preserving duplicates.\n\n        This is equivalent to `EXCEPT ALL` in SQL.\n\n        >>> df1 = spark.createDataFrame(\n        ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n\n        >>> df1.exceptAll(df2).show()\n        +---+---+\n        | C1| C2|\n        +---+---+\n        |  a|  1|\n        |  a|  1|\n        |  a|  2|\n        |  c|  4|\n        +---+---+\n\n        Also as standard in SQL, this function resolves columns by position (not by name).\n        \"\"\"\n        return DataFrame(self._jdf.exceptAll(other._jdf), self.sql_ctx)", "docstring": "Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n        not in another :class:`DataFrame` while preserving duplicates.\n\n        This is equivalent to `EXCEPT ALL` in SQL.\n\n        >>> df1 = spark.createDataFrame(\n        ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n\n        >>> df1.exceptAll(df2).show()\n        +---+---+\n        | C1| C2|\n        ", "func_name": "DataFrame.exceptAll", "language": "python"}, {"code": "def show(self, n=20, truncate=True, vertical=False):\n        \"\"\"Prints the first ``n`` rows to the console.\n\n        :param n: Number of rows to show.\n        :param truncate: If set to True, truncate strings longer than 20 chars by default.\n            If set to a number greater than one, truncates long strings to length ``truncate``\n            and align cells right.\n        :param vertical: If set to True, print output rows vertically (one line\n            per column value).\n\n        >>> df\n        DataFrame[age: int, name: string]\n        >>> df.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  2|Alice|\n        |  5|  Bob|\n        +---+-----+\n        >>> df.show(truncate=3)\n        +---+----+\n        |age|name|\n        +---+----+\n        |  2| Ali|\n        |  5| Bob|\n        +---+----+\n        >>> df.show(vertical=True)\n        -RECORD 0-----\n         age  | 2\n         name | Alice\n        -RECORD 1-----\n         age  | 5\n         name | Bob\n        \"\"\"\n        if isinstance(truncate, bool) and truncate:\n            print(self._jdf.showString(n, 20, vertical))\n        else:\n            print(self._jdf.showString(n, int(truncate), vertical))", "docstring": "Prints the first ``n`` rows to the console.\n\n        :param n: Number of rows to show.\n        :param truncate: If set to True, truncate strings longer than 20 chars by default.\n            If set to a number greater than one, truncates long strings to length ``truncate``\n            and align cells right.\n        :param vertical: If set to True, print output rows vertically (one line\n            per column value).\n\n        >>> df\n        DataFrame[age: int, name: string]\n        >>> df.show()\n ", "func_name": "DataFrame.show", "language": "python"}, {"code": "def _repr_html_(self):\n        \"\"\"Returns a dataframe with html code when you enabled eager evaluation\n        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are\n        using support eager evaluation with HTML.\n        \"\"\"\n        import cgi\n        if not self._support_repr_html:\n            self._support_repr_html = True\n        if self.sql_ctx._conf.isReplEagerEvalEnabled():\n            max_num_rows = max(self.sql_ctx._conf.replEagerEvalMaxNumRows(), 0)\n            sock_info = self._jdf.getRowsToPython(\n                max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n            rows = list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n            head = rows[0]\n            row_data = rows[1:]\n            has_more_data = len(row_data) > max_num_rows\n            row_data = row_data[:max_num_rows]\n\n            html = \"<table border='1'>\\n\"\n            # generate table head\n            html += \"<tr><th>%s</th></tr>\\n\" % \"</th><th>\".join(map(lambda x: cgi.escape(x), head))\n            # generate table rows\n            for row in row_data:\n                html += \"<tr><td>%s</td></tr>\\n\" % \"</td><td>\".join(\n                    map(lambda x: cgi.escape(x), row))\n            html += \"</table>\\n\"\n            if has_more_data:\n                html += \"only showing top %d %s\\n\" % (\n                    max_num_rows, \"row\" if max_num_rows == 1 else \"rows\")\n            return html\n        else:\n            return None", "docstring": "Returns a dataframe with html code when you enabled eager evaluation\n        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are\n        using support eager evaluation with HTML.", "func_name": "DataFrame._repr_html_", "language": "python"}, {"code": "def checkpoint(self, eager=True):\n        \"\"\"Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n        logical plan of this DataFrame, which is especially useful in iterative algorithms where the\n        plan may grow exponentially. It will be saved to files inside the checkpoint\n        directory set with L{SparkContext.setCheckpointDir()}.\n\n        :param eager: Whether to checkpoint this DataFrame immediately\n\n        .. note:: Experimental\n        \"\"\"\n        jdf = self._jdf.checkpoint(eager)\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n        logical plan of this DataFrame, which is especially useful in iterative algorithms where the\n        plan may grow exponentially. It will be saved to files inside the checkpoint\n        directory set with L{SparkContext.setCheckpointDir()}.\n\n        :param eager: Whether to checkpoint this DataFrame immediately\n\n        .. note:: Experimental", "func_name": "DataFrame.checkpoint", "language": "python"}, {"code": "def localCheckpoint(self, eager=True):\n        \"\"\"Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n        truncate the logical plan of this DataFrame, which is especially useful in iterative\n        algorithms where the plan may grow exponentially. Local checkpoints are stored in the\n        executors using the caching subsystem and therefore they are not reliable.\n\n        :param eager: Whether to checkpoint this DataFrame immediately\n\n        .. note:: Experimental\n        \"\"\"\n        jdf = self._jdf.localCheckpoint(eager)\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n        truncate the logical plan of this DataFrame, which is especially useful in iterative\n        algorithms where the plan may grow exponentially. Local checkpoints are stored in the\n        executors using the caching subsystem and therefore they are not reliable.\n\n        :param eager: Whether to checkpoint this DataFrame immediately\n\n        .. note:: Experimental", "func_name": "DataFrame.localCheckpoint", "language": "python"}, {"code": "def withWatermark(self, eventTime, delayThreshold):\n        \"\"\"Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n        in time before which we assume no more late data is going to arrive.\n\n        Spark will use this watermark for several purposes:\n          - To know when a given time window aggregation can be finalized and thus can be emitted\n            when using output modes that do not allow updates.\n\n          - To minimize the amount of state that we need to keep for on-going aggregations.\n\n        The current watermark is computed by looking at the `MAX(eventTime)` seen across\n        all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n        of coordinating this value across partitions, the actual watermark used is only guaranteed\n        to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n        process records that arrive more than `delayThreshold` late.\n\n        :param eventTime: the name of the column that contains the event time of the row.\n        :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n            latest record that has been processed in the form of an interval\n            (e.g. \"1 minute\" or \"5 hours\").\n\n        .. note:: Evolving\n\n        >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')\n        DataFrame[name: string, time: timestamp]\n        \"\"\"\n        if not eventTime or type(eventTime) is not str:\n            raise TypeError(\"eventTime should be provided as a string\")\n        if not delayThreshold or type(delayThreshold) is not str:\n            raise TypeError(\"delayThreshold should be provided as a string interval\")\n        jdf = self._jdf.withWatermark(eventTime, delayThreshold)\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n        in time before which we assume no more late data is going to arrive.\n\n        Spark will use this watermark for several purposes:\n          - To know when a given time window aggregation can be finalized and thus can be emitted\n            when using output modes that do not allow updates.\n\n          - To minimize the amount of state that we need to keep for on-going aggregations.\n\n        The current", "func_name": "DataFrame.withWatermark", "language": "python"}, {"code": "def hint(self, name, *parameters):\n        \"\"\"Specifies some hint on the current DataFrame.\n\n        :param name: A name of the hint.\n        :param parameters: Optional parameters.\n        :return: :class:`DataFrame`\n\n        >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n        +----+---+------+\n        |name|age|height|\n        +----+---+------+\n        | Bob|  5|    85|\n        +----+---+------+\n        \"\"\"\n        if len(parameters) == 1 and isinstance(parameters[0], list):\n            parameters = parameters[0]\n\n        if not isinstance(name, str):\n            raise TypeError(\"name should be provided as str, got {0}\".format(type(name)))\n\n        allowed_types = (basestring, list, float, int)\n        for p in parameters:\n            if not isinstance(p, allowed_types):\n                raise TypeError(\n                    \"all parameters should be in {0}, got {1} of type {2}\".format(\n                        allowed_types, p, type(p)))\n\n        jdf = self._jdf.hint(name, self._jseq(parameters))\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Specifies some hint on the current DataFrame.\n\n        :param name: A name of the hint.\n        :param parameters: Optional parameters.\n        :return: :class:`DataFrame`\n\n        >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n        +----+---+------+\n        |name|age|height|\n        +----+---+------+\n        | Bob|  5|    85|\n        +----+---+------+", "func_name": "DataFrame.hint", "language": "python"}, {"code": "def collect(self):\n        \"\"\"Returns all the records as a list of :class:`Row`.\n\n        >>> df.collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        \"\"\"\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.collectToPython()\n        return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))", "docstring": "Returns all the records as a list of :class:`Row`.\n\n        >>> df.collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]", "func_name": "DataFrame.collect", "language": "python"}, {"code": "def toLocalIterator(self):\n        \"\"\"\n        Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n        The iterator will consume as much memory as the largest partition in this DataFrame.\n\n        >>> list(df.toLocalIterator())\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        \"\"\"\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.toPythonIterator()\n        return _load_from_socket(sock_info, BatchedSerializer(PickleSerializer()))", "docstring": "Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n        The iterator will consume as much memory as the largest partition in this DataFrame.\n\n        >>> list(df.toLocalIterator())\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]", "func_name": "DataFrame.toLocalIterator", "language": "python"}, {"code": "def limit(self, num):\n        \"\"\"Limits the result count to the number specified.\n\n        >>> df.limit(1).collect()\n        [Row(age=2, name=u'Alice')]\n        >>> df.limit(0).collect()\n        []\n        \"\"\"\n        jdf = self._jdf.limit(num)\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Limits the result count to the number specified.\n\n        >>> df.limit(1).collect()\n        [Row(age=2, name=u'Alice')]\n        >>> df.limit(0).collect()\n        []", "func_name": "DataFrame.limit", "language": "python"}, {"code": "def persist(self, storageLevel=StorageLevel.MEMORY_AND_DISK):\n        \"\"\"Sets the storage level to persist the contents of the :class:`DataFrame` across\n        operations after the first time it is computed. This can only be used to assign\n        a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_AND_DISK}).\n\n        .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.\n        \"\"\"\n        self.is_cached = True\n        javaStorageLevel = self._sc._getJavaStorageLevel(storageLevel)\n        self._jdf.persist(javaStorageLevel)\n        return self", "docstring": "Sets the storage level to persist the contents of the :class:`DataFrame` across\n        operations after the first time it is computed. This can only be used to assign\n        a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_AND_DISK}).\n\n        .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.", "func_name": "DataFrame.persist", "language": "python"}, {"code": "def storageLevel(self):\n        \"\"\"Get the :class:`DataFrame`'s current storage level.\n\n        >>> df.storageLevel\n        StorageLevel(False, False, False, False, 1)\n        >>> df.cache().storageLevel\n        StorageLevel(True, True, False, True, 1)\n        >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n        StorageLevel(True, False, False, False, 2)\n        \"\"\"\n        java_storage_level = self._jdf.storageLevel()\n        storage_level = StorageLevel(java_storage_level.useDisk(),\n                                     java_storage_level.useMemory(),\n                                     java_storage_level.useOffHeap(),\n                                     java_storage_level.deserialized(),\n                                     java_storage_level.replication())\n        return storage_level", "docstring": "Get the :class:`DataFrame`'s current storage level.\n\n        >>> df.storageLevel\n        StorageLevel(False, False, False, False, 1)\n        >>> df.cache().storageLevel\n        StorageLevel(True, True, False, True, 1)\n        >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n        StorageLevel(True, False, False, False, 2)", "func_name": "DataFrame.storageLevel", "language": "python"}, {"code": "def unpersist(self, blocking=False):\n        \"\"\"Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. note:: `blocking` default has changed to False to match Scala in 2.0.\n        \"\"\"\n        self.is_cached = False\n        self._jdf.unpersist(blocking)\n        return self", "docstring": "Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. note:: `blocking` default has changed to False to match Scala in 2.0.", "func_name": "DataFrame.unpersist", "language": "python"}, {"code": "def coalesce(self, numPartitions):\n        \"\"\"\n        Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n\n        :param numPartitions: int, to specify the target number of partitions\n\n        Similar to coalesce defined on an :class:`RDD`, this operation results in a\n        narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n        there will not be a shuffle, instead each of the 100 new partitions will\n        claim 10 of the current partitions. If a larger number of partitions is requested,\n        it will stay at the current number of partitions.\n\n        However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n        this may result in your computation taking place on fewer nodes than\n        you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n        you can call repartition(). This will add a shuffle step, but means the\n        current upstream partitions will be executed in parallel (per whatever\n        the current partitioning is).\n\n        >>> df.coalesce(1).rdd.getNumPartitions()\n        1\n        \"\"\"\n        return DataFrame(self._jdf.coalesce(numPartitions), self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n\n        :param numPartitions: int, to specify the target number of partitions\n\n        Similar to coalesce defined on an :class:`RDD`, this operation results in a\n        narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n        there will not be a shuffle, instead each of the 100 new partitions will\n        claim 10 of the current partitions. If a larger number of partitions is requested,\n    ", "func_name": "DataFrame.coalesce", "language": "python"}, {"code": "def repartition(self, numPartitions, *cols):\n        \"\"\"\n        Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n        resulting DataFrame is hash partitioned.\n\n        :param numPartitions:\n            can be an int to specify the target number of partitions or a Column.\n            If it is a Column, it will be used as the first partitioning column. If not specified,\n            the default number of partitions is used.\n\n        .. versionchanged:: 1.6\n           Added optional arguments to specify the partitioning columns. Also made numPartitions\n           optional if partitioning columns are specified.\n\n        >>> df.repartition(10).rdd.getNumPartitions()\n        10\n        >>> data = df.union(df).repartition(\"age\")\n        >>> data.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  5|  Bob|\n        |  5|  Bob|\n        |  2|Alice|\n        |  2|Alice|\n        +---+-----+\n        >>> data = data.repartition(7, \"age\")\n        >>> data.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  2|Alice|\n        |  5|  Bob|\n        |  2|Alice|\n        |  5|  Bob|\n        +---+-----+\n        >>> data.rdd.getNumPartitions()\n        7\n        >>> data = data.repartition(\"name\", \"age\")\n        >>> data.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  5|  Bob|\n        |  5|  Bob|\n        |  2|Alice|\n        |  2|Alice|\n        +---+-----+\n        \"\"\"\n        if isinstance(numPartitions, int):\n            if len(cols) == 0:\n                return DataFrame(self._jdf.repartition(numPartitions), self.sql_ctx)\n            else:\n                return DataFrame(\n                    self._jdf.repartition(numPartitions, self._jcols(*cols)), self.sql_ctx)\n        elif isinstance(numPartitions, (basestring, Column)):\n            cols = (numPartitions, ) + cols\n            return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sql_ctx)\n        else:\n      ", "docstring": "Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n        resulting DataFrame is hash partitioned.\n\n        :param numPartitions:\n            can be an int to specify the target number of partitions or a Column.\n            If it is a Column, it will be used as the first partitioning column. If not specified,\n            the default number of partitions is used.\n\n        .. versionchanged:: 1.6\n           Added optional arguments to specify the partitioning ", "func_name": "DataFrame.repartition", "language": "python"}, {"code": "def sample(self, withReplacement=None, fraction=None, seed=None):\n        \"\"\"Returns a sampled subset of this :class:`DataFrame`.\n\n        :param withReplacement: Sample with replacement or not (default False).\n        :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n        :param seed: Seed for sampling (default a random seed).\n\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n            count of the given :class:`DataFrame`.\n\n        .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n\n        >>> df = spark.range(10)\n        >>> df.sample(0.5, 3).count()\n        7\n        >>> df.sample(fraction=0.5, seed=3).count()\n        7\n        >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n        1\n        >>> df.sample(1.0).count()\n        10\n        >>> df.sample(fraction=1.0).count()\n        10\n        >>> df.sample(False, fraction=1.0).count()\n        10\n        \"\"\"\n\n        # For the cases below:\n        #   sample(True, 0.5 [, seed])\n        #   sample(True, fraction=0.5 [, seed])\n        #   sample(withReplacement=False, fraction=0.5 [, seed])\n        is_withReplacement_set = \\\n            type(withReplacement) == bool and isinstance(fraction, float)\n\n        # For the case below:\n        #   sample(faction=0.5 [, seed])\n        is_withReplacement_omitted_kwargs = \\\n            withReplacement is None and isinstance(fraction, float)\n\n        # For the case below:\n        #   sample(0.5 [, seed])\n        is_withReplacement_omitted_args = isinstance(withReplacement, float)\n\n        if not (is_withReplacement_set\n                or is_withReplacement_omitted_kwargs\n                or is_withReplacement_omitted_args):\n            argtypes = [\n                str(type(arg)) for arg in [withReplacement, fraction, seed] if arg is not None]\n            raise TypeError(\n                \"withReplacement (optional), fraction (required) and seed (optional)\"\n         ", "docstring": "Returns a sampled subset of this :class:`DataFrame`.\n\n        :param withReplacement: Sample with replacement or not (default False).\n        :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n        :param seed: Seed for sampling (default a random seed).\n\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n            count of the given :class:`DataFrame`.\n\n        .. note:: `fraction` is required and, `withReplacement` and `seed` are o", "func_name": "DataFrame.sample", "language": "python"}, {"code": "def sampleBy(self, col, fractions, seed=None):\n        \"\"\"\n        Returns a stratified sample without replacement based on the\n        fraction given on each stratum.\n\n        :param col: column that defines strata\n        :param fractions:\n            sampling fraction for each stratum. If a stratum is not\n            specified, we treat its fraction as zero.\n        :param seed: random seed\n        :return: a new DataFrame that represents the stratified sample\n\n        >>> from pyspark.sql.functions import col\n        >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n        >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n        >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n        +---+-----+\n        |key|count|\n        +---+-----+\n        |  0|    3|\n        |  1|    6|\n        +---+-----+\n        >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n        33\n\n        .. versionchanged:: 3.0\n           Added sampling by a column of :class:`Column`\n        \"\"\"\n        if isinstance(col, basestring):\n            col = Column(col)\n        elif not isinstance(col, Column):\n            raise ValueError(\"col must be a string or a column, but got %r\" % type(col))\n        if not isinstance(fractions, dict):\n            raise ValueError(\"fractions must be a dict but got %r\" % type(fractions))\n        for k, v in fractions.items():\n            if not isinstance(k, (float, int, long, basestring)):\n                raise ValueError(\"key must be float, int, long, or string, but got %r\" % type(k))\n            fractions[k] = float(v)\n        col = col._jc\n        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n        return DataFrame(self._jdf.stat().sampleBy(col, self._jmap(fractions), seed), self.sql_ctx)", "docstring": "Returns a stratified sample without replacement based on the\n        fraction given on each stratum.\n\n        :param col: column that defines strata\n        :param fractions:\n            sampling fraction for each stratum. If a stratum is not\n            specified, we treat its fraction as zero.\n        :param seed: random seed\n        :return: a new DataFrame that represents the stratified sample\n\n        >>> from pyspark.sql.functions import col\n        >>> dataset = sqlContext.range(0, 100).s", "func_name": "DataFrame.sampleBy", "language": "python"}, {"code": "def randomSplit(self, weights, seed=None):\n        \"\"\"Randomly splits this :class:`DataFrame` with the provided weights.\n\n        :param weights: list of doubles as weights with which to split the DataFrame. Weights will\n            be normalized if they don't sum up to 1.0.\n        :param seed: The seed for sampling.\n\n        >>> splits = df4.randomSplit([1.0, 2.0], 24)\n        >>> splits[0].count()\n        2\n\n        >>> splits[1].count()\n        2\n        \"\"\"\n        for w in weights:\n            if w < 0.0:\n                raise ValueError(\"Weights must be positive. Found weight value: %s\" % w)\n        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n        rdd_array = self._jdf.randomSplit(_to_list(self.sql_ctx._sc, weights), long(seed))\n        return [DataFrame(rdd, self.sql_ctx) for rdd in rdd_array]", "docstring": "Randomly splits this :class:`DataFrame` with the provided weights.\n\n        :param weights: list of doubles as weights with which to split the DataFrame. Weights will\n            be normalized if they don't sum up to 1.0.\n        :param seed: The seed for sampling.\n\n        >>> splits = df4.randomSplit([1.0, 2.0], 24)\n        >>> splits[0].count()\n        2\n\n        >>> splits[1].count()\n        2", "func_name": "DataFrame.randomSplit", "language": "python"}, {"code": "def dtypes(self):\n        \"\"\"Returns all column names and their data types as a list.\n\n        >>> df.dtypes\n        [('age', 'int'), ('name', 'string')]\n        \"\"\"\n        return [(str(f.name), f.dataType.simpleString()) for f in self.schema.fields]", "docstring": "Returns all column names and their data types as a list.\n\n        >>> df.dtypes\n        [('age', 'int'), ('name', 'string')]", "func_name": "DataFrame.dtypes", "language": "python"}, {"code": "def colRegex(self, colName):\n        \"\"\"\n        Selects column based on the column name specified as a regex and returns it\n        as :class:`Column`.\n\n        :param colName: string, column name specified as a regex.\n\n        >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n        >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n        +----+\n        |Col2|\n        +----+\n        |   1|\n        |   2|\n        |   3|\n        +----+\n        \"\"\"\n        if not isinstance(colName, basestring):\n            raise ValueError(\"colName should be provided as string\")\n        jc = self._jdf.colRegex(colName)\n        return Column(jc)", "docstring": "Selects column based on the column name specified as a regex and returns it\n        as :class:`Column`.\n\n        :param colName: string, column name specified as a regex.\n\n        >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n        >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n        +----+\n        |Col2|\n        +----+\n        |   1|\n        |   2|\n        |   3|\n        +----+", "func_name": "DataFrame.colRegex", "language": "python"}, {"code": "def alias(self, alias):\n        \"\"\"Returns a new :class:`DataFrame` with an alias set.\n\n        :param alias: string, an alias name to be set for the DataFrame.\n\n        >>> from pyspark.sql.functions import *\n        >>> df_as1 = df.alias(\"df_as1\")\n        >>> df_as2 = df.alias(\"df_as2\")\n        >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n        >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()\n        [Row(name=u'Bob', name=u'Bob', age=5), Row(name=u'Alice', name=u'Alice', age=2)]\n        \"\"\"\n        assert isinstance(alias, basestring), \"alias should be a string\"\n        return DataFrame(getattr(self._jdf, \"as\")(alias), self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` with an alias set.\n\n        :param alias: string, an alias name to be set for the DataFrame.\n\n        >>> from pyspark.sql.functions import *\n        >>> df_as1 = df.alias(\"df_as1\")\n        >>> df_as2 = df.alias(\"df_as2\")\n        >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n        >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()\n        [Row(name=u'Bob', name=u'Bob', age=5), Row(name=u'Alice', n", "func_name": "DataFrame.alias", "language": "python"}, {"code": "def crossJoin(self, other):\n        \"\"\"Returns the cartesian product with another :class:`DataFrame`.\n\n        :param other: Right side of the cartesian product.\n\n        >>> df.select(\"age\", \"name\").collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        >>> df2.select(\"name\", \"height\").collect()\n        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85)]\n        >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n        [Row(age=2, name=u'Alice', height=80), Row(age=2, name=u'Alice', height=85),\n         Row(age=5, name=u'Bob', height=80), Row(age=5, name=u'Bob', height=85)]\n        \"\"\"\n\n        jdf = self._jdf.crossJoin(other._jdf)\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Returns the cartesian product with another :class:`DataFrame`.\n\n        :param other: Right side of the cartesian product.\n\n        >>> df.select(\"age\", \"name\").collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        >>> df2.select(\"name\", \"height\").collect()\n        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85)]\n        >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n        [Row(age=2, name=u'Alice', height=80), Row(age=2, nam", "func_name": "DataFrame.crossJoin", "language": "python"}, {"code": "def join(self, other, on=None, how=None):\n        \"\"\"Joins with another :class:`DataFrame`, using the given join expression.\n\n        :param other: Right side of the join\n        :param on: a string for the join column name, a list of column names,\n            a join expression (Column), or a list of Columns.\n            If `on` is a string or a list of strings indicating the name of the join column(s),\n            the column(s) must exist on both sides, and this performs an equi-join.\n        :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n            ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n            ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n            ``anti``, ``leftanti`` and ``left_anti``.\n\n        The following performs a full outer join between ``df1`` and ``df2``.\n\n        >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()\n        [Row(name=None, height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]\n\n        >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()\n        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]\n\n        >>> cond = [df.name == df3.name, df.age == df3.age]\n        >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]\n\n        >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n        [Row(name=u'Bob', height=85)]\n\n        >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n        [Row(name=u'Bob', age=5)]\n        \"\"\"\n\n        if on is not None and not isinstance(on, list):\n            on = [on]\n\n        if on is not None:\n            if isinstance(on[0], basestring):\n                on = self._jseq(on)\n            else:\n                assert isinstance(on[0], Column), \"on should be Column or list of Colum", "docstring": "Joins with another :class:`DataFrame`, using the given join expression.\n\n        :param other: Right side of the join\n        :param on: a string for the join column name, a list of column names,\n            a join expression (Column), or a list of Columns.\n            If `on` is a string or a list of strings indicating the name of the join column(s),\n            the column(s) must exist on both sides, and this performs an equi-join.\n        :param how: str, default ``inner``. Must be one of: ``", "func_name": "DataFrame.join", "language": "python"}, {"code": "def sortWithinPartitions(self, *cols, **kwargs):\n        \"\"\"Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n\n        :param cols: list of :class:`Column` or column names to sort by.\n        :param ascending: boolean or list of boolean (default True).\n            Sort ascending vs. descending. Specify list for multiple sort orders.\n            If a list is specified, length of the list must equal length of the `cols`.\n\n        >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  2|Alice|\n        |  5|  Bob|\n        +---+-----+\n        \"\"\"\n        jdf = self._jdf.sortWithinPartitions(self._sort_cols(cols, kwargs))\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n\n        :param cols: list of :class:`Column` or column names to sort by.\n        :param ascending: boolean or list of boolean (default True).\n            Sort ascending vs. descending. Specify list for multiple sort orders.\n            If a list is specified, length of the list must equal length of the `cols`.\n\n        >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n        +---+-----+\n        |ag", "func_name": "DataFrame.sortWithinPartitions", "language": "python"}, {"code": "def _jseq(self, cols, converter=None):\n        \"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\n        return _to_seq(self.sql_ctx._sc, cols, converter)", "docstring": "Return a JVM Seq of Columns from a list of Column or names", "func_name": "DataFrame._jseq", "language": "python"}, {"code": "def _jcols(self, *cols):\n        \"\"\"Return a JVM Seq of Columns from a list of Column or column names\n\n        If `cols` has only one list in it, cols[0] will be used as the list.\n        \"\"\"\n        if len(cols) == 1 and isinstance(cols[0], list):\n            cols = cols[0]\n        return self._jseq(cols, _to_java_column)", "docstring": "Return a JVM Seq of Columns from a list of Column or column names\n\n        If `cols` has only one list in it, cols[0] will be used as the list.", "func_name": "DataFrame._jcols", "language": "python"}, {"code": "def _sort_cols(self, cols, kwargs):\n        \"\"\" Return a JVM Seq of Columns that describes the sort order\n        \"\"\"\n        if not cols:\n            raise ValueError(\"should sort by at least one column\")\n        if len(cols) == 1 and isinstance(cols[0], list):\n            cols = cols[0]\n        jcols = [_to_java_column(c) for c in cols]\n        ascending = kwargs.get('ascending', True)\n        if isinstance(ascending, (bool, int)):\n            if not ascending:\n                jcols = [jc.desc() for jc in jcols]\n        elif isinstance(ascending, list):\n            jcols = [jc if asc else jc.desc()\n                     for asc, jc in zip(ascending, jcols)]\n        else:\n            raise TypeError(\"ascending can only be boolean or list, but got %s\" % type(ascending))\n        return self._jseq(jcols)", "docstring": "Return a JVM Seq of Columns that describes the sort order", "func_name": "DataFrame._sort_cols", "language": "python"}, {"code": "def describe(self, *cols):\n        \"\"\"Computes basic statistics for numeric and string columns.\n\n        This include count, mean, stddev, min, and max. If no columns are\n        given, this function computes statistics for all numerical or string columns.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        >>> df.describe(['age']).show()\n        +-------+------------------+\n        |summary|               age|\n        +-------+------------------+\n        |  count|                 2|\n        |   mean|               3.5|\n        | stddev|2.1213203435596424|\n        |    min|                 2|\n        |    max|                 5|\n        +-------+------------------+\n        >>> df.describe().show()\n        +-------+------------------+-----+\n        |summary|               age| name|\n        +-------+------------------+-----+\n        |  count|                 2|    2|\n        |   mean|               3.5| null|\n        | stddev|2.1213203435596424| null|\n        |    min|                 2|Alice|\n        |    max|                 5|  Bob|\n        +-------+------------------+-----+\n\n        Use summary for expanded statistics and control over which statistics to compute.\n        \"\"\"\n        if len(cols) == 1 and isinstance(cols[0], list):\n            cols = cols[0]\n        jdf = self._jdf.describe(self._jseq(cols))\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Computes basic statistics for numeric and string columns.\n\n        This include count, mean, stddev, min, and max. If no columns are\n        given, this function computes statistics for all numerical or string columns.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        >>> df.describe(['age']).show()\n        +-------+------------------+\n        |summary|  ", "func_name": "DataFrame.describe", "language": "python"}, {"code": "def summary(self, *statistics):\n        \"\"\"Computes specified statistics for numeric and string columns. Available statistics are:\n        - count\n        - mean\n        - stddev\n        - min\n        - max\n        - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n\n        If no statistics are given, this function computes count, mean, stddev, min,\n        approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        >>> df.summary().show()\n        +-------+------------------+-----+\n        |summary|               age| name|\n        +-------+------------------+-----+\n        |  count|                 2|    2|\n        |   mean|               3.5| null|\n        | stddev|2.1213203435596424| null|\n        |    min|                 2|Alice|\n        |    25%|                 2| null|\n        |    50%|                 2| null|\n        |    75%|                 5| null|\n        |    max|                 5|  Bob|\n        +-------+------------------+-----+\n\n        >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n        +-------+---+-----+\n        |summary|age| name|\n        +-------+---+-----+\n        |  count|  2|    2|\n        |    min|  2|Alice|\n        |    25%|  2| null|\n        |    75%|  5| null|\n        |    max|  5|  Bob|\n        +-------+---+-----+\n\n        To do a summary for specific columns first select them:\n\n        >>> df.select(\"age\", \"name\").summary(\"count\").show()\n        +-------+---+----+\n        |summary|age|name|\n        +-------+---+----+\n        |  count|  2|   2|\n        +-------+---+----+\n\n        See also describe for basic statistics.\n        \"\"\"\n        if len(statistics) == 1 and isinstance(statistics[0], list):\n            statistics = statistics[0]\n        jdf = self._jdf.summary(self._jseq(statistics))\n        retu", "docstring": "Computes specified statistics for numeric and string columns. Available statistics are:\n        - count\n        - mean\n        - stddev\n        - min\n        - max\n        - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n\n        If no statistics are given, this function computes count, mean, stddev, min,\n        approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n        ", "func_name": "DataFrame.summary", "language": "python"}, {"code": "def head(self, n=None):\n        \"\"\"Returns the first ``n`` rows.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        :param n: int, default 1. Number of rows to return.\n        :return: If n is greater than 1, return a list of :class:`Row`.\n            If n is 1, return a single Row.\n\n        >>> df.head()\n        Row(age=2, name=u'Alice')\n        >>> df.head(1)\n        [Row(age=2, name=u'Alice')]\n        \"\"\"\n        if n is None:\n            rs = self.head(1)\n            return rs[0] if rs else None\n        return self.take(n)", "docstring": "Returns the first ``n`` rows.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        :param n: int, default 1. Number of rows to return.\n        :return: If n is greater than 1, return a list of :class:`Row`.\n            If n is 1, return a single Row.\n\n        >>> df.head()\n        Row(age=2, name=u'Alice')\n        >>> df.head(1)\n        [Row(age=2, name=u'Alice')]", "func_name": "DataFrame.head", "language": "python"}, {"code": "def select(self, *cols):\n        \"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\n\n        :param cols: list of column names (string) or expressions (:class:`Column`).\n            If one of the column names is '*', that column is expanded to include all columns\n            in the current DataFrame.\n\n        >>> df.select('*').collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        >>> df.select('name', 'age').collect()\n        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]\n        >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n        [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\n        \"\"\"\n        jdf = self._jdf.select(self._jcols(*cols))\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Projects a set of expressions and returns a new :class:`DataFrame`.\n\n        :param cols: list of column names (string) or expressions (:class:`Column`).\n            If one of the column names is '*', that column is expanded to include all columns\n            in the current DataFrame.\n\n        >>> df.select('*').collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        >>> df.select('name', 'age').collect()\n        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]\n       ", "func_name": "DataFrame.select", "language": "python"}, {"code": "def selectExpr(self, *expr):\n        \"\"\"Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n\n        This is a variant of :func:`select` that accepts SQL expressions.\n\n        >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n        \"\"\"\n        if len(expr) == 1 and isinstance(expr[0], list):\n            expr = expr[0]\n        jdf = self._jdf.selectExpr(self._jseq(expr))\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n\n        This is a variant of :func:`select` that accepts SQL expressions.\n\n        >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]", "func_name": "DataFrame.selectExpr", "language": "python"}, {"code": "def filter(self, condition):\n        \"\"\"Filters rows using the given condition.\n\n        :func:`where` is an alias for :func:`filter`.\n\n        :param condition: a :class:`Column` of :class:`types.BooleanType`\n            or a string of SQL expression.\n\n        >>> df.filter(df.age > 3).collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df.where(df.age == 2).collect()\n        [Row(age=2, name=u'Alice')]\n\n        >>> df.filter(\"age > 3\").collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df.where(\"age = 2\").collect()\n        [Row(age=2, name=u'Alice')]\n        \"\"\"\n        if isinstance(condition, basestring):\n            jdf = self._jdf.filter(condition)\n        elif isinstance(condition, Column):\n            jdf = self._jdf.filter(condition._jc)\n        else:\n            raise TypeError(\"condition should be string or Column\")\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Filters rows using the given condition.\n\n        :func:`where` is an alias for :func:`filter`.\n\n        :param condition: a :class:`Column` of :class:`types.BooleanType`\n            or a string of SQL expression.\n\n        >>> df.filter(df.age > 3).collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df.where(df.age == 2).collect()\n        [Row(age=2, name=u'Alice')]\n\n        >>> df.filter(\"age > 3\").collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df.where(\"age = 2\").collect()\n        ", "func_name": "DataFrame.filter", "language": "python"}, {"code": "def groupBy(self, *cols):\n        \"\"\"Groups the :class:`DataFrame` using the specified columns,\n        so we can run aggregation on them. See :class:`GroupedData`\n        for all the available aggregate functions.\n\n        :func:`groupby` is an alias for :func:`groupBy`.\n\n        :param cols: list of columns to group by.\n            Each element should be a column name (string) or an expression (:class:`Column`).\n\n        >>> df.groupBy().avg().collect()\n        [Row(avg(age)=3.5)]\n        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\n        >>> sorted(df.groupBy(df.name).avg().collect())\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\n        >>> sorted(df.groupBy(['name', df.age]).count().collect())\n        [Row(name=u'Alice', age=2, count=1), Row(name=u'Bob', age=5, count=1)]\n        \"\"\"\n        jgd = self._jdf.groupBy(self._jcols(*cols))\n        from pyspark.sql.group import GroupedData\n        return GroupedData(jgd, self)", "docstring": "Groups the :class:`DataFrame` using the specified columns,\n        so we can run aggregation on them. See :class:`GroupedData`\n        for all the available aggregate functions.\n\n        :func:`groupby` is an alias for :func:`groupBy`.\n\n        :param cols: list of columns to group by.\n            Each element should be a column name (string) or an expression (:class:`Column`).\n\n        >>> df.groupBy().avg().collect()\n        [Row(avg(age)=3.5)]\n        >>> sorted(df.groupBy('name').agg({'age':", "func_name": "DataFrame.groupBy", "language": "python"}, {"code": "def union(self, other):\n        \"\"\" Return a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n        (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        Also as standard in SQL, this function resolves columns by position (not by name).\n        \"\"\"\n        return DataFrame(self._jdf.union(other._jdf), self.sql_ctx)", "docstring": "Return a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n        (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        Also as standard in SQL, this function resolves columns by position (not by name).", "func_name": "DataFrame.union", "language": "python"}, {"code": "def unionByName(self, other):\n        \"\"\" Returns a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n        union (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        The difference between this function and :func:`union` is that this function\n        resolves columns by name (not by position):\n\n        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n        >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n        >>> df1.unionByName(df2).show()\n        +----+----+----+\n        |col0|col1|col2|\n        +----+----+----+\n        |   1|   2|   3|\n        |   6|   4|   5|\n        +----+----+----+\n        \"\"\"\n        return DataFrame(self._jdf.unionByName(other._jdf), self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n        union (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        The difference between this function and :func:`union` is that this function\n        resolves columns by name (not by position):\n\n        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2", "func_name": "DataFrame.unionByName", "language": "python"}, {"code": "def intersect(self, other):\n        \"\"\" Return a new :class:`DataFrame` containing rows only in\n        both this frame and another frame.\n\n        This is equivalent to `INTERSECT` in SQL.\n        \"\"\"\n        return DataFrame(self._jdf.intersect(other._jdf), self.sql_ctx)", "docstring": "Return a new :class:`DataFrame` containing rows only in\n        both this frame and another frame.\n\n        This is equivalent to `INTERSECT` in SQL.", "func_name": "DataFrame.intersect", "language": "python"}, {"code": "def intersectAll(self, other):\n        \"\"\" Return a new :class:`DataFrame` containing rows in both this dataframe and other\n        dataframe while preserving duplicates.\n\n        This is equivalent to `INTERSECT ALL` in SQL.\n        >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n\n        >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n        +---+---+\n        | C1| C2|\n        +---+---+\n        |  a|  1|\n        |  a|  1|\n        |  b|  3|\n        +---+---+\n\n        Also as standard in SQL, this function resolves columns by position (not by name).\n        \"\"\"\n        return DataFrame(self._jdf.intersectAll(other._jdf), self.sql_ctx)", "docstring": "Return a new :class:`DataFrame` containing rows in both this dataframe and other\n        dataframe while preserving duplicates.\n\n        This is equivalent to `INTERSECT ALL` in SQL.\n        >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n\n        >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n        +---+---+\n        | C1| C2|\n        +---+---+\n        |  a|  1|\n    ", "func_name": "DataFrame.intersectAll", "language": "python"}, {"code": "def subtract(self, other):\n        \"\"\" Return a new :class:`DataFrame` containing rows in this frame\n        but not in another frame.\n\n        This is equivalent to `EXCEPT DISTINCT` in SQL.\n\n        \"\"\"\n        return DataFrame(getattr(self._jdf, \"except\")(other._jdf), self.sql_ctx)", "docstring": "Return a new :class:`DataFrame` containing rows in this frame\n        but not in another frame.\n\n        This is equivalent to `EXCEPT DISTINCT` in SQL.", "func_name": "DataFrame.subtract", "language": "python"}, {"code": "def dropDuplicates(self, subset=None):\n        \"\"\"Return a new :class:`DataFrame` with duplicate rows removed,\n        optionally only considering certain columns.\n\n        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n        be and system will accordingly limit the state. In addition, too late data older than\n        watermark will be dropped to avoid any possibility of duplicates.\n\n        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n\n        >>> from pyspark.sql import Row\n        >>> df = sc.parallelize([ \\\\\n        ...     Row(name='Alice', age=5, height=80), \\\\\n        ...     Row(name='Alice', age=5, height=80), \\\\\n        ...     Row(name='Alice', age=10, height=80)]).toDF()\n        >>> df.dropDuplicates().show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        |  5|    80|Alice|\n        | 10|    80|Alice|\n        +---+------+-----+\n\n        >>> df.dropDuplicates(['name', 'height']).show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        |  5|    80|Alice|\n        +---+------+-----+\n        \"\"\"\n        if subset is None:\n            jdf = self._jdf.dropDuplicates()\n        else:\n            jdf = self._jdf.dropDuplicates(self._jseq(subset))\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Return a new :class:`DataFrame` with duplicate rows removed,\n        optionally only considering certain columns.\n\n        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n        be and system will accordingly limit the state. In addition, too late data older than\n  ", "func_name": "DataFrame.dropDuplicates", "language": "python"}, {"code": "def dropna(self, how='any', thresh=None, subset=None):\n        \"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\n        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n\n        :param how: 'any' or 'all'.\n            If 'any', drop a row if it contains any nulls.\n            If 'all', drop a row only if all its values are null.\n        :param thresh: int, default None\n            If specified, drop rows that have less than `thresh` non-null values.\n            This overwrites the `how` parameter.\n        :param subset: optional list of column names to consider.\n\n        >>> df4.na.drop().show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        | 10|    80|Alice|\n        +---+------+-----+\n        \"\"\"\n        if how is not None and how not in ['any', 'all']:\n            raise ValueError(\"how ('\" + how + \"') should be 'any' or 'all'\")\n\n        if subset is None:\n            subset = self.columns\n        elif isinstance(subset, basestring):\n            subset = [subset]\n        elif not isinstance(subset, (list, tuple)):\n            raise ValueError(\"subset should be a list or tuple of column names\")\n\n        if thresh is None:\n            thresh = len(subset) if how == 'any' else 1\n\n        return DataFrame(self._jdf.na().drop(thresh, self._jseq(subset)), self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` omitting rows with null values.\n        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n\n        :param how: 'any' or 'all'.\n            If 'any', drop a row if it contains any nulls.\n            If 'all', drop a row only if all its values are null.\n        :param thresh: int, default None\n            If specified, drop rows that have less than `thresh` non-null values.\n            This overwrites the `how` parameter.\n   ", "func_name": "DataFrame.dropna", "language": "python"}, {"code": "def fillna(self, value, subset=None):\n        \"\"\"Replace null values, alias for ``na.fill()``.\n        :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n\n        :param value: int, long, float, string, bool or dict.\n            Value to replace null values with.\n            If the value is a dict, then `subset` is ignored and `value` must be a mapping\n            from column name (string) to replacement value. The replacement value must be\n            an int, long, float, boolean, or string.\n        :param subset: optional list of column names to consider.\n            Columns specified in subset that do not have matching data type are ignored.\n            For example, if `value` is a string, and subset contains a non-string column,\n            then the non-string column is simply ignored.\n\n        >>> df4.na.fill(50).show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        | 10|    80|Alice|\n        |  5|    50|  Bob|\n        | 50|    50|  Tom|\n        | 50|    50| null|\n        +---+------+-----+\n\n        >>> df5.na.fill(False).show()\n        +----+-------+-----+\n        | age|   name|  spy|\n        +----+-------+-----+\n        |  10|  Alice|false|\n        |   5|    Bob|false|\n        |null|Mallory| true|\n        +----+-------+-----+\n\n        >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n        +---+------+-------+\n        |age|height|   name|\n        +---+------+-------+\n        | 10|    80|  Alice|\n        |  5|  null|    Bob|\n        | 50|  null|    Tom|\n        | 50|  null|unknown|\n        +---+------+-------+\n        \"\"\"\n        if not isinstance(value, (float, int, long, basestring, bool, dict)):\n            raise ValueError(\"value should be a float, int, long, string, bool or dict\")\n\n        # Note that bool validates isinstance(int), but we don't want to\n        # convert bools to floats\n\n        if not isinstance(value, bool) and isinstance(value, (int, long)):\n       ", "docstring": "Replace null values, alias for ``na.fill()``.\n        :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n\n        :param value: int, long, float, string, bool or dict.\n            Value to replace null values with.\n            If the value is a dict, then `subset` is ignored and `value` must be a mapping\n            from column name (string) to replacement value. The replacement value must be\n            an int, long, float, boolean, or string.\n        :par", "func_name": "DataFrame.fillna", "language": "python"}, {"code": "def replace(self, to_replace, value=_NoValue, subset=None):\n        \"\"\"Returns a new :class:`DataFrame` replacing a value with another value.\n        :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n        aliases of each other.\n        Values to_replace and value must have the same type and can only be numerics, booleans,\n        or strings. Value can have None. When replacing, the new value will be cast\n        to the type of the existing column.\n        For numeric replacements all values to be replaced should have unique\n        floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n        and arbitrary replacement will be used.\n\n        :param to_replace: bool, int, long, float, string, list or dict.\n            Value to be replaced.\n            If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n            must be a mapping between a value and a replacement.\n        :param value: bool, int, long, float, string, list or None.\n            The replacement value must be a bool, int, long, float, string or None. If `value` is a\n            list, `value` should be of the same length and type as `to_replace`.\n            If `value` is a scalar and `to_replace` is a sequence, then `value` is\n            used as a replacement for each item in `to_replace`.\n        :param subset: optional list of column names to consider.\n            Columns specified in subset that do not have matching data type are ignored.\n            For example, if `value` is a string, and subset contains a non-string column,\n            then the non-string column is simply ignored.\n\n        >>> df4.na.replace(10, 20).show()\n        +----+------+-----+\n        | age|height| name|\n        +----+------+-----+\n        |  20|    80|Alice|\n        |   5|  null|  Bob|\n        |null|  null|  Tom|\n        |null|  null| null|\n        +----+------+-----+\n\n        >>> df4.na.replace('Alice', None).show()\n        +----", "docstring": "Returns a new :class:`DataFrame` replacing a value with another value.\n        :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n        aliases of each other.\n        Values to_replace and value must have the same type and can only be numerics, booleans,\n        or strings. Value can have None. When replacing, the new value will be cast\n        to the type of the existing column.\n        For numeric replacements all values to be replaced should have unique\n        floating ", "func_name": "DataFrame.replace", "language": "python"}, {"code": "def approxQuantile(self, col, probabilities, relativeError):\n        \"\"\"\n        Calculates the approximate quantiles of numerical columns of a\n        DataFrame.\n\n        The result of this algorithm has the following deterministic bound:\n        If the DataFrame has N elements and if we request the quantile at\n        probability `p` up to error `err`, then the algorithm will return\n        a sample `x` from the DataFrame so that the *exact* rank of `x` is\n        close to (p * N). More precisely,\n\n          floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n\n        This method implements a variation of the Greenwald-Khanna\n        algorithm (with some speed optimizations). The algorithm was first\n        present in [[https://doi.org/10.1145/375663.375670\n        Space-efficient Online Computation of Quantile Summaries]]\n        by Greenwald and Khanna.\n\n        Note that null values will be ignored in numerical columns before calculation.\n        For columns only containing null values, an empty list is returned.\n\n        :param col: str, list.\n          Can be a single column name, or a list of names for multiple columns.\n        :param probabilities: a list of quantile probabilities\n          Each number must belong to [0, 1].\n          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n        :param relativeError:  The relative target precision to achieve\n          (>= 0). If set to zero, the exact quantiles are computed, which\n          could be very expensive. Note that values greater than 1 are\n          accepted but give the same result as 1.\n        :return:  the approximate quantiles at the given probabilities. If\n          the input `col` is a string, the output is a list of floats. If the\n          input `col` is a list or tuple of strings, the output is also a\n          list, but each element in it is a list of floats, i.e., the output\n          is a list of list of floats.\n\n        .. versionchanged:: 2.2\n           Added suppo", "docstring": "Calculates the approximate quantiles of numerical columns of a\n        DataFrame.\n\n        The result of this algorithm has the following deterministic bound:\n        If the DataFrame has N elements and if we request the quantile at\n        probability `p` up to error `err`, then the algorithm will return\n        a sample `x` from the DataFrame so that the *exact* rank of `x` is\n        close to (p * N). More precisely,\n\n          floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n\n        ", "func_name": "DataFrame.approxQuantile", "language": "python"}, {"code": "def corr(self, col1, col2, method=None):\n        \"\"\"\n        Calculates the correlation of two columns of a DataFrame as a double value.\n        Currently only supports the Pearson Correlation Coefficient.\n        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n\n        :param col1: The name of the first column\n        :param col2: The name of the second column\n        :param method: The correlation method. Currently only supports \"pearson\"\n        \"\"\"\n        if not isinstance(col1, basestring):\n            raise ValueError(\"col1 should be a string.\")\n        if not isinstance(col2, basestring):\n            raise ValueError(\"col2 should be a string.\")\n        if not method:\n            method = \"pearson\"\n        if not method == \"pearson\":\n            raise ValueError(\"Currently only the calculation of the Pearson Correlation \" +\n                             \"coefficient is supported.\")\n        return self._jdf.stat().corr(col1, col2, method)", "docstring": "Calculates the correlation of two columns of a DataFrame as a double value.\n        Currently only supports the Pearson Correlation Coefficient.\n        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n\n        :param col1: The name of the first column\n        :param col2: The name of the second column\n        :param method: The correlation method. Currently only supports \"pearson\"", "func_name": "DataFrame.corr", "language": "python"}, {"code": "def cov(self, col1, col2):\n        \"\"\"\n        Calculate the sample covariance for the given columns, specified by their names, as a\n        double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n\n        :param col1: The name of the first column\n        :param col2: The name of the second column\n        \"\"\"\n        if not isinstance(col1, basestring):\n            raise ValueError(\"col1 should be a string.\")\n        if not isinstance(col2, basestring):\n            raise ValueError(\"col2 should be a string.\")\n        return self._jdf.stat().cov(col1, col2)", "docstring": "Calculate the sample covariance for the given columns, specified by their names, as a\n        double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n\n        :param col1: The name of the first column\n        :param col2: The name of the second column", "func_name": "DataFrame.cov", "language": "python"}, {"code": "def crosstab(self, col1, col2):\n        \"\"\"\n        Computes a pair-wise frequency table of the given columns. Also known as a contingency\n        table. The number of distinct values for each column should be less than 1e4. At most 1e6\n        non-zero pair frequencies will be returned.\n        The first column of each row will be the distinct values of `col1` and the column names\n        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n        Pairs that have no occurrences will have zero as their counts.\n        :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n\n        :param col1: The name of the first column. Distinct items will make the first item of\n            each row.\n        :param col2: The name of the second column. Distinct items will make the column names\n            of the DataFrame.\n        \"\"\"\n        if not isinstance(col1, basestring):\n            raise ValueError(\"col1 should be a string.\")\n        if not isinstance(col2, basestring):\n            raise ValueError(\"col2 should be a string.\")\n        return DataFrame(self._jdf.stat().crosstab(col1, col2), self.sql_ctx)", "docstring": "Computes a pair-wise frequency table of the given columns. Also known as a contingency\n        table. The number of distinct values for each column should be less than 1e4. At most 1e6\n        non-zero pair frequencies will be returned.\n        The first column of each row will be the distinct values of `col1` and the column names\n        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n        Pairs that have no occurrences will have zero as their count", "func_name": "DataFrame.crosstab", "language": "python"}, {"code": "def freqItems(self, cols, support=None):\n        \"\"\"\n        Finding frequent items for columns, possibly with false positives. Using the\n        frequent element count algorithm described in\n        \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n            strings.\n        :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n            The support must be greater than 1e-4.\n        \"\"\"\n        if isinstance(cols, tuple):\n            cols = list(cols)\n        if not isinstance(cols, list):\n            raise ValueError(\"cols must be a list or tuple of column names as strings.\")\n        if not support:\n            support = 0.01\n        return DataFrame(self._jdf.stat().freqItems(_to_seq(self._sc, cols), support), self.sql_ctx)", "docstring": "Finding frequent items for columns, possibly with false positives. Using the\n        frequent element count algorithm described in\n        \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFr", "func_name": "DataFrame.freqItems", "language": "python"}, {"code": "def withColumn(self, colName, col):\n        \"\"\"\n        Returns a new :class:`DataFrame` by adding a column or replacing the\n        existing column that has the same name.\n\n        The column expression must be an expression over this DataFrame; attempting to add\n        a column from some other dataframe will raise an error.\n\n        :param colName: string, name of the new column.\n        :param col: a :class:`Column` expression for the new column.\n\n        .. note:: This method introduces a projection internally. Therefore, calling it multiple\n            times, for instance, via loops in order to add multiple columns can generate big\n            plans which can cause performance issues and even `StackOverflowException`.\n            To avoid this, use :func:`select` with the multiple columns at once.\n\n        >>> df.withColumn('age2', df.age + 2).collect()\n        [Row(age=2, name=u'Alice', age2=4), Row(age=5, name=u'Bob', age2=7)]\n\n        \"\"\"\n        assert isinstance(col, Column), \"col should be Column\"\n        return DataFrame(self._jdf.withColumn(colName, col._jc), self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` by adding a column or replacing the\n        existing column that has the same name.\n\n        The column expression must be an expression over this DataFrame; attempting to add\n        a column from some other dataframe will raise an error.\n\n        :param colName: string, name of the new column.\n        :param col: a :class:`Column` expression for the new column.\n\n        .. note:: This method introduces a projection internally. Therefore, calling it multiple\n   ", "func_name": "DataFrame.withColumn", "language": "python"}, {"code": "def withColumnRenamed(self, existing, new):\n        \"\"\"Returns a new :class:`DataFrame` by renaming an existing column.\n        This is a no-op if schema doesn't contain the given column name.\n\n        :param existing: string, name of the existing column to rename.\n        :param new: string, new name of the column.\n\n        >>> df.withColumnRenamed('age', 'age2').collect()\n        [Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]\n        \"\"\"\n        return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` by renaming an existing column.\n        This is a no-op if schema doesn't contain the given column name.\n\n        :param existing: string, name of the existing column to rename.\n        :param new: string, new name of the column.\n\n        >>> df.withColumnRenamed('age', 'age2').collect()\n        [Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]", "func_name": "DataFrame.withColumnRenamed", "language": "python"}, {"code": "def drop(self, *cols):\n        \"\"\"Returns a new :class:`DataFrame` that drops the specified column.\n        This is a no-op if schema doesn't contain the given column name(s).\n\n        :param cols: a string name of the column to drop, or a\n            :class:`Column` to drop, or a list of string name of the columns to drop.\n\n        >>> df.drop('age').collect()\n        [Row(name=u'Alice'), Row(name=u'Bob')]\n\n        >>> df.drop(df.age).collect()\n        [Row(name=u'Alice'), Row(name=u'Bob')]\n\n        >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n        [Row(age=5, height=85, name=u'Bob')]\n\n        >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n        [Row(age=5, name=u'Bob', height=85)]\n\n        >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n        [Row(name=u'Bob')]\n        \"\"\"\n        if len(cols) == 1:\n            col = cols[0]\n            if isinstance(col, basestring):\n                jdf = self._jdf.drop(col)\n            elif isinstance(col, Column):\n                jdf = self._jdf.drop(col._jc)\n            else:\n                raise TypeError(\"col should be a string or a Column\")\n        else:\n            for col in cols:\n                if not isinstance(col, basestring):\n                    raise TypeError(\"each col in the param list should be a string\")\n            jdf = self._jdf.drop(self._jseq(cols))\n\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Returns a new :class:`DataFrame` that drops the specified column.\n        This is a no-op if schema doesn't contain the given column name(s).\n\n        :param cols: a string name of the column to drop, or a\n            :class:`Column` to drop, or a list of string name of the columns to drop.\n\n        >>> df.drop('age').collect()\n        [Row(name=u'Alice'), Row(name=u'Bob')]\n\n        >>> df.drop(df.age).collect()\n        [Row(name=u'Alice'), Row(name=u'Bob')]\n\n        >>> df.join(df2, df.name == ", "func_name": "DataFrame.drop", "language": "python"}, {"code": "def toDF(self, *cols):\n        \"\"\"Returns a new class:`DataFrame` that with new specified column names\n\n        :param cols: list of new column names (string)\n\n        >>> df.toDF('f1', 'f2').collect()\n        [Row(f1=2, f2=u'Alice'), Row(f1=5, f2=u'Bob')]\n        \"\"\"\n        jdf = self._jdf.toDF(self._jseq(cols))\n        return DataFrame(jdf, self.sql_ctx)", "docstring": "Returns a new class:`DataFrame` that with new specified column names\n\n        :param cols: list of new column names (string)\n\n        >>> df.toDF('f1', 'f2').collect()\n        [Row(f1=2, f2=u'Alice'), Row(f1=5, f2=u'Bob')]", "func_name": "DataFrame.toDF", "language": "python"}, {"code": "def transform(self, func):\n        \"\"\"Returns a new class:`DataFrame`. Concise syntax for chaining custom transformations.\n\n        :param func: a function that takes and returns a class:`DataFrame`.\n\n        >>> from pyspark.sql.functions import col\n        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n        >>> def cast_all_to_int(input_df):\n        ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n        >>> def sort_columns_asc(input_df):\n        ...     return input_df.select(*sorted(input_df.columns))\n        >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n        +-----+---+\n        |float|int|\n        +-----+---+\n        |    1|  1|\n        |    2|  2|\n        +-----+---+\n        \"\"\"\n        result = func(self)\n        assert isinstance(result, DataFrame), \"Func returned an instance of type [%s], \" \\\n                                              \"should have been DataFrame.\" % type(result)\n        return result", "docstring": "Returns a new class:`DataFrame`. Concise syntax for chaining custom transformations.\n\n        :param func: a function that takes and returns a class:`DataFrame`.\n\n        >>> from pyspark.sql.functions import col\n        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n        >>> def cast_all_to_int(input_df):\n        ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n        >>> def sort_columns_asc(input_df):\n        ...     retu", "func_name": "DataFrame.transform", "language": "python"}, {"code": "def toPandas(self):\n        \"\"\"\n        Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n\n        This is only available if Pandas is installed and available.\n\n        .. note:: This method should only be used if the resulting Pandas's DataFrame is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n\n        >>> df.toPandas()  # doctest: +SKIP\n           age   name\n        0    2  Alice\n        1    5    Bob\n        \"\"\"\n        from pyspark.sql.utils import require_minimum_pandas_version\n        require_minimum_pandas_version()\n\n        import pandas as pd\n\n        if self.sql_ctx._conf.pandasRespectSessionTimeZone():\n            timezone = self.sql_ctx._conf.sessionLocalTimeZone()\n        else:\n            timezone = None\n\n        if self.sql_ctx._conf.arrowEnabled():\n            use_arrow = True\n            try:\n                from pyspark.sql.types import to_arrow_schema\n                from pyspark.sql.utils import require_minimum_pyarrow_version\n\n                require_minimum_pyarrow_version()\n                to_arrow_schema(self.schema)\n            except Exception as e:\n\n                if self.sql_ctx._conf.arrowFallbackEnabled():\n                    msg = (\n                        \"toPandas attempted Arrow optimization because \"\n                        \"'spark.sql.execution.arrow.enabled' is set to true; however, \"\n                        \"failed by the reason below:\\n  %s\\n\"\n                        \"Attempting non-optimization as \"\n                        \"'spark.sql.execution.arrow.fallback.enabled' is set to \"\n                        \"true.\" % _exception_message(e))\n                    warnings.warn(msg)\n                    use_arrow = False\n                else:\n                    msg = (\n                        \"toPandas attempted Arrow optimization because \"\n                        \"'spark.sql.execu", "docstring": "Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n\n        This is only available if Pandas is installed and available.\n\n        .. note:: This method should only be used if the resulting Pandas's DataFrame is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n\n        >>> df.toPandas()  # doctest: +SKIP\n           age   name\n        0    2  Alice", "func_name": "DataFrame.toPandas", "language": "python"}, {"code": "def _collectAsArrow(self):\n        \"\"\"\n        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed\n        and available on driver and worker Python environments.\n\n        .. note:: Experimental.\n        \"\"\"\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.collectAsArrowToPython()\n\n        # Collect list of un-ordered batches where last element is a list of correct order indices\n        results = list(_load_from_socket(sock_info, ArrowCollectSerializer()))\n        batches = results[:-1]\n        batch_order = results[-1]\n\n        # Re-order the batch list using the correct order\n        return [batches[i] for i in batch_order]", "docstring": "Returns all records as a list of ArrowRecordBatches, pyarrow must be installed\n        and available on driver and worker Python environments.\n\n        .. note:: Experimental.", "func_name": "DataFrame._collectAsArrow", "language": "python"}, {"code": "def asDict(self, sample=False):\n        \"\"\"Returns the :class:`StatCounter` members as a ``dict``.\n\n        >>> sc.parallelize([1., 2., 3., 4.]).stats().asDict()\n        {'count': 4L,\n         'max': 4.0,\n         'mean': 2.5,\n         'min': 1.0,\n         'stdev': 1.2909944487358056,\n         'sum': 10.0,\n         'variance': 1.6666666666666667}\n        \"\"\"\n        return {\n            'count': self.count(),\n            'mean': self.mean(),\n            'sum': self.sum(),\n            'min': self.min(),\n            'max': self.max(),\n            'stdev': self.stdev() if sample else self.sampleStdev(),\n            'variance': self.variance() if sample else self.sampleVariance()\n        }", "docstring": "Returns the :class:`StatCounter` members as a ``dict``.\n\n        >>> sc.parallelize([1., 2., 3., 4.]).stats().asDict()\n        {'count': 4L,\n         'max': 4.0,\n         'mean': 2.5,\n         'min': 1.0,\n         'stdev': 1.2909944487358056,\n         'sum': 10.0,\n         'variance': 1.6666666666666667}", "func_name": "StatCounter.asDict", "language": "python"}, {"code": "def _list_function_infos(jvm):\n    \"\"\"\n    Returns a list of function information via JVM. Sorts wrapped expression infos by name\n    and returns them.\n    \"\"\"\n\n    jinfos = jvm.org.apache.spark.sql.api.python.PythonSQLUtils.listBuiltinFunctionInfos()\n    infos = []\n    for jinfo in jinfos:\n        name = jinfo.getName()\n        usage = jinfo.getUsage()\n        usage = usage.replace(\"_FUNC_\", name) if usage is not None else usage\n        infos.append(ExpressionInfo(\n            className=jinfo.getClassName(),\n            name=name,\n            usage=usage,\n            arguments=jinfo.getArguments().replace(\"_FUNC_\", name),\n            examples=jinfo.getExamples().replace(\"_FUNC_\", name),\n            note=jinfo.getNote(),\n            since=jinfo.getSince(),\n            deprecated=jinfo.getDeprecated()))\n    return sorted(infos, key=lambda i: i.name)", "docstring": "Returns a list of function information via JVM. Sorts wrapped expression infos by name\n    and returns them.", "func_name": "_list_function_infos", "language": "python"}, {"code": "def _make_pretty_usage(usage):\n    \"\"\"\n    Makes the usage description pretty and returns a formatted string if `usage`\n    is not an empty string. Otherwise, returns None.\n    \"\"\"\n\n    if usage is not None and usage.strip() != \"\":\n        usage = \"\\n\".join(map(lambda u: u.strip(), usage.split(\"\\n\")))\n        return \"%s\\n\\n\" % usage", "docstring": "Makes the usage description pretty and returns a formatted string if `usage`\n    is not an empty string. Otherwise, returns None.", "func_name": "_make_pretty_usage", "language": "python"}, {"code": "def _make_pretty_arguments(arguments):\n    \"\"\"\n    Makes the arguments description pretty and returns a formatted string if `arguments`\n    starts with the argument prefix. Otherwise, returns None.\n\n    Expected input:\n\n        Arguments:\n          * arg0 - ...\n              ...\n          * arg0 - ...\n              ...\n\n    Expected output:\n    **Arguments:**\n\n    * arg0 - ...\n        ...\n    * arg0 - ...\n        ...\n\n    \"\"\"\n\n    if arguments.startswith(\"\\n    Arguments:\"):\n        arguments = \"\\n\".join(map(lambda u: u[6:], arguments.strip().split(\"\\n\")[1:]))\n        return \"**Arguments:**\\n\\n%s\\n\\n\" % arguments", "docstring": "Makes the arguments description pretty and returns a formatted string if `arguments`\n    starts with the argument prefix. Otherwise, returns None.\n\n    Expected input:\n\n        Arguments:\n          * arg0 - ...\n              ...\n          * arg0 - ...\n              ...\n\n    Expected output:\n    **Arguments:**\n\n    * arg0 - ...\n        ...\n    * arg0 - ...\n        ...", "func_name": "_make_pretty_arguments", "language": "python"}, {"code": "def _make_pretty_examples(examples):\n    \"\"\"\n    Makes the examples description pretty and returns a formatted string if `examples`\n    starts with the example prefix. Otherwise, returns None.\n\n    Expected input:\n\n        Examples:\n          > SELECT ...;\n           ...\n          > SELECT ...;\n           ...\n\n    Expected output:\n    **Examples:**\n\n    ```\n    > SELECT ...;\n     ...\n    > SELECT ...;\n     ...\n    ```\n\n    \"\"\"\n\n    if examples.startswith(\"\\n    Examples:\"):\n        examples = \"\\n\".join(map(lambda u: u[6:], examples.strip().split(\"\\n\")[1:]))\n        return \"**Examples:**\\n\\n```\\n%s\\n```\\n\\n\" % examples", "docstring": "Makes the examples description pretty and returns a formatted string if `examples`\n    starts with the example prefix. Otherwise, returns None.\n\n    Expected input:\n\n        Examples:\n          > SELECT ...;\n           ...\n          > SELECT ...;\n           ...\n\n    Expected output:\n    **Examples:**\n\n    ```\n    > SELECT ...;\n     ...\n    > SELECT ...;\n     ...\n    ```", "func_name": "_make_pretty_examples", "language": "python"}, {"code": "def _make_pretty_note(note):\n    \"\"\"\n    Makes the note description pretty and returns a formatted string if `note` is not\n    an empty string. Otherwise, returns None.\n\n    Expected input:\n\n        ...\n\n    Expected output:\n    **Note:**\n\n    ...\n\n    \"\"\"\n\n    if note != \"\":\n        note = \"\\n\".join(map(lambda n: n[4:], note.split(\"\\n\")))\n        return \"**Note:**\\n%s\\n\" % note", "docstring": "Makes the note description pretty and returns a formatted string if `note` is not\n    an empty string. Otherwise, returns None.\n\n    Expected input:\n\n        ...\n\n    Expected output:\n    **Note:**\n\n    ...", "func_name": "_make_pretty_note", "language": "python"}, {"code": "def _make_pretty_deprecated(deprecated):\n    \"\"\"\n    Makes the deprecated description pretty and returns a formatted string if `deprecated`\n    is not an empty string. Otherwise, returns None.\n\n    Expected input:\n\n        ...\n\n    Expected output:\n    **Deprecated:**\n\n    ...\n\n    \"\"\"\n\n    if deprecated != \"\":\n        deprecated = \"\\n\".join(map(lambda n: n[4:], deprecated.split(\"\\n\")))\n        return \"**Deprecated:**\\n%s\\n\" % deprecated", "docstring": "Makes the deprecated description pretty and returns a formatted string if `deprecated`\n    is not an empty string. Otherwise, returns None.\n\n    Expected input:\n\n        ...\n\n    Expected output:\n    **Deprecated:**\n\n    ...", "func_name": "_make_pretty_deprecated", "language": "python"}, {"code": "def generate_sql_markdown(jvm, path):\n    \"\"\"\n    Generates a markdown file after listing the function information. The output file\n    is created in `path`.\n\n    Expected output:\n    ### NAME\n\n    USAGE\n\n    **Arguments:**\n\n    ARGUMENTS\n\n    **Examples:**\n\n    ```\n    EXAMPLES\n    ```\n\n    **Note:**\n\n    NOTE\n\n    **Since:** SINCE\n\n    **Deprecated:**\n\n    DEPRECATED\n\n    <br/>\n\n    \"\"\"\n\n    with open(path, 'w') as mdfile:\n        for info in _list_function_infos(jvm):\n            name = info.name\n            usage = _make_pretty_usage(info.usage)\n            arguments = _make_pretty_arguments(info.arguments)\n            examples = _make_pretty_examples(info.examples)\n            note = _make_pretty_note(info.note)\n            since = info.since\n            deprecated = _make_pretty_deprecated(info.deprecated)\n\n            mdfile.write(\"### %s\\n\\n\" % name)\n            if usage is not None:\n                mdfile.write(\"%s\\n\\n\" % usage.strip())\n            if arguments is not None:\n                mdfile.write(arguments)\n            if examples is not None:\n                mdfile.write(examples)\n            if note is not None:\n                mdfile.write(note)\n            if since is not None and since != \"\":\n                mdfile.write(\"**Since:** %s\\n\\n\" % since.strip())\n            if deprecated is not None:\n                mdfile.write(deprecated)\n            mdfile.write(\"<br/>\\n\\n\")", "docstring": "Generates a markdown file after listing the function information. The output file\n    is created in `path`.\n\n    Expected output:\n    ### NAME\n\n    USAGE\n\n    **Arguments:**\n\n    ARGUMENTS\n\n    **Examples:**\n\n    ```\n    EXAMPLES\n    ```\n\n    **Note:**\n\n    NOTE\n\n    **Since:** SINCE\n\n    **Deprecated:**\n\n    DEPRECATED\n\n    <br/>", "func_name": "generate_sql_markdown", "language": "python"}, {"code": "def predict(self, x):\n        \"\"\"\n        Predict values for a single data point or an RDD of points\n        using the model trained.\n        \"\"\"\n        if isinstance(x, RDD):\n            return x.map(lambda v: self.predict(v))\n\n        x = _convert_to_vector(x)\n        if self.numClasses == 2:\n            margin = self.weights.dot(x) + self._intercept\n            if margin > 0:\n                prob = 1 / (1 + exp(-margin))\n            else:\n                exp_margin = exp(margin)\n                prob = exp_margin / (1 + exp_margin)\n            if self._threshold is None:\n                return prob\n            else:\n                return 1 if prob > self._threshold else 0\n        else:\n            best_class = 0\n            max_margin = 0.0\n            if x.size + 1 == self._dataWithBiasSize:\n                for i in range(0, self._numClasses - 1):\n                    margin = x.dot(self._weightsMatrix[i][0:x.size]) + \\\n                        self._weightsMatrix[i][x.size]\n                    if margin > max_margin:\n                        max_margin = margin\n                        best_class = i + 1\n            else:\n                for i in range(0, self._numClasses - 1):\n                    margin = x.dot(self._weightsMatrix[i])\n                    if margin > max_margin:\n                        max_margin = margin\n                        best_class = i + 1\n            return best_class", "docstring": "Predict values for a single data point or an RDD of points\n        using the model trained.", "func_name": "LogisticRegressionModel.predict", "language": "python"}, {"code": "def save(self, sc, path):\n        \"\"\"\n        Save this model to the given path.\n        \"\"\"\n        java_model = sc._jvm.org.apache.spark.mllib.classification.LogisticRegressionModel(\n            _py2java(sc, self._coeff), self.intercept, self.numFeatures, self.numClasses)\n        java_model.save(sc._jsc.sc(), path)", "docstring": "Save this model to the given path.", "func_name": "LogisticRegressionModel.save", "language": "python"}, {"code": "def train(cls, data, iterations=100, initialWeights=None, regParam=0.0, regType=\"l2\",\n              intercept=False, corrections=10, tolerance=1e-6, validateData=True, numClasses=2):\n        \"\"\"\n        Train a logistic regression model on the given data.\n\n        :param data:\n          The training data, an RDD of LabeledPoint.\n        :param iterations:\n          The number of iterations.\n          (default: 100)\n        :param initialWeights:\n          The initial weights.\n          (default: None)\n        :param regParam:\n          The regularizer parameter.\n          (default: 0.0)\n        :param regType:\n          The type of regularizer used for training our model.\n          Supported values:\n\n            - \"l1\" for using L1 regularization\n            - \"l2\" for using L2 regularization (default)\n            - None for no regularization\n        :param intercept:\n          Boolean parameter which indicates the use or not of the\n          augmented representation for training data (i.e., whether bias\n          features are activated or not).\n          (default: False)\n        :param corrections:\n          The number of corrections used in the LBFGS update.\n          If a known updater is used for binary classification,\n          it calls the ml implementation and this parameter will\n          have no effect. (default: 10)\n        :param tolerance:\n          The convergence tolerance of iterations for L-BFGS.\n          (default: 1e-6)\n        :param validateData:\n          Boolean parameter which indicates if the algorithm should\n          validate data before training.\n          (default: True)\n        :param numClasses:\n          The number of classes (i.e., outcomes) a label can take in\n          Multinomial Logistic Regression.\n          (default: 2)\n\n        >>> data = [\n        ...     LabeledPoint(0.0, [0.0, 1.0]),\n        ...     LabeledPoint(1.0, [1.0, 0.0]),\n        ... ]\n        >>> lrm = LogisticRegressionWithLBFGS.train(sc.parallelize(data), iteratio", "docstring": "Train a logistic regression model on the given data.\n\n        :param data:\n          The training data, an RDD of LabeledPoint.\n        :param iterations:\n          The number of iterations.\n          (default: 100)\n        :param initialWeights:\n          The initial weights.\n          (default: None)\n        :param regParam:\n          The regularizer parameter.\n          (default: 0.0)\n        :param regType:\n          The type of regularizer used for training our model.\n          Supported va", "func_name": "LogisticRegressionWithLBFGS.train", "language": "python"}, {"code": "def predict(self, x):\n        \"\"\"\n        Predict values for a single data point or an RDD of points\n        using the model trained.\n        \"\"\"\n        if isinstance(x, RDD):\n            return x.map(lambda v: self.predict(v))\n\n        x = _convert_to_vector(x)\n        margin = self.weights.dot(x) + self.intercept\n        if self._threshold is None:\n            return margin\n        else:\n            return 1 if margin > self._threshold else 0", "docstring": "Predict values for a single data point or an RDD of points\n        using the model trained.", "func_name": "SVMModel.predict", "language": "python"}, {"code": "def save(self, sc, path):\n        \"\"\"\n        Save this model to the given path.\n        \"\"\"\n        java_model = sc._jvm.org.apache.spark.mllib.classification.SVMModel(\n            _py2java(sc, self._coeff), self.intercept)\n        java_model.save(sc._jsc.sc(), path)", "docstring": "Save this model to the given path.", "func_name": "SVMModel.save", "language": "python"}, {"code": "def load(cls, sc, path):\n        \"\"\"\n        Load a model from the given path.\n        \"\"\"\n        java_model = sc._jvm.org.apache.spark.mllib.classification.SVMModel.load(\n            sc._jsc.sc(), path)\n        weights = _java2py(sc, java_model.weights())\n        intercept = java_model.intercept()\n        threshold = java_model.getThreshold().get()\n        model = SVMModel(weights, intercept)\n        model.setThreshold(threshold)\n        return model", "docstring": "Load a model from the given path.", "func_name": "SVMModel.load", "language": "python"}, {"code": "def train(cls, data, lambda_=1.0):\n        \"\"\"\n        Train a Naive Bayes model given an RDD of (label, features)\n        vectors.\n\n        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which\n        can handle all kinds of discrete data.  For example, by\n        converting documents into TF-IDF vectors, it can be used for\n        document classification. By making every vector a 0-1 vector,\n        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).\n        The input feature values must be nonnegative.\n\n        :param data:\n          RDD of LabeledPoint.\n        :param lambda_:\n          The smoothing parameter.\n          (default: 1.0)\n        \"\"\"\n        first = data.first()\n        if not isinstance(first, LabeledPoint):\n            raise ValueError(\"`data` should be an RDD of LabeledPoint\")\n        labels, pi, theta = callMLlibFunc(\"trainNaiveBayesModel\", data, lambda_)\n        return NaiveBayesModel(labels.toArray(), pi.toArray(), numpy.array(theta))", "docstring": "Train a Naive Bayes model given an RDD of (label, features)\n        vectors.\n\n        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which\n        can handle all kinds of discrete data.  For example, by\n        converting documents into TF-IDF vectors, it can be used for\n        document classification. By making every vector a 0-1 vector,\n        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).\n        The input feature values must be nonnegative.\n\n        :param ", "func_name": "NaiveBayes.train", "language": "python"}, {"code": "def heappush(heap, item):\n    \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\"\n    heap.append(item)\n    _siftdown(heap, 0, len(heap)-1)", "docstring": "Push item onto heap, maintaining the heap invariant.", "func_name": "heappush", "language": "python"}, {"code": "def heappop(heap):\n    \"\"\"Pop the smallest item off the heap, maintaining the heap invariant.\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup(heap, 0)\n        return returnitem\n    return lastelt", "docstring": "Pop the smallest item off the heap, maintaining the heap invariant.", "func_name": "heappop", "language": "python"}, {"code": "def heapreplace(heap, item):\n    \"\"\"Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    \"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem", "docstring": "Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)", "func_name": "heapreplace", "language": "python"}, {"code": "def heappushpop(heap, item):\n    \"\"\"Fast version of a heappush followed by a heappop.\"\"\"\n    if heap and heap[0] < item:\n        item, heap[0] = heap[0], item\n        _siftup(heap, 0)\n    return item", "docstring": "Fast version of a heappush followed by a heappop.", "func_name": "heappushpop", "language": "python"}, {"code": "def heapify(x):\n    \"\"\"Transform list into a heap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    # Transform bottom-up.  The largest index there's any point to looking at\n    # is the largest with a child index in-range, so must have 2*i + 1 < n,\n    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\n    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is\n    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.\n    for i in reversed(range(n//2)):\n        _siftup(x, i)", "docstring": "Transform list into a heap, in-place, in O(len(x)) time.", "func_name": "heapify", "language": "python"}, {"code": "def _heappop_max(heap):\n    \"\"\"Maxheap version of a heappop.\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup_max(heap, 0)\n        return returnitem\n    return lastelt", "docstring": "Maxheap version of a heappop.", "func_name": "_heappop_max", "language": "python"}, {"code": "def _heapreplace_max(heap, item):\n    \"\"\"Maxheap version of a heappop followed by a heappush.\"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup_max(heap, 0)\n    return returnitem", "docstring": "Maxheap version of a heappop followed by a heappush.", "func_name": "_heapreplace_max", "language": "python"}, {"code": "def _heapify_max(x):\n    \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)", "docstring": "Transform list into a maxheap, in-place, in O(len(x)) time.", "func_name": "_heapify_max", "language": "python"}, {"code": "def _siftdown_max(heap, startpos, pos):\n    'Maxheap variant of _siftdown'\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if parent < newitem:\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem", "docstring": "Maxheap variant of _siftdown", "func_name": "_siftdown_max", "language": "python"}, {"code": "def _siftup_max(heap, pos):\n    'Maxheap variant of _siftup'\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the larger child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of larger child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not heap[rightpos] < heap[childpos]:\n            childpos = rightpos\n        # Move the larger child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown_max(heap, startpos, pos)", "docstring": "Maxheap variant of _siftup", "func_name": "_siftup_max", "language": "python"}, {"code": "def merge(iterables, key=None, reverse=False):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    If *key* is not None, applies a key function to each element to determine\n    its sort order.\n\n    >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))\n    ['dog', 'cat', 'fish', 'horse', 'kangaroo']\n\n    '''\n\n    h = []\n    h_append = h.append\n\n    if reverse:\n        _heapify = _heapify_max\n        _heappop = _heappop_max\n        _heapreplace = _heapreplace_max\n        direction = -1\n    else:\n        _heapify = heapify\n        _heappop = heappop\n        _heapreplace = heapreplace\n        direction = 1\n\n    if key is None:\n        for order, it in enumerate(map(iter, iterables)):\n            try:\n                h_append([next(it), order * direction, it])\n            except StopIteration:\n                pass\n        _heapify(h)\n        while len(h) > 1:\n            try:\n                while True:\n                    value, order, it = s = h[0]\n                    yield value\n                    s[0] = next(it)           # raises StopIteration when exhausted\n                    _heapreplace(h, s)      # restore heap condition\n            except StopIteration:\n                _heappop(h)                 # remove empty iterator\n        if h:\n            # fast case when only a single iterator remains\n            value, order, it = h[0]\n            yield value\n            for value in it:\n                yield value\n        return\n\n    for order, it in enumerate(map(iter, iterables)):\n        try:\n            value = next(it)\n            h_append([key(value), order * direction, value, it])\n        except St", "docstring": "Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    If *key* is not None, applies a key function to each element to determine\n    its sort order.\n\n    >>> li", "func_name": "merge", "language": "python"}, {"code": "def nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use min()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = min(it, default=sentinel)\n        else:\n            result = min(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        # put the range(n) first so that zip() doesn't\n        # consume one too many elements from the iterator\n        result = [(elem, i) for i, elem in zip(range(n), it)]\n        if not result:\n            return result\n        _heapify_max(result)\n        top = result[0][0]\n        order = n\n        _heapreplace = _heapreplace_max\n        for elem in it:\n            if elem < top:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order += 1\n        result.sort()\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(n), it)]\n    if not result:\n        return result\n    _heapify_max(result)\n    top = result[0][0]\n    order = n\n    _heapreplace = _heapreplace_max\n    for elem in it:\n        k = key(elem)\n        if k < top:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order += 1\n    result.sort()\n    return [r[2] for r in result]", "docstring": "Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]", "func_name": "nsmallest", "language": "python"}, {"code": "def nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = max(it, default=sentinel)\n        else:\n            result = max(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        result = [(elem, i) for i, elem in zip(range(0, -n, -1), it)]\n        if not result:\n            return result\n        heapify(result)\n        top = result[0][0]\n        order = -n\n        _heapreplace = heapreplace\n        for elem in it:\n            if top < elem:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order -= 1\n        result.sort(reverse=True)\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]\n    if not result:\n        return result\n    heapify(result)\n    top = result[0][0]\n    order = -n\n    _heapreplace = heapreplace\n    for elem in it:\n        k = key(elem)\n        if top < k:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order -= 1\n    result.sort(reverse=True)\n    return [r[2] for r in result]", "docstring": "Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]", "func_name": "nlargest", "language": "python"}, {"code": "def corr(dataset, column, method=\"pearson\"):\n        \"\"\"\n        Compute the correlation matrix with specified method using dataset.\n\n        :param dataset:\n          A Dataset or a DataFrame.\n        :param column:\n          The name of the column of vectors for which the correlation coefficient needs\n          to be computed. This must be a column of the dataset, and it must contain\n          Vector objects.\n        :param method:\n          String specifying the method to use for computing correlation.\n          Supported: `pearson` (default), `spearman`.\n        :return:\n          A DataFrame that contains the correlation matrix of the column of vectors. This\n          DataFrame contains a single row and a single column of name\n          '$METHODNAME($COLUMN)'.\n\n        >>> from pyspark.ml.linalg import Vectors\n        >>> from pyspark.ml.stat import Correlation\n        >>> dataset = [[Vectors.dense([1, 0, 0, -2])],\n        ...            [Vectors.dense([4, 5, 0, 3])],\n        ...            [Vectors.dense([6, 7, 0, 8])],\n        ...            [Vectors.dense([9, 0, 0, 1])]]\n        >>> dataset = spark.createDataFrame(dataset, ['features'])\n        >>> pearsonCorr = Correlation.corr(dataset, 'features', 'pearson').collect()[0][0]\n        >>> print(str(pearsonCorr).replace('nan', 'NaN'))\n        DenseMatrix([[ 1.        ,  0.0556...,         NaN,  0.4004...],\n                     [ 0.0556...,  1.        ,         NaN,  0.9135...],\n                     [        NaN,         NaN,  1.        ,         NaN],\n                     [ 0.4004...,  0.9135...,         NaN,  1.        ]])\n        >>> spearmanCorr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0]\n        >>> print(str(spearmanCorr).replace('nan', 'NaN'))\n        DenseMatrix([[ 1.        ,  0.1054...,         NaN,  0.4       ],\n                     [ 0.1054...,  1.        ,         NaN,  0.9486... ],\n                     [        NaN,         NaN,  1.        ,         NaN],\n           ", "docstring": "Compute the correlation matrix with specified method using dataset.\n\n        :param dataset:\n          A Dataset or a DataFrame.\n        :param column:\n          The name of the column of vectors for which the correlation coefficient needs\n          to be computed. This must be a column of the dataset, and it must contain\n          Vector objects.\n        :param method:\n          String specifying the method to use for computing correlation.\n          Supported: `pearson` (default), `spearman`.\n", "func_name": "Correlation.corr", "language": "python"}, {"code": "def metrics(*metrics):\n        \"\"\"\n        Given a list of metrics, provides a builder that it turns computes metrics from a column.\n\n        See the documentation of [[Summarizer]] for an example.\n\n        The following metrics are accepted (case sensitive):\n         - mean: a vector that contains the coefficient-wise mean.\n         - variance: a vector tha contains the coefficient-wise variance.\n         - count: the count of all vectors seen.\n         - numNonzeros: a vector with the number of non-zeros for each coefficients\n         - max: the maximum for each coefficient.\n         - min: the minimum for each coefficient.\n         - normL2: the Euclidean norm for each coefficient.\n         - normL1: the L1 norm of each coefficient (sum of the absolute values).\n\n        :param metrics:\n         metrics that can be provided.\n        :return:\n         an object of :py:class:`pyspark.ml.stat.SummaryBuilder`\n\n        Note: Currently, the performance of this interface is about 2x~3x slower then using the RDD\n        interface.\n        \"\"\"\n        sc = SparkContext._active_spark_context\n        js = JavaWrapper._new_java_obj(\"org.apache.spark.ml.stat.Summarizer.metrics\",\n                                       _to_seq(sc, metrics))\n        return SummaryBuilder(js)", "docstring": "Given a list of metrics, provides a builder that it turns computes metrics from a column.\n\n        See the documentation of [[Summarizer]] for an example.\n\n        The following metrics are accepted (case sensitive):\n         - mean: a vector that contains the coefficient-wise mean.\n         - variance: a vector tha contains the coefficient-wise variance.\n         - count: the count of all vectors seen.\n         - numNonzeros: a vector with the number of non-zeros for each coefficients\n         ", "func_name": "Summarizer.metrics", "language": "python"}, {"code": "def summary(self, featuresCol, weightCol=None):\n        \"\"\"\n        Returns an aggregate object that contains the summary of the column with the requested\n        metrics.\n\n        :param featuresCol:\n         a column that contains features Vector object.\n        :param weightCol:\n         a column that contains weight value. Default weight is 1.0.\n        :return:\n         an aggregate column that contains the statistics. The exact content of this\n         structure is determined during the creation of the builder.\n        \"\"\"\n        featuresCol, weightCol = Summarizer._check_param(featuresCol, weightCol)\n        return Column(self._java_obj.summary(featuresCol._jc, weightCol._jc))", "docstring": "Returns an aggregate object that contains the summary of the column with the requested\n        metrics.\n\n        :param featuresCol:\n         a column that contains features Vector object.\n        :param weightCol:\n         a column that contains weight value. Default weight is 1.0.\n        :return:\n         an aggregate column that contains the statistics. The exact content of this\n         structure is determined during the creation of the builder.", "func_name": "SummaryBuilder.summary", "language": "python"}, {"code": "def corr(x, y=None, method=None):\n        \"\"\"\n        Compute the correlation (matrix) for the input RDD(s) using the\n        specified method.\n        Methods currently supported: I{pearson (default), spearman}.\n\n        If a single RDD of Vectors is passed in, a correlation matrix\n        comparing the columns in the input RDD is returned. Use C{method=}\n        to specify the method to be used for single RDD inout.\n        If two RDDs of floats are passed in, a single float is returned.\n\n        :param x: an RDD of vector for which the correlation matrix is to be computed,\n                  or an RDD of float of the same cardinality as y when y is specified.\n        :param y: an RDD of float of the same cardinality as x.\n        :param method: String specifying the method to use for computing correlation.\n                       Supported: `pearson` (default), `spearman`\n        :return: Correlation matrix comparing columns in x.\n\n        >>> x = sc.parallelize([1.0, 0.0, -2.0], 2)\n        >>> y = sc.parallelize([4.0, 5.0, 3.0], 2)\n        >>> zeros = sc.parallelize([0.0, 0.0, 0.0], 2)\n        >>> abs(Statistics.corr(x, y) - 0.6546537) < 1e-7\n        True\n        >>> Statistics.corr(x, y) == Statistics.corr(x, y, \"pearson\")\n        True\n        >>> Statistics.corr(x, y, \"spearman\")\n        0.5\n        >>> from math import isnan\n        >>> isnan(Statistics.corr(x, zeros))\n        True\n        >>> from pyspark.mllib.linalg import Vectors\n        >>> rdd = sc.parallelize([Vectors.dense([1, 0, 0, -2]), Vectors.dense([4, 5, 0, 3]),\n        ...                       Vectors.dense([6, 7, 0,  8]), Vectors.dense([9, 0, 0, 1])])\n        >>> pearsonCorr = Statistics.corr(rdd)\n        >>> print(str(pearsonCorr).replace('nan', 'NaN'))\n        [[ 1.          0.05564149         NaN  0.40047142]\n         [ 0.05564149  1.                 NaN  0.91359586]\n         [        NaN         NaN  1.                 NaN]\n         [ 0.40047142  0.91359586         NaN  1.        ]]\n        ", "docstring": "Compute the correlation (matrix) for the input RDD(s) using the\n        specified method.\n        Methods currently supported: I{pearson (default), spearman}.\n\n        If a single RDD of Vectors is passed in, a correlation matrix\n        comparing the columns in the input RDD is returned. Use C{method=}\n        to specify the method to be used for single RDD inout.\n        If two RDDs of floats are passed in, a single float is returned.\n\n        :param x: an RDD of vector for which the correlati", "func_name": "Statistics.corr", "language": "python"}, {"code": "def _parallelFitTasks(est, train, eva, validation, epm, collectSubModel):\n    \"\"\"\n    Creates a list of callables which can be called from different threads to fit and evaluate\n    an estimator in parallel. Each callable returns an `(index, metric)` pair.\n\n    :param est: Estimator, the estimator to be fit.\n    :param train: DataFrame, training data set, used for fitting.\n    :param eva: Evaluator, used to compute `metric`\n    :param validation: DataFrame, validation data set, used for evaluation.\n    :param epm: Sequence of ParamMap, params maps to be used during fitting & evaluation.\n    :param collectSubModel: Whether to collect sub model.\n    :return: (int, float, subModel), an index into `epm` and the associated metric value.\n    \"\"\"\n    modelIter = est.fitMultiple(train, epm)\n\n    def singleTask():\n        index, model = next(modelIter)\n        metric = eva.evaluate(model.transform(validation, epm[index]))\n        return index, metric, model if collectSubModel else None\n\n    return [singleTask] * len(epm)", "docstring": "Creates a list of callables which can be called from different threads to fit and evaluate\n    an estimator in parallel. Each callable returns an `(index, metric)` pair.\n\n    :param est: Estimator, the estimator to be fit.\n    :param train: DataFrame, training data set, used for fitting.\n    :param eva: Evaluator, used to compute `metric`\n    :param validation: DataFrame, validation data set, used for evaluation.\n    :param epm: Sequence of ParamMap, params maps to be used during fitting & evalu", "func_name": "_parallelFitTasks", "language": "python"}, {"code": "def baseOn(self, *args):\n        \"\"\"\n        Sets the given parameters in this grid to fixed values.\n        Accepts either a parameter dictionary or a list of (parameter, value) pairs.\n        \"\"\"\n        if isinstance(args[0], dict):\n            self.baseOn(*args[0].items())\n        else:\n            for (param, value) in args:\n                self.addGrid(param, [value])\n\n        return self", "docstring": "Sets the given parameters in this grid to fixed values.\n        Accepts either a parameter dictionary or a list of (parameter, value) pairs.", "func_name": "ParamGridBuilder.baseOn", "language": "python"}, {"code": "def build(self):\n        \"\"\"\n        Builds and returns all combinations of parameters specified\n        by the param grid.\n        \"\"\"\n        keys = self._param_grid.keys()\n        grid_values = self._param_grid.values()\n\n        def to_key_value_pairs(keys, values):\n            return [(key, key.typeConverter(value)) for key, value in zip(keys, values)]\n\n        return [dict(to_key_value_pairs(keys, prod)) for prod in itertools.product(*grid_values)]", "docstring": "Builds and returns all combinations of parameters specified\n        by the param grid.", "func_name": "ParamGridBuilder.build", "language": "python"}, {"code": "def _from_java_impl(cls, java_stage):\n        \"\"\"\n        Return Python estimator, estimatorParamMaps, and evaluator from a Java ValidatorParams.\n        \"\"\"\n\n        # Load information from java_stage to the instance.\n        estimator = JavaParams._from_java(java_stage.getEstimator())\n        evaluator = JavaParams._from_java(java_stage.getEvaluator())\n        epms = [estimator._transfer_param_map_from_java(epm)\n                for epm in java_stage.getEstimatorParamMaps()]\n        return estimator, epms, evaluator", "docstring": "Return Python estimator, estimatorParamMaps, and evaluator from a Java ValidatorParams.", "func_name": "ValidatorParams._from_java_impl", "language": "python"}, {"code": "def _to_java_impl(self):\n        \"\"\"\n        Return Java estimator, estimatorParamMaps, and evaluator from this Python instance.\n        \"\"\"\n\n        gateway = SparkContext._gateway\n        cls = SparkContext._jvm.org.apache.spark.ml.param.ParamMap\n\n        java_epms = gateway.new_array(cls, len(self.getEstimatorParamMaps()))\n        for idx, epm in enumerate(self.getEstimatorParamMaps()):\n            java_epms[idx] = self.getEstimator()._transfer_param_map_to_java(epm)\n\n        java_estimator = self.getEstimator()._to_java()\n        java_evaluator = self.getEvaluator()._to_java()\n        return java_estimator, java_epms, java_evaluator", "docstring": "Return Java estimator, estimatorParamMaps, and evaluator from this Python instance.", "func_name": "ValidatorParams._to_java_impl", "language": "python"}, {"code": "def _from_java(cls, java_stage):\n        \"\"\"\n        Given a Java CrossValidator, create and return a Python wrapper of it.\n        Used for ML persistence.\n        \"\"\"\n\n        estimator, epms, evaluator = super(CrossValidator, cls)._from_java_impl(java_stage)\n        numFolds = java_stage.getNumFolds()\n        seed = java_stage.getSeed()\n        parallelism = java_stage.getParallelism()\n        collectSubModels = java_stage.getCollectSubModels()\n        # Create a new instance of this stage.\n        py_stage = cls(estimator=estimator, estimatorParamMaps=epms, evaluator=evaluator,\n                       numFolds=numFolds, seed=seed, parallelism=parallelism,\n                       collectSubModels=collectSubModels)\n        py_stage._resetUid(java_stage.uid())\n        return py_stage", "docstring": "Given a Java CrossValidator, create and return a Python wrapper of it.\n        Used for ML persistence.", "func_name": "CrossValidator._from_java", "language": "python"}, {"code": "def _to_java(self):\n        \"\"\"\n        Transfer this instance to a Java CrossValidator. Used for ML persistence.\n\n        :return: Java object equivalent to this instance.\n        \"\"\"\n\n        estimator, epms, evaluator = super(CrossValidator, self)._to_java_impl()\n\n        _java_obj = JavaParams._new_java_obj(\"org.apache.spark.ml.tuning.CrossValidator\", self.uid)\n        _java_obj.setEstimatorParamMaps(epms)\n        _java_obj.setEvaluator(evaluator)\n        _java_obj.setEstimator(estimator)\n        _java_obj.setSeed(self.getSeed())\n        _java_obj.setNumFolds(self.getNumFolds())\n        _java_obj.setParallelism(self.getParallelism())\n        _java_obj.setCollectSubModels(self.getCollectSubModels())\n\n        return _java_obj", "docstring": "Transfer this instance to a Java CrossValidator. Used for ML persistence.\n\n        :return: Java object equivalent to this instance.", "func_name": "CrossValidator._to_java", "language": "python"}, {"code": "def copy(self, extra=None):\n        \"\"\"\n        Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies the underlying bestModel,\n        creates a deep copy of the embedded paramMap, and\n        copies the embedded and extra parameters over.\n        It does not copy the extra Params into the subModels.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance\n        \"\"\"\n        if extra is None:\n            extra = dict()\n        bestModel = self.bestModel.copy(extra)\n        avgMetrics = self.avgMetrics\n        subModels = self.subModels\n        return CrossValidatorModel(bestModel, avgMetrics, subModels)", "docstring": "Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies the underlying bestModel,\n        creates a deep copy of the embedded paramMap, and\n        copies the embedded and extra parameters over.\n        It does not copy the extra Params into the subModels.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance", "func_name": "CrossValidatorModel.copy", "language": "python"}, {"code": "def setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\n                  parallelism=1, collectSubModels=False, seed=None):\n        \"\"\"\n        setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\\\n                  parallelism=1, collectSubModels=False, seed=None):\n        Sets params for the train validation split.\n        \"\"\"\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)", "docstring": "setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\\\n                  parallelism=1, collectSubModels=False, seed=None):\n        Sets params for the train validation split.", "func_name": "TrainValidationSplit.setParams", "language": "python"}, {"code": "def copy(self, extra=None):\n        \"\"\"\n        Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies creates a deep copy of\n        the embedded paramMap, and copies the embedded and extra parameters over.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance\n        \"\"\"\n        if extra is None:\n            extra = dict()\n        newTVS = Params.copy(self, extra)\n        if self.isSet(self.estimator):\n            newTVS.setEstimator(self.getEstimator().copy(extra))\n        # estimatorParamMaps remain the same\n        if self.isSet(self.evaluator):\n            newTVS.setEvaluator(self.getEvaluator().copy(extra))\n        return newTVS", "docstring": "Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies creates a deep copy of\n        the embedded paramMap, and copies the embedded and extra parameters over.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance", "func_name": "TrainValidationSplit.copy", "language": "python"}, {"code": "def _from_java(cls, java_stage):\n        \"\"\"\n        Given a Java TrainValidationSplit, create and return a Python wrapper of it.\n        Used for ML persistence.\n        \"\"\"\n\n        estimator, epms, evaluator = super(TrainValidationSplit, cls)._from_java_impl(java_stage)\n        trainRatio = java_stage.getTrainRatio()\n        seed = java_stage.getSeed()\n        parallelism = java_stage.getParallelism()\n        collectSubModels = java_stage.getCollectSubModels()\n        # Create a new instance of this stage.\n        py_stage = cls(estimator=estimator, estimatorParamMaps=epms, evaluator=evaluator,\n                       trainRatio=trainRatio, seed=seed, parallelism=parallelism,\n                       collectSubModels=collectSubModels)\n        py_stage._resetUid(java_stage.uid())\n        return py_stage", "docstring": "Given a Java TrainValidationSplit, create and return a Python wrapper of it.\n        Used for ML persistence.", "func_name": "TrainValidationSplit._from_java", "language": "python"}, {"code": "def _to_java(self):\n        \"\"\"\n        Transfer this instance to a Java TrainValidationSplit. Used for ML persistence.\n        :return: Java object equivalent to this instance.\n        \"\"\"\n\n        estimator, epms, evaluator = super(TrainValidationSplit, self)._to_java_impl()\n\n        _java_obj = JavaParams._new_java_obj(\"org.apache.spark.ml.tuning.TrainValidationSplit\",\n                                             self.uid)\n        _java_obj.setEstimatorParamMaps(epms)\n        _java_obj.setEvaluator(evaluator)\n        _java_obj.setEstimator(estimator)\n        _java_obj.setTrainRatio(self.getTrainRatio())\n        _java_obj.setSeed(self.getSeed())\n        _java_obj.setParallelism(self.getParallelism())\n        _java_obj.setCollectSubModels(self.getCollectSubModels())\n        return _java_obj", "docstring": "Transfer this instance to a Java TrainValidationSplit. Used for ML persistence.\n        :return: Java object equivalent to this instance.", "func_name": "TrainValidationSplit._to_java", "language": "python"}, {"code": "def copy(self, extra=None):\n        \"\"\"\n        Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies the underlying bestModel,\n        creates a deep copy of the embedded paramMap, and\n        copies the embedded and extra parameters over.\n        And, this creates a shallow copy of the validationMetrics.\n        It does not copy the extra Params into the subModels.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance\n        \"\"\"\n        if extra is None:\n            extra = dict()\n        bestModel = self.bestModel.copy(extra)\n        validationMetrics = list(self.validationMetrics)\n        subModels = self.subModels\n        return TrainValidationSplitModel(bestModel, validationMetrics, subModels)", "docstring": "Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies the underlying bestModel,\n        creates a deep copy of the embedded paramMap, and\n        copies the embedded and extra parameters over.\n        And, this creates a shallow copy of the validationMetrics.\n        It does not copy the extra Params into the subModels.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance", "func_name": "TrainValidationSplitModel.copy", "language": "python"}, {"code": "def _from_java(cls, java_stage):\n        \"\"\"\n        Given a Java TrainValidationSplitModel, create and return a Python wrapper of it.\n        Used for ML persistence.\n        \"\"\"\n\n        # Load information from java_stage to the instance.\n        bestModel = JavaParams._from_java(java_stage.bestModel())\n        estimator, epms, evaluator = super(TrainValidationSplitModel,\n                                           cls)._from_java_impl(java_stage)\n        # Create a new instance of this stage.\n        py_stage = cls(bestModel=bestModel).setEstimator(estimator)\n        py_stage = py_stage.setEstimatorParamMaps(epms).setEvaluator(evaluator)\n\n        if java_stage.hasSubModels():\n            py_stage.subModels = [JavaParams._from_java(sub_model)\n                                  for sub_model in java_stage.subModels()]\n\n        py_stage._resetUid(java_stage.uid())\n        return py_stage", "docstring": "Given a Java TrainValidationSplitModel, create and return a Python wrapper of it.\n        Used for ML persistence.", "func_name": "TrainValidationSplitModel._from_java", "language": "python"}, {"code": "def _to_java(self):\n        \"\"\"\n        Transfer this instance to a Java TrainValidationSplitModel. Used for ML persistence.\n        :return: Java object equivalent to this instance.\n        \"\"\"\n\n        sc = SparkContext._active_spark_context\n        # TODO: persst validation metrics as well\n        _java_obj = JavaParams._new_java_obj(\n            \"org.apache.spark.ml.tuning.TrainValidationSplitModel\",\n            self.uid,\n            self.bestModel._to_java(),\n            _py2java(sc, []))\n        estimator, epms, evaluator = super(TrainValidationSplitModel, self)._to_java_impl()\n\n        _java_obj.set(\"evaluator\", evaluator)\n        _java_obj.set(\"estimator\", estimator)\n        _java_obj.set(\"estimatorParamMaps\", epms)\n\n        if self.subModels is not None:\n            java_sub_models = [sub_model._to_java() for sub_model in self.subModels]\n            _java_obj.setSubModels(java_sub_models)\n\n        return _java_obj", "docstring": "Transfer this instance to a Java TrainValidationSplitModel. Used for ML persistence.\n        :return: Java object equivalent to this instance.", "func_name": "TrainValidationSplitModel._to_java", "language": "python"}, {"code": "def get(self, key, default=_NoValue):\n        \"\"\"Returns the value of Spark runtime configuration property for the given key,\n        assuming it is set.\n        \"\"\"\n        self._checkType(key, \"key\")\n        if default is _NoValue:\n            return self._jconf.get(key)\n        else:\n            if default is not None:\n                self._checkType(default, \"default\")\n            return self._jconf.get(key, default)", "docstring": "Returns the value of Spark runtime configuration property for the given key,\n        assuming it is set.", "func_name": "RuntimeConfig.get", "language": "python"}, {"code": "def _checkType(self, obj, identifier):\n        \"\"\"Assert that an object is of type str.\"\"\"\n        if not isinstance(obj, basestring):\n            raise TypeError(\"expected %s '%s' to be a string (was '%s')\" %\n                            (identifier, obj, type(obj).__name__))", "docstring": "Assert that an object is of type str.", "func_name": "RuntimeConfig._checkType", "language": "python"}, {"code": "def _create_function(name, doc=\"\"):\n    \"\"\"Create a PySpark function by its name\"\"\"\n    def _(col):\n        sc = SparkContext._active_spark_context\n        jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _", "docstring": "Create a PySpark function by its name", "func_name": "_create_function", "language": "python"}, {"code": "def _create_function_over_column(name, doc=\"\"):\n    \"\"\"Similar with `_create_function` but creates a PySpark function that takes a column\n    (as string as well). This is mainly for PySpark functions to take strings as\n    column names.\n    \"\"\"\n    def _(col):\n        sc = SparkContext._active_spark_context\n        jc = getattr(sc._jvm.functions, name)(_to_java_column(col))\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _", "docstring": "Similar with `_create_function` but creates a PySpark function that takes a column\n    (as string as well). This is mainly for PySpark functions to take strings as\n    column names.", "func_name": "_create_function_over_column", "language": "python"}, {"code": "def _wrap_deprecated_function(func, message):\n    \"\"\" Wrap the deprecated function to print out deprecation warnings\"\"\"\n    def _(col):\n        warnings.warn(message, DeprecationWarning)\n        return func(col)\n    return functools.wraps(func)(_)", "docstring": "Wrap the deprecated function to print out deprecation warnings", "func_name": "_wrap_deprecated_function", "language": "python"}, {"code": "def _create_binary_mathfunction(name, doc=\"\"):\n    \"\"\" Create a binary mathfunction by name\"\"\"\n    def _(col1, col2):\n        sc = SparkContext._active_spark_context\n        # For legacy reasons, the arguments here can be implicitly converted into floats,\n        # if they are not columns or strings.\n        if isinstance(col1, Column):\n            arg1 = col1._jc\n        elif isinstance(col1, basestring):\n            arg1 = _create_column_from_name(col1)\n        else:\n            arg1 = float(col1)\n\n        if isinstance(col2, Column):\n            arg2 = col2._jc\n        elif isinstance(col2, basestring):\n            arg2 = _create_column_from_name(col2)\n        else:\n            arg2 = float(col2)\n\n        jc = getattr(sc._jvm.functions, name)(arg1, arg2)\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _", "docstring": "Create a binary mathfunction by name", "func_name": "_create_binary_mathfunction", "language": "python"}, {"code": "def _create_window_function(name, doc=''):\n    \"\"\" Create a window function by name \"\"\"\n    def _():\n        sc = SparkContext._active_spark_context\n        jc = getattr(sc._jvm.functions, name)()\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = 'Window function: ' + doc\n    return _", "docstring": "Create a window function by name", "func_name": "_create_window_function", "language": "python"}, {"code": "def approx_count_distinct(col, rsd=None):\n    \"\"\"Aggregate function: returns a new :class:`Column` for approximate distinct count of\n    column `col`.\n\n    :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\n        efficient to use :func:`countDistinct`\n\n    >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n    [Row(distinct_ages=2)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if rsd is None:\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col))\n    else:\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col), rsd)\n    return Column(jc)", "docstring": "Aggregate function: returns a new :class:`Column` for approximate distinct count of\n    column `col`.\n\n    :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\n        efficient to use :func:`countDistinct`\n\n    >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n    [Row(distinct_ages=2)]", "func_name": "approx_count_distinct", "language": "python"}, {"code": "def broadcast(df):\n    \"\"\"Marks a DataFrame as small enough for use in broadcast joins.\"\"\"\n\n    sc = SparkContext._active_spark_context\n    return DataFrame(sc._jvm.functions.broadcast(df._jdf), df.sql_ctx)", "docstring": "Marks a DataFrame as small enough for use in broadcast joins.", "func_name": "broadcast", "language": "python"}, {"code": "def countDistinct(col, *cols):\n    \"\"\"Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n\n    >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n    [Row(c=2)]\n\n    >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n    [Row(c=2)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.countDistinct(_to_java_column(col), _to_seq(sc, cols, _to_java_column))\n    return Column(jc)", "docstring": "Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n\n    >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n    [Row(c=2)]\n\n    >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n    [Row(c=2)]", "func_name": "countDistinct", "language": "python"}, {"code": "def last(col, ignorenulls=False):\n    \"\"\"Aggregate function: returns the last value in a group.\n\n    The function by default returns the last values it sees. It will return the last non-null\n    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n\n    .. note:: The function is non-deterministic because its results depends on order of rows\n        which may be non-deterministic after a shuffle.\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.last(_to_java_column(col), ignorenulls)\n    return Column(jc)", "docstring": "Aggregate function: returns the last value in a group.\n\n    The function by default returns the last values it sees. It will return the last non-null\n    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n\n    .. note:: The function is non-deterministic because its results depends on order of rows\n        which may be non-deterministic after a shuffle.", "func_name": "last", "language": "python"}, {"code": "def nanvl(col1, col2):\n    \"\"\"Returns col1 if it is not NaN, or col2 if col1 is NaN.\n\n    Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n\n    >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n    >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n    [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.nanvl(_to_java_column(col1), _to_java_column(col2)))", "docstring": "Returns col1 if it is not NaN, or col2 if col1 is NaN.\n\n    Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n\n    >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n    >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n    [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]", "func_name": "nanvl", "language": "python"}, {"code": "def rand(seed=None):\n    \"\"\"Generates a random column with independent and identically distributed (i.i.d.) samples\n    from U[0.0, 1.0].\n\n    .. note:: The function is non-deterministic in general case.\n\n    >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n    [Row(age=2, name=u'Alice', rand=2.4052597283576684),\n     Row(age=5, name=u'Bob', rand=2.3913904055683974)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if seed is not None:\n        jc = sc._jvm.functions.rand(seed)\n    else:\n        jc = sc._jvm.functions.rand()\n    return Column(jc)", "docstring": "Generates a random column with independent and identically distributed (i.i.d.) samples\n    from U[0.0, 1.0].\n\n    .. note:: The function is non-deterministic in general case.\n\n    >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n    [Row(age=2, name=u'Alice', rand=2.4052597283576684),\n     Row(age=5, name=u'Bob', rand=2.3913904055683974)]", "func_name": "rand", "language": "python"}, {"code": "def round(col, scale=0):\n    \"\"\"\n    Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n    or at integral part when `scale` < 0.\n\n    >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n    [Row(r=3.0)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.round(_to_java_column(col), scale))", "docstring": "Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n    or at integral part when `scale` < 0.\n\n    >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n    [Row(r=3.0)]", "func_name": "round", "language": "python"}, {"code": "def shiftLeft(col, numBits):\n    \"\"\"Shift the given value numBits left.\n\n    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n    [Row(r=42)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.shiftLeft(_to_java_column(col), numBits))", "docstring": "Shift the given value numBits left.\n\n    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n    [Row(r=42)]", "func_name": "shiftLeft", "language": "python"}, {"code": "def shiftRight(col, numBits):\n    \"\"\"(Signed) shift the given value numBits right.\n\n    >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n    [Row(r=21)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.shiftRight(_to_java_column(col), numBits)\n    return Column(jc)", "docstring": "(Signed) shift the given value numBits right.\n\n    >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n    [Row(r=21)]", "func_name": "shiftRight", "language": "python"}, {"code": "def expr(str):\n    \"\"\"Parses the expression string into the column that it represents\n\n    >>> df.select(expr(\"length(name)\")).collect()\n    [Row(length(name)=5), Row(length(name)=3)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.expr(str))", "docstring": "Parses the expression string into the column that it represents\n\n    >>> df.select(expr(\"length(name)\")).collect()\n    [Row(length(name)=5), Row(length(name)=3)]", "func_name": "expr", "language": "python"}, {"code": "def when(condition, value):\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\n    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n    :param condition: a boolean :class:`Column` expression.\n    :param value: a literal value, or a :class:`Column` expression.\n\n    >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n    [Row(age=3), Row(age=4)]\n\n    >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n    [Row(age=3), Row(age=None)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if not isinstance(condition, Column):\n        raise TypeError(\"condition should be a Column\")\n    v = value._jc if isinstance(value, Column) else value\n    jc = sc._jvm.functions.when(condition._jc, v)\n    return Column(jc)", "docstring": "Evaluates a list of conditions and returns one of multiple possible result expressions.\n    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n    :param condition: a boolean :class:`Column` expression.\n    :param value: a literal value, or a :class:`Column` expression.\n\n    >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n    [Row(age=3), Row(age=4)]\n\n    >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n    [Row(a", "func_name": "when", "language": "python"}, {"code": "def log(arg1, arg2=None):\n    \"\"\"Returns the first argument-based logarithm of the second argument.\n\n    If there is only one argument, then this takes the natural logarithm of the argument.\n\n    >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n    ['0.30102', '0.69897']\n\n    >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n    ['0.69314', '1.60943']\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if arg2 is None:\n        jc = sc._jvm.functions.log(_to_java_column(arg1))\n    else:\n        jc = sc._jvm.functions.log(arg1, _to_java_column(arg2))\n    return Column(jc)", "docstring": "Returns the first argument-based logarithm of the second argument.\n\n    If there is only one argument, then this takes the natural logarithm of the argument.\n\n    >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n    ['0.30102', '0.69897']\n\n    >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n    ['0.69314', '1.60943']", "func_name": "log", "language": "python"}, {"code": "def conv(col, fromBase, toBase):\n    \"\"\"\n    Convert a number in a string column from one base to another.\n\n    >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n    >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n    [Row(hex=u'15')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.conv(_to_java_column(col), fromBase, toBase))", "docstring": "Convert a number in a string column from one base to another.\n\n    >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n    >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n    [Row(hex=u'15')]", "func_name": "conv", "language": "python"}, {"code": "def lag(col, offset=1, default=None):\n    \"\"\"\n    Window function: returns the value that is `offset` rows before the current row, and\n    `defaultValue` if there is less than `offset` rows before the current row. For example,\n    an `offset` of one will return the previous row at any given point in the window partition.\n\n    This is equivalent to the LAG function in SQL.\n\n    :param col: name of column or expression\n    :param offset: number of row to extend\n    :param default: default value\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.lag(_to_java_column(col), offset, default))", "docstring": "Window function: returns the value that is `offset` rows before the current row, and\n    `defaultValue` if there is less than `offset` rows before the current row. For example,\n    an `offset` of one will return the previous row at any given point in the window partition.\n\n    This is equivalent to the LAG function in SQL.\n\n    :param col: name of column or expression\n    :param offset: number of row to extend\n    :param default: default value", "func_name": "lag", "language": "python"}, {"code": "def ntile(n):\n    \"\"\"\n    Window function: returns the ntile group id (from 1 to `n` inclusive)\n    in an ordered window partition. For example, if `n` is 4, the first\n    quarter of the rows will get value 1, the second quarter will get 2,\n    the third quarter will get 3, and the last quarter will get 4.\n\n    This is equivalent to the NTILE function in SQL.\n\n    :param n: an integer\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.ntile(int(n)))", "docstring": "Window function: returns the ntile group id (from 1 to `n` inclusive)\n    in an ordered window partition. For example, if `n` is 4, the first\n    quarter of the rows will get value 1, the second quarter will get 2,\n    the third quarter will get 3, and the last quarter will get 4.\n\n    This is equivalent to the NTILE function in SQL.\n\n    :param n: an integer", "func_name": "ntile", "language": "python"}, {"code": "def date_format(date, format):\n    \"\"\"\n    Converts a date/timestamp/string to a value of string in the format specified by the date\n    format given by the second argument.\n\n    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n    pattern letters of the Java class `java.time.format.DateTimeFormatter` can be used.\n\n    .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n        specialized implementation.\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n    [Row(date=u'04/08/2015')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_format(_to_java_column(date), format))", "docstring": "Converts a date/timestamp/string to a value of string in the format specified by the date\n    format given by the second argument.\n\n    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n    pattern letters of the Java class `java.time.format.DateTimeFormatter` can be used.\n\n    .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n        specialized implementation.\n\n    >>> df = spark.createDataFrame([('2015-04-08',)", "func_name": "date_format", "language": "python"}, {"code": "def date_add(start, days):\n    \"\"\"\n    Returns the date that is `days` days after `start`\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n    [Row(next_date=datetime.date(2015, 4, 9))]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_add(_to_java_column(start), days))", "docstring": "Returns the date that is `days` days after `start`\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n    [Row(next_date=datetime.date(2015, 4, 9))]", "func_name": "date_add", "language": "python"}, {"code": "def datediff(end, start):\n    \"\"\"\n    Returns the number of days from `start` to `end`.\n\n    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n    [Row(diff=32)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.datediff(_to_java_column(end), _to_java_column(start)))", "docstring": "Returns the number of days from `start` to `end`.\n\n    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n    [Row(diff=32)]", "func_name": "datediff", "language": "python"}, {"code": "def add_months(start, months):\n    \"\"\"\n    Returns the date that is `months` months after `start`\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n    [Row(next_month=datetime.date(2015, 5, 8))]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.add_months(_to_java_column(start), months))", "docstring": "Returns the date that is `months` months after `start`\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n    [Row(next_month=datetime.date(2015, 5, 8))]", "func_name": "add_months", "language": "python"}, {"code": "def months_between(date1, date2, roundOff=True):\n    \"\"\"\n    Returns number of months between dates date1 and date2.\n    If date1 is later than date2, then the result is positive.\n    If date1 and date2 are on the same day of month, or both are the last day of month,\n    returns an integer (time of day will be ignored).\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n    [Row(months=3.94959677)]\n    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n    [Row(months=3.9495967741935485)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.months_between(\n        _to_java_column(date1), _to_java_column(date2), roundOff))", "docstring": "Returns number of months between dates date1 and date2.\n    If date1 is later than date2, then the result is positive.\n    If date1 and date2 are on the same day of month, or both are the last day of month,\n    returns an integer (time of day will be ignored).\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n    >>> df.select(months_between(df.date1, df.date2).alias('mon", "func_name": "months_between", "language": "python"}, {"code": "def to_date(col, format=None):\n    \"\"\"Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n    using the optionally specified format. Specify formats according to\n    `DateTimeFormatter <https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html>`_. # noqa\n    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n    is omitted (equivalent to ``col.cast(\"date\")``).\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t).alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if format is None:\n        jc = sc._jvm.functions.to_date(_to_java_column(col))\n    else:\n        jc = sc._jvm.functions.to_date(_to_java_column(col), format)\n    return Column(jc)", "docstring": "Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n    using the optionally specified format. Specify formats according to\n    `DateTimeFormatter <https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html>`_. # noqa\n    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n    is omitted (equivalent to ``col.cast(\"date\")``).\n\n    >>", "func_name": "to_date", "language": "python"}, {"code": "def date_trunc(format, timestamp):\n    \"\"\"\n    Returns timestamp truncated to the unit specified by the format.\n\n    :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n        'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_trunc(format, _to_java_column(timestamp)))", "docstring": "Returns timestamp truncated to the unit specified by the format.\n\n    :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n        'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n    [Row(month=datetime.datetime(1997, 2, 1, ", "func_name": "date_trunc", "language": "python"}, {"code": "def next_day(date, dayOfWeek):\n    \"\"\"\n    Returns the first date which is later than the value of the date column.\n\n    Day of the week parameter is case insensitive, and accepts:\n        \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n\n    >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n    >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n    [Row(date=datetime.date(2015, 8, 2))]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.next_day(_to_java_column(date), dayOfWeek))", "docstring": "Returns the first date which is later than the value of the date column.\n\n    Day of the week parameter is case insensitive, and accepts:\n        \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n\n    >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n    >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n    [Row(date=datetime.date(2015, 8, 2))]", "func_name": "next_day", "language": "python"}, {"code": "def last_day(date):\n    \"\"\"\n    Returns the last day of the month which the given date belongs to.\n\n    >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n    >>> df.select(last_day(df.d).alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.last_day(_to_java_column(date)))", "docstring": "Returns the last day of the month which the given date belongs to.\n\n    >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n    >>> df.select(last_day(df.d).alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]", "func_name": "last_day", "language": "python"}, {"code": "def unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss'):\n    \"\"\"\n    Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n    to Unix time stamp (in seconds), using the default timezone and the default\n    locale, return null if fail.\n\n    if `timestamp` is None, then it returns current timestamp.\n\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n    >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n    [Row(unix_time=1428476400)]\n    >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if timestamp is None:\n        return Column(sc._jvm.functions.unix_timestamp())\n    return Column(sc._jvm.functions.unix_timestamp(_to_java_column(timestamp), format))", "docstring": "Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n    to Unix time stamp (in seconds), using the default timezone and the default\n    locale, return null if fail.\n\n    if `timestamp` is None, then it returns current timestamp.\n\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n    >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n    [Row(unix_ti", "func_name": "unix_timestamp", "language": "python"}, {"code": "def from_utc_timestamp(timestamp, tz):\n    \"\"\"\n    This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n    renders that timestamp as a timestamp in the given time zone.\n\n    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n    timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n    the given timezone.\n\n    This function may return confusing result if the input is a string with timezone, e.g.\n    '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n    according to the timezone in the string, and finally display the result by converting the\n    timestamp to string according to the session local timezone.\n\n    :param timestamp: the column that contains timestamps\n    :param tz: a string that has the ID of timezone, e.g. \"GMT\", \"America/Los_Angeles\", etc\n\n    .. versionchanged:: 2.4\n       `tz` can take a :class:`Column` containing timezone ID strings.\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n    >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n    [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n    >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n    [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n\n    .. note:: Deprecated in 3.0. See SPARK-25496\n    \"\"\"\n    warnings.warn(\"Deprecated in 3.0. See SPARK-25496\", DeprecationWarning)\n    sc = SparkContext._active_spark_context\n    if isinstance(tz, Column):\n        tz = _to_java_column(tz)\n    return Column(sc._jvm.functions.from_utc_timestamp(_to_java_column(timestamp), tz))", "docstring": "This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n    renders that timestamp as a timestamp in the given time zone.\n\n    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n    timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n    the given timezone.\n\n    This function m", "func_name": "from_utc_timestamp", "language": "python"}, {"code": "def window(timeColumn, windowDuration, slideDuration=None, startTime=None):\n    \"\"\"Bucketize rows into one or more time windows given a timestamp specifying column. Window\n    starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n    [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n    the order of months are not supported.\n\n    The time column must be of :class:`pyspark.sql.types.TimestampType`.\n\n    Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n    interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n    If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n\n    The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n    window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n    past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n\n    The output column will be a struct called 'window' by default with the nested columns 'start'\n    and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n\n    >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n    >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n    >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n    ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n    [Row(start=u'2016-03-11 09:00:05', end=u'2016-03-11 09:00:10', sum=1)]\n    \"\"\"\n    def check_string_field(field, fieldName):\n        if not field or type(field) is not str:\n            raise TypeError(\"%s should be provided as a string\" % fieldName)\n\n    sc = SparkContext._active_spark_context\n    time_col = _to_java_column(timeColumn)\n    check_string_field(windowDuration, \"windowDuration\")\n    if slideDuration and startTime:\n        check_string_field(slide", "docstring": "Bucketize rows into one or more time windows given a timestamp specifying column. Window\n    starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n    [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n    the order of months are not supported.\n\n    The time column must be of :class:`pyspark.sql.types.TimestampType`.\n\n    Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n    inter", "func_name": "window", "language": "python"}, {"code": "def hash(*cols):\n    \"\"\"Calculates the hash code of given columns, and returns the result as an int column.\n\n    >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n    [Row(hash=-757602832)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\n    return Column(jc)", "docstring": "Calculates the hash code of given columns, and returns the result as an int column.\n\n    >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n    [Row(hash=-757602832)]", "func_name": "hash", "language": "python"}, {"code": "def concat_ws(sep, *cols):\n    \"\"\"\n    Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd-123')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.concat_ws(sep, _to_seq(sc, cols, _to_java_column)))", "docstring": "Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd-123')]", "func_name": "concat_ws", "language": "python"}, {"code": "def decode(col, charset):\n    \"\"\"\n    Computes the first argument into a string from a binary using the provided character set\n    (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.decode(_to_java_column(col), charset))", "docstring": "Computes the first argument into a string from a binary using the provided character set\n    (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').", "func_name": "decode", "language": "python"}, {"code": "def format_number(col, d):\n    \"\"\"\n    Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n    with HALF_EVEN round mode, and returns the result as a string.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n    [Row(v=u'5.0000')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.format_number(_to_java_column(col), d))", "docstring": "Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n    with HALF_EVEN round mode, and returns the result as a string.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n    [Row(v=u'5.0000')]", "func_name": "format_number", "language": "python"}, {"code": "def format_string(format, *cols):\n    \"\"\"\n    Formats the arguments in printf-style and returns the result as a string column.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n    >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n    [Row(v=u'5 hello')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.format_string(format, _to_seq(sc, cols, _to_java_column)))", "docstring": "Formats the arguments in printf-style and returns the result as a string column.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n    >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n    [Row(v=u'5 hello')]", "func_name": "format_string", "language": "python"}, {"code": "def instr(str, substr):\n    \"\"\"\n    Locate the position of the first occurrence of substr column in the given string.\n    Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(instr(df.s, 'b').alias('s')).collect()\n    [Row(s=2)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.instr(_to_java_column(str), substr))", "docstring": "Locate the position of the first occurrence of substr column in the given string.\n    Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(instr(df.s, 'b').alias('s')).collect()\n    [Row(s=2)]", "func_name": "instr", "language": "python"}, {"code": "def substring_index(str, delim, count):\n    \"\"\"\n    Returns the substring from string str before count occurrences of the delimiter delim.\n    If count is positive, everything the left of the final delimiter (counting from left) is\n    returned. If count is negative, every to the right of the final delimiter (counting from the\n    right) is returned. substring_index performs a case-sensitive match when searching for delim.\n\n    >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n    >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n    [Row(s=u'a.b')]\n    >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n    [Row(s=u'b.c.d')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.substring_index(_to_java_column(str), delim, count))", "docstring": "Returns the substring from string str before count occurrences of the delimiter delim.\n    If count is positive, everything the left of the final delimiter (counting from left) is\n    returned. If count is negative, every to the right of the final delimiter (counting from the\n    right) is returned. substring_index performs a case-sensitive match when searching for delim.\n\n    >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n    >>> df.select(substring_index(df.s, '.', 2).alias('s')).collec", "func_name": "substring_index", "language": "python"}, {"code": "def levenshtein(left, right):\n    \"\"\"Computes the Levenshtein distance of the two given strings.\n\n    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n    [Row(d=3)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))\n    return Column(jc)", "docstring": "Computes the Levenshtein distance of the two given strings.\n\n    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n    [Row(d=3)]", "func_name": "levenshtein", "language": "python"}, {"code": "def locate(substr, str, pos=1):\n    \"\"\"\n    Locate the position of the first occurrence of substr in a string column, after position pos.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    :param substr: a string\n    :param str: a Column of :class:`pyspark.sql.types.StringType`\n    :param pos: start position (zero based)\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n    [Row(s=2)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.locate(substr, _to_java_column(str), pos))", "docstring": "Locate the position of the first occurrence of substr in a string column, after position pos.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    :param substr: a string\n    :param str: a Column of :class:`pyspark.sql.types.StringType`\n    :param pos: start position (zero based)\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n    [Row(s=2)]", "func_name": "locate", "language": "python"}, {"code": "def lpad(col, len, pad):\n    \"\"\"\n    Left-pad the string column to width `len` with `pad`.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n    [Row(s=u'##abcd')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.lpad(_to_java_column(col), len, pad))", "docstring": "Left-pad the string column to width `len` with `pad`.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n    [Row(s=u'##abcd')]", "func_name": "lpad", "language": "python"}, {"code": "def repeat(col, n):\n    \"\"\"\n    Repeats a string column n times, and returns it as a new string column.\n\n    >>> df = spark.createDataFrame([('ab',)], ['s',])\n    >>> df.select(repeat(df.s, 3).alias('s')).collect()\n    [Row(s=u'ababab')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.repeat(_to_java_column(col), n))", "docstring": "Repeats a string column n times, and returns it as a new string column.\n\n    >>> df = spark.createDataFrame([('ab',)], ['s',])\n    >>> df.select(repeat(df.s, 3).alias('s')).collect()\n    [Row(s=u'ababab')]", "func_name": "repeat", "language": "python"}, {"code": "def split(str, pattern, limit=-1):\n    \"\"\"\n    Splits str around matches of the given pattern.\n\n    :param str: a string expression to split\n    :param pattern: a string representing a regular expression. The regex string should be\n        a Java regular expression.\n    :param limit: an integer which controls the number of times `pattern` is applied.\n\n        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n                         resulting array's last entry will contain all input beyond the last\n                         matched pattern.\n        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n                          array can be of any size.\n\n    .. versionchanged:: 3.0\n       `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n\n    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n    [Row(s=[u'one', u'twoBthreeC'])]\n    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n    [Row(s=[u'one', u'two', u'three', u''])]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.split(_to_java_column(str), pattern, limit))", "docstring": "Splits str around matches of the given pattern.\n\n    :param str: a string expression to split\n    :param pattern: a string representing a regular expression. The regex string should be\n        a Java regular expression.\n    :param limit: an integer which controls the number of times `pattern` is applied.\n\n        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n                         resulting array's last entry will contain all input beyond the last\n       ", "func_name": "split", "language": "python"}, {"code": "def regexp_extract(str, pattern, idx):\n    r\"\"\"Extract a specific group matched by a Java regex, from the specified string column.\n    If the regex did not match, or the specified group did not match, an empty string is returned.\n\n    >>> df = spark.createDataFrame([('100-200',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n    [Row(d=u'100')]\n    >>> df = spark.createDataFrame([('foo',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n    [Row(d=u'')]\n    >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n    >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n    [Row(d=u'')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.regexp_extract(_to_java_column(str), pattern, idx)\n    return Column(jc)", "docstring": "r\"\"\"Extract a specific group matched by a Java regex, from the specified string column.\n    If the regex did not match, or the specified group did not match, an empty string is returned.\n\n    >>> df = spark.createDataFrame([('100-200',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n    [Row(d=u'100')]\n    >>> df = spark.createDataFrame([('foo',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n    [Row(d=u'')]\n   ", "func_name": "regexp_extract", "language": "python"}, {"code": "def regexp_replace(str, pattern, replacement):\n    r\"\"\"Replace all substrings of the specified string value that match regexp with rep.\n\n    >>> df = spark.createDataFrame([('100-200',)], ['str'])\n    >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n    [Row(d=u'-----')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.regexp_replace(_to_java_column(str), pattern, replacement)\n    return Column(jc)", "docstring": "r\"\"\"Replace all substrings of the specified string value that match regexp with rep.\n\n    >>> df = spark.createDataFrame([('100-200',)], ['str'])\n    >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n    [Row(d=u'-----')]", "func_name": "regexp_replace", "language": "python"}, {"code": "def translate(srcCol, matching, replace):\n    \"\"\"A function translate any character in the `srcCol` by a character in `matching`.\n    The characters in `replace` is corresponding to the characters in `matching`.\n    The translate will happen when any character in the string matching with the character\n    in the `matching`.\n\n    >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\\\n    ...     .alias('r')).collect()\n    [Row(r=u'1a2s3ae')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.translate(_to_java_column(srcCol), matching, replace))", "docstring": "A function translate any character in the `srcCol` by a character in `matching`.\n    The characters in `replace` is corresponding to the characters in `matching`.\n    The translate will happen when any character in the string matching with the character\n    in the `matching`.\n\n    >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\\\n    ...     .alias('r')).collect()\n    [Row(r=u'1a2s3ae')]", "func_name": "translate", "language": "python"}, {"code": "def arrays_overlap(a1, a2):\n    \"\"\"\n    Collection function: returns true if the arrays contain any common non-null element; if not,\n    returns null if both the arrays are non-empty and any of them contains a null element; returns\n    false otherwise.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n    >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n    [Row(overlap=True), Row(overlap=False)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.arrays_overlap(_to_java_column(a1), _to_java_column(a2)))", "docstring": "Collection function: returns true if the arrays contain any common non-null element; if not,\n    returns null if both the arrays are non-empty and any of them contains a null element; returns\n    false otherwise.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n    >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n    [Row(overlap=True), Row(overlap=False)]", "func_name": "arrays_overlap", "language": "python"}, {"code": "def slice(x, start, length):\n    \"\"\"\n    Collection function: returns an array containing  all the elements in `x` from index `start`\n    (or starting from the end if `start` is negative) with the specified `length`.\n    >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n    >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n    [Row(sliced=[2, 3]), Row(sliced=[5])]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.slice(_to_java_column(x), start, length))", "docstring": "Collection function: returns an array containing  all the elements in `x` from index `start`\n    (or starting from the end if `start` is negative) with the specified `length`.\n    >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n    >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n    [Row(sliced=[2, 3]), Row(sliced=[5])]", "func_name": "slice", "language": "python"}, {"code": "def array_join(col, delimiter, null_replacement=None):\n    \"\"\"\n    Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n    `null_replacement` if set, otherwise they are ignored.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n    >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a')]\n    >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a,NULL')]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if null_replacement is None:\n        return Column(sc._jvm.functions.array_join(_to_java_column(col), delimiter))\n    else:\n        return Column(sc._jvm.functions.array_join(\n            _to_java_column(col), delimiter, null_replacement))", "docstring": "Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n    `null_replacement` if set, otherwise they are ignored.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n    >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a')]\n    >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a,NULL')]", "func_name": "array_join", "language": "python"}, {"code": "def concat(*cols):\n    \"\"\"\n    Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd123')]\n\n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.concat(_to_seq(sc, cols, _to_java_column)))", "docstring": "Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd123')]\n\n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)", "func_name": "concat", "language": "python"}, {"code": "def array_position(col, value):\n    \"\"\"\n    Collection function: Locates the position of the first occurrence of the given value\n    in the given array. Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if the given\n        value could not be found in the array.\n\n    >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n    >>> df.select(array_position(df.data, \"a\")).collect()\n    [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.array_position(_to_java_column(col), value))", "docstring": "Collection function: Locates the position of the first occurrence of the given value\n    in the given array. Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if the given\n        value could not be found in the array.\n\n    >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n    >>> df.select(array_position(df.data, \"a\")).collect()\n    [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]", "func_name": "array_position", "language": "python"}, {"code": "def element_at(col, extraction):\n    \"\"\"\n    Collection function: Returns element of array at given index in extraction if col is array.\n    Returns value for the given key in extraction if col is map.\n\n    :param col: name of column containing array or map\n    :param extraction: index to check for in array or key to check for in map\n\n    .. note:: The position is not zero based, but 1 based index.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n    >>> df.select(element_at(df.data, 1)).collect()\n    [Row(element_at(data, 1)=u'a'), Row(element_at(data, 1)=None)]\n\n    >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], ['data'])\n    >>> df.select(element_at(df.data, \"a\")).collect()\n    [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.element_at(_to_java_column(col), extraction))", "docstring": "Collection function: Returns element of array at given index in extraction if col is array.\n    Returns value for the given key in extraction if col is map.\n\n    :param col: name of column containing array or map\n    :param extraction: index to check for in array or key to check for in map\n\n    .. note:: The position is not zero based, but 1 based index.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n    >>> df.select(element_at(df.data, 1)).collect()\n    [Row(element", "func_name": "element_at", "language": "python"}, {"code": "def array_remove(col, element):\n    \"\"\"\n    Collection function: Remove all elements that equal to element from the given array.\n\n    :param col: name of column containing array\n    :param element: element to be removed from the array\n\n    >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n    >>> df.select(array_remove(df.data, 1)).collect()\n    [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.array_remove(_to_java_column(col), element))", "docstring": "Collection function: Remove all elements that equal to element from the given array.\n\n    :param col: name of column containing array\n    :param element: element to be removed from the array\n\n    >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n    >>> df.select(array_remove(df.data, 1)).collect()\n    [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]", "func_name": "array_remove", "language": "python"}, {"code": "def explode(col):\n    \"\"\"\n    Returns a new row for each element in the given array or map.\n    Uses the default column name `col` for elements in the array and\n    `key` and `value` for elements in the map unless specified otherwise.\n\n    >>> from pyspark.sql import Row\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n    >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n\n    >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n    +---+-----+\n    |key|value|\n    +---+-----+\n    |  a|    b|\n    +---+-----+\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.explode(_to_java_column(col))\n    return Column(jc)", "docstring": "Returns a new row for each element in the given array or map.\n    Uses the default column name `col` for elements in the array and\n    `key` and `value` for elements in the map unless specified otherwise.\n\n    >>> from pyspark.sql import Row\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n    >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n\n    >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"val", "func_name": "explode", "language": "python"}, {"code": "def get_json_object(col, path):\n    \"\"\"\n    Extracts json object from a json string based on json path specified, and returns json string\n    of the extracted json object. It will return null if the input json string is invalid.\n\n    :param col: string column in json format\n    :param path: path to the json object to extract\n\n    >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n    >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\\\n    ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.get_json_object(_to_java_column(col), path)\n    return Column(jc)", "docstring": "Extracts json object from a json string based on json path specified, and returns json string\n    of the extracted json object. It will return null if the input json string is invalid.\n\n    :param col: string column in json format\n    :param path: path to the json object to extract\n\n    >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n    >>> df.select(df.key, get_json_object(df.jstring, '$.f1'", "func_name": "get_json_object", "language": "python"}, {"code": "def json_tuple(col, *fields):\n    \"\"\"Creates a new row for a json column according to the given field names.\n\n    :param col: string column in json format\n    :param fields: list of fields to extract\n\n    >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n    >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.json_tuple(_to_java_column(col), _to_seq(sc, fields))\n    return Column(jc)", "docstring": "Creates a new row for a json column according to the given field names.\n\n    :param col: string column in json format\n    :param fields: list of fields to extract\n\n    >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n    >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]", "func_name": "json_tuple", "language": "python"}, {"code": "def schema_of_json(json, options={}):\n    \"\"\"\n    Parses a JSON string and infers its schema in DDL format.\n\n    :param json: a JSON string or a string literal containing a JSON string.\n    :param options: options to control parsing. accepts the same options as the JSON datasource\n\n    .. versionchanged:: 3.0\n       It accepts `options` parameter to control schema inferring.\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n    [Row(json=u'struct<a:bigint>')]\n    >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n    >>> df.select(schema.alias(\"json\")).collect()\n    [Row(json=u'struct<a:bigint>')]\n    \"\"\"\n    if isinstance(json, basestring):\n        col = _create_column_from_literal(json)\n    elif isinstance(json, Column):\n        col = _to_java_column(json)\n    else:\n        raise TypeError(\"schema argument should be a column or string\")\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.schema_of_json(col, options)\n    return Column(jc)", "docstring": "Parses a JSON string and infers its schema in DDL format.\n\n    :param json: a JSON string or a string literal containing a JSON string.\n    :param options: options to control parsing. accepts the same options as the JSON datasource\n\n    .. versionchanged:: 3.0\n       It accepts `options` parameter to control schema inferring.\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n    [Row(json=u'struct<a:bigint>')]\n    >>> schema = schema_of_json(", "func_name": "schema_of_json", "language": "python"}, {"code": "def schema_of_csv(csv, options={}):\n    \"\"\"\n    Parses a CSV string and infers its schema in DDL format.\n\n    :param col: a CSV string or a string literal containing a CSV string.\n    :param options: options to control parsing. accepts the same options as the CSV datasource\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>')]\n    >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>')]\n    \"\"\"\n    if isinstance(csv, basestring):\n        col = _create_column_from_literal(csv)\n    elif isinstance(csv, Column):\n        col = _to_java_column(csv)\n    else:\n        raise TypeError(\"schema argument should be a column or string\")\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.schema_of_csv(col, options)\n    return Column(jc)", "docstring": "Parses a CSV string and infers its schema in DDL format.\n\n    :param col: a CSV string or a string literal containing a CSV string.\n    :param options: options to control parsing. accepts the same options as the CSV datasource\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>')]\n    >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>'", "func_name": "schema_of_csv", "language": "python"}, {"code": "def to_csv(col, options={}):\n    \"\"\"\n    Converts a column containing a :class:`StructType` into a CSV string.\n    Throws an exception, in the case of an unsupported type.\n\n    :param col: name of column containing a struct.\n    :param options: options to control converting. accepts the same options as the CSV datasource.\n\n    >>> from pyspark.sql import Row\n    >>> data = [(1, Row(name='Alice', age=2))]\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n    >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n    [Row(csv=u'2,Alice')]\n    \"\"\"\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.to_csv(_to_java_column(col), options)\n    return Column(jc)", "docstring": "Converts a column containing a :class:`StructType` into a CSV string.\n    Throws an exception, in the case of an unsupported type.\n\n    :param col: name of column containing a struct.\n    :param options: options to control converting. accepts the same options as the CSV datasource.\n\n    >>> from pyspark.sql import Row\n    >>> data = [(1, Row(name='Alice', age=2))]\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n    >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n    [Row(csv=u'", "func_name": "to_csv", "language": "python"}, {"code": "def size(col):\n    \"\"\"\n    Collection function: returns the length of the array or map stored in the column.\n\n    :param col: name of column or expression\n\n    >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n    >>> df.select(size(df.data)).collect()\n    [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.size(_to_java_column(col)))", "docstring": "Collection function: returns the length of the array or map stored in the column.\n\n    :param col: name of column or expression\n\n    >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n    >>> df.select(size(df.data)).collect()\n    [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]", "func_name": "size", "language": "python"}, {"code": "def sort_array(col, asc=True):\n    \"\"\"\n    Collection function: sorts the input array in ascending or descending order according\n    to the natural ordering of the array elements. Null elements will be placed at the beginning\n    of the returned array in ascending order or at the end of the returned array in descending\n    order.\n\n    :param col: name of column or expression\n\n    >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n    >>> df.select(sort_array(df.data).alias('r')).collect()\n    [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n    >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n    [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.sort_array(_to_java_column(col), asc))", "docstring": "Collection function: sorts the input array in ascending or descending order according\n    to the natural ordering of the array elements. Null elements will be placed at the beginning\n    of the returned array in ascending order or at the end of the returned array in descending\n    order.\n\n    :param col: name of column or expression\n\n    >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n    >>> df.select(sort_array(df.data).alias('r')).collect()\n    [Row(r=[None, 1, 2, ", "func_name": "sort_array", "language": "python"}, {"code": "def array_repeat(col, count):\n    \"\"\"\n    Collection function: creates an array containing a column repeated count times.\n\n    >>> df = spark.createDataFrame([('ab',)], ['data'])\n    >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n    [Row(r=[u'ab', u'ab', u'ab'])]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.array_repeat(_to_java_column(col), count))", "docstring": "Collection function: creates an array containing a column repeated count times.\n\n    >>> df = spark.createDataFrame([('ab',)], ['data'])\n    >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n    [Row(r=[u'ab', u'ab', u'ab'])]", "func_name": "array_repeat", "language": "python"}, {"code": "def map_concat(*cols):\n    \"\"\"Returns the union of all the given maps.\n\n    :param cols: list of column names (string) or list of :class:`Column` expressions\n\n    >>> from pyspark.sql.functions import map_concat\n    >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c', 1, 'd') as map2\")\n    >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n    +------------------------+\n    |map3                    |\n    +------------------------+\n    |[1 -> d, 2 -> b, 3 -> c]|\n    +------------------------+\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if len(cols) == 1 and isinstance(cols[0], (list, set)):\n        cols = cols[0]\n    jc = sc._jvm.functions.map_concat(_to_seq(sc, cols, _to_java_column))\n    return Column(jc)", "docstring": "Returns the union of all the given maps.\n\n    :param cols: list of column names (string) or list of :class:`Column` expressions\n\n    >>> from pyspark.sql.functions import map_concat\n    >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c', 1, 'd') as map2\")\n    >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n    +------------------------+\n    |map3                    |\n    +------------------------+\n    |[1 -> d, 2 -> b, 3 -> c]|\n    +------------------", "func_name": "map_concat", "language": "python"}, {"code": "def sequence(start, stop, step=None):\n    \"\"\"\n    Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n    If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n    otherwise -1.\n\n    >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n    >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n    [Row(r=[-2, -1, 0, 1, 2])]\n    >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n    >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n    [Row(r=[4, 2, 0, -2, -4])]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if step is None:\n        return Column(sc._jvm.functions.sequence(_to_java_column(start), _to_java_column(stop)))\n    else:\n        return Column(sc._jvm.functions.sequence(\n            _to_java_column(start), _to_java_column(stop), _to_java_column(step)))", "docstring": "Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n    If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n    otherwise -1.\n\n    >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n    >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n    [Row(r=[-2, -1, 0, 1, 2])]\n    >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n    >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n    [Row(r=[4, 2, 0,", "func_name": "sequence", "language": "python"}, {"code": "def from_csv(col, schema, options={}):\n    \"\"\"\n    Parses a column containing a CSV string to a row with the specified schema.\n    Returns `null`, in the case of an unparseable string.\n\n    :param col: string column in CSV format\n    :param schema: a string with schema in DDL format to use when parsing the CSV column.\n    :param options: options to control parsing. accepts the same options as the CSV datasource\n\n    >>> data = [(\"1,2,3\",)]\n    >>> df = spark.createDataFrame(data, (\"value\",))\n    >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\n    [Row(csv=Row(a=1, b=2, c=3))]\n    >>> value = data[0][0]\n    >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\n    [Row(csv=Row(_c0=1, _c1=2, _c2=3))]\n    \"\"\"\n\n    sc = SparkContext._active_spark_context\n    if isinstance(schema, basestring):\n        schema = _create_column_from_literal(schema)\n    elif isinstance(schema, Column):\n        schema = _to_java_column(schema)\n    else:\n        raise TypeError(\"schema argument should be a column or string\")\n\n    jc = sc._jvm.functions.from_csv(_to_java_column(col), schema, options)\n    return Column(jc)", "docstring": "Parses a column containing a CSV string to a row with the specified schema.\n    Returns `null`, in the case of an unparseable string.\n\n    :param col: string column in CSV format\n    :param schema: a string with schema in DDL format to use when parsing the CSV column.\n    :param options: options to control parsing. accepts the same options as the CSV datasource\n\n    >>> data = [(\"1,2,3\",)]\n    >>> df = spark.createDataFrame(data, (\"value\",))\n    >>> df.select(from_csv(df.value, \"a INT, b INT, c ", "func_name": "from_csv", "language": "python"}, {"code": "def udf(f=None, returnType=StringType()):\n    \"\"\"Creates a user defined function (UDF).\n\n    .. note:: The user-defined functions are considered deterministic by default. Due to\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\n        more times than it is present in the query. If your function is not deterministic, call\n        `asNondeterministic` on the user defined function. E.g.:\n\n    >>> from pyspark.sql.types import IntegerType\n    >>> import random\n    >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n\n    .. note:: The user-defined functions do not support conditional expressions or short circuiting\n        in boolean expressions and it ends up with being executed all internally. If the functions\n        can fail on special rows, the workaround is to incorporate the condition into the functions.\n\n    .. note:: The user-defined functions do not take keyword arguments on the calling side.\n\n    :param f: python function if used as a standalone function\n    :param returnType: the return type of the user-defined function. The value can be either a\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n\n    >>> from pyspark.sql.types import IntegerType\n    >>> slen = udf(lambda s: len(s), IntegerType())\n    >>> @udf\n    ... def to_upper(s):\n    ...     if s is not None:\n    ...         return s.upper()\n    ...\n    >>> @udf(returnType=IntegerType())\n    ... def add_one(x):\n    ...     if x is not None:\n    ...         return x + 1\n    ...\n    >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n    >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n    +----------+--------------+------------+\n    |slen(name)|to_upper(name)|add_one(age)|\n    +----------+--------------+------------+\n    |         8|      JOHN DOE|          22|\n    +----------+--------------+------------+\n    \"\"\"\n\n    # T", "docstring": "Creates a user defined function (UDF).\n\n    .. note:: The user-defined functions are considered deterministic by default. Due to\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\n        more times than it is present in the query. If your function is not deterministic, call\n        `asNondeterministic` on the user defined function. E.g.:\n\n    >>> from pyspark.sql.types import IntegerType\n    >>> import random\n    >>> random_udf = udf(lambda: int(ra", "func_name": "udf", "language": "python"}, {"code": "def pandas_udf(f=None, returnType=None, functionType=None):\n    \"\"\"\n    Creates a vectorized user defined function (UDF).\n\n    :param f: user-defined function. A python function if used as a standalone function\n    :param returnType: the return type of the user-defined function. The value can be either a\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n                         Default: SCALAR.\n\n    .. note:: Experimental\n\n    The function type of the UDF can be one of the following:\n\n    1. SCALAR\n\n       A scalar UDF defines a transformation: One or more `pandas.Series` -> A `pandas.Series`.\n       The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.\n       If the return type is :class:`StructType`, the returned value should be a `pandas.DataFrame`.\n\n       :class:`MapType`, nested :class:`StructType` are currently not supported as output types.\n\n       Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\n       :meth:`pyspark.sql.DataFrame.select`.\n\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n       >>> from pyspark.sql.types import IntegerType, StringType\n       >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\n       >>> @pandas_udf(StringType())  # doctest: +SKIP\n       ... def to_upper(s):\n       ...     return s.str.upper()\n       ...\n       >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n       ... def add_one(x):\n       ...     return x + 1\n       ...\n       >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\n       ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\n       >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\\\n       ...     .show()  # doctest: +SKIP\n       +----------+--------------+------------+\n       |slen(name)|to_upper(name)|add_one(", "docstring": "Creates a vectorized user defined function (UDF).\n\n    :param f: user-defined function. A python function if used as a standalone function\n    :param returnType: the return type of the user-defined function. The value can be either a\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n                         Default: SCALAR.\n\n    .. note:: Experimental\n\n    The function type of", "func_name": "pandas_udf", "language": "python"}, {"code": "def to_str(value):\n    \"\"\"\n    A wrapper over str(), but converts bool values to lower case strings.\n    If None is given, just returns None, instead of converting it to string \"None\".\n    \"\"\"\n    if isinstance(value, bool):\n        return str(value).lower()\n    elif value is None:\n        return value\n    else:\n        return str(value)", "docstring": "A wrapper over str(), but converts bool values to lower case strings.\n    If None is given, just returns None, instead of converting it to string \"None\".", "func_name": "to_str", "language": "python"}, {"code": "def _set_opts(self, schema=None, **options):\n        \"\"\"\n        Set named options (filter out those the value is None)\n        \"\"\"\n        if schema is not None:\n            self.schema(schema)\n        for k, v in options.items():\n            if v is not None:\n                self.option(k, v)", "docstring": "Set named options (filter out those the value is None)", "func_name": "OptionUtils._set_opts", "language": "python"}, {"code": "def format(self, source):\n        \"\"\"Specifies the input data source format.\n\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\n\n        >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\n        >>> df.dtypes\n        [('age', 'bigint'), ('name', 'string')]\n\n        \"\"\"\n        self._jreader = self._jreader.format(source)\n        return self", "docstring": "Specifies the input data source format.\n\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\n\n        >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\n        >>> df.dtypes\n        [('age', 'bigint'), ('name', 'string')]", "func_name": "DataFrameReader.format", "language": "python"}, {"code": "def schema(self, schema):\n        \"\"\"Specifies the input schema.\n\n        Some data sources (e.g. JSON) can infer the input schema automatically from data.\n        By specifying the schema here, the underlying data source can skip the schema\n        inference step, and thus speed up data loading.\n\n        :param schema: a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n                       (For example ``col0 INT, col1 DOUBLE``).\n\n        >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")\n        \"\"\"\n        from pyspark.sql import SparkSession\n        spark = SparkSession.builder.getOrCreate()\n        if isinstance(schema, StructType):\n            jschema = spark._jsparkSession.parseDataType(schema.json())\n            self._jreader = self._jreader.schema(jschema)\n        elif isinstance(schema, basestring):\n            self._jreader = self._jreader.schema(schema)\n        else:\n            raise TypeError(\"schema should be StructType or string\")\n        return self", "docstring": "Specifies the input schema.\n\n        Some data sources (e.g. JSON) can infer the input schema automatically from data.\n        By specifying the schema here, the underlying data source can skip the schema\n        inference step, and thus speed up data loading.\n\n        :param schema: a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n                       (For example ``col0 INT, col1 DOUBLE``).\n\n        >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")", "func_name": "DataFrameReader.schema", "language": "python"}, {"code": "def option(self, key, value):\n        \"\"\"Adds an input option for the underlying data source.\n\n        You can set the following option(s) for reading files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n                in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.\n        \"\"\"\n        self._jreader = self._jreader.option(key, to_str(value))\n        return self", "docstring": "Adds an input option for the underlying data source.\n\n        You can set the following option(s) for reading files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n                in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.", "func_name": "DataFrameReader.option", "language": "python"}, {"code": "def options(self, **options):\n        \"\"\"Adds input options for the underlying data source.\n\n        You can set the following option(s) for reading files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n                in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.\n        \"\"\"\n        for k in options:\n            self._jreader = self._jreader.option(k, to_str(options[k]))\n        return self", "docstring": "Adds input options for the underlying data source.\n\n        You can set the following option(s) for reading files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n                in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.", "func_name": "DataFrameReader.options", "language": "python"}, {"code": "def load(self, path=None, format=None, schema=None, **options):\n        \"\"\"Loads data from a data source and returns it as a :class`DataFrame`.\n\n        :param path: optional string or a list of string for file-system backed data sources.\n        :param format: optional string for format of the data source. Default to 'parquet'.\n        :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n        :param options: all other string options\n\n        >>> df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n        ...     opt1=True, opt2=1, opt3='str')\n        >>> df.dtypes\n        [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n\n        >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n        ...     'python/test_support/sql/people1.json'])\n        >>> df.dtypes\n        [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n        \"\"\"\n        if format is not None:\n            self.format(format)\n        if schema is not None:\n            self.schema(schema)\n        self.options(**options)\n        if isinstance(path, basestring):\n            return self._df(self._jreader.load(path))\n        elif path is not None:\n            if type(path) != list:\n                path = [path]\n            return self._df(self._jreader.load(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n        else:\n            return self._df(self._jreader.load())", "docstring": "Loads data from a data source and returns it as a :class`DataFrame`.\n\n        :param path: optional string or a list of string for file-system backed data sources.\n        :param format: optional string for format of the data source. Default to 'parquet'.\n        :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n        :param options: all other string options\n\n        >>> d", "func_name": "DataFrameReader.load", "language": "python"}, {"code": "def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\n             allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None,\n             allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None,\n             mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None,\n             multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None,\n             dropFieldIfAllNull=None, encoding=None, locale=None):\n        \"\"\"\n        Loads JSON files and returns the results as a :class:`DataFrame`.\n\n        `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n        For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n\n        If the ``schema`` parameter is not specified, this function goes\n        through the input once to determine the input schema.\n\n        :param path: string represents path to the JSON dataset, or a list of paths,\n                     or RDD of Strings storing JSON objects.\n        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema or\n                       a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n        :param primitivesAsString: infers all primitive values as a string type. If None is set,\n                                   it uses the default value, ``false``.\n        :param prefersDecimal: infers all floating-point values as a decimal type. If the values\n                               do not fit in decimal, then it infers them as doubles. If None is\n                               set, it uses the default value, ``false``.\n        :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n                              it uses the default value, ``false``.\n        :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n                                        it uses the default value, ", "docstring": "Loads JSON files and returns the results as a :class:`DataFrame`.\n\n        `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n        For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n\n        If the ``schema`` parameter is not specified, this function goes\n        through the input once to determine the input schema.\n\n        :param path: string represents path to the JSON dataset, or a list of paths,\n                     or RDD of", "func_name": "DataFrameReader.json", "language": "python"}, {"code": "def parquet(self, *paths):\n        \"\"\"Loads Parquet files, returning the result as a :class:`DataFrame`.\n\n        You can set the following Parquet-specific option(s) for reading Parquet files:\n            * ``mergeSchema``: sets whether we should merge schemas collected from all \\\n                Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``. \\\n                The default value is specified in ``spark.sql.parquet.mergeSchema``.\n\n        >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n        >>> df.dtypes\n        [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n        \"\"\"\n        return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))", "docstring": "Loads Parquet files, returning the result as a :class:`DataFrame`.\n\n        You can set the following Parquet-specific option(s) for reading Parquet files:\n            * ``mergeSchema``: sets whether we should merge schemas collected from all \\\n                Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``. \\\n                The default value is specified in ``spark.sql.parquet.mergeSchema``.\n\n        >>> df = spark.read.parquet('python/test_support/sql/parquet_partitio", "func_name": "DataFrameReader.parquet", "language": "python"}, {"code": "def text(self, paths, wholetext=False, lineSep=None):\n        \"\"\"\n        Loads text files and returns a :class:`DataFrame` whose schema starts with a\n        string column named \"value\", and followed by partitioned columns if there\n        are any.\n        The text files must be encoded as UTF-8.\n\n        By default, each line in the text file is a new row in the resulting DataFrame.\n\n        :param paths: string, or list of strings, for input path(s).\n        :param wholetext: if true, read each file from input path(s) as a single row.\n        :param lineSep: defines the line separator that should be used for parsing. If None is\n                        set, it covers all ``\\\\r``, ``\\\\r\\\\n`` and ``\\\\n``.\n\n        >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n        >>> df.collect()\n        [Row(value=u'hello'), Row(value=u'this')]\n        >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n        >>> df.collect()\n        [Row(value=u'hello\\\\nthis')]\n        \"\"\"\n        self._set_opts(wholetext=wholetext, lineSep=lineSep)\n        if isinstance(paths, basestring):\n            paths = [paths]\n        return self._df(self._jreader.text(self._spark._sc._jvm.PythonUtils.toSeq(paths)))", "docstring": "Loads text files and returns a :class:`DataFrame` whose schema starts with a\n        string column named \"value\", and followed by partitioned columns if there\n        are any.\n        The text files must be encoded as UTF-8.\n\n        By default, each line in the text file is a new row in the resulting DataFrame.\n\n        :param paths: string, or list of strings, for input path(s).\n        :param wholetext: if true, read each file from input path(s) as a single row.\n        :param lineSep: define", "func_name": "DataFrameReader.text", "language": "python"}, {"code": "def csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None,\n            comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None,\n            ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None,\n            negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None,\n            maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None,\n            columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None,\n            samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None):\n        r\"\"\"Loads a CSV file and returns the result as a  :class:`DataFrame`.\n\n        This function will go through the input once to determine the input schema if\n        ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n        ``inferSchema`` option or specify the schema explicitly using ``schema``.\n\n        :param path: string, or list of strings, for input path(s),\n                     or RDD of Strings storing CSV rows.\n        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n        :param sep: sets a single character as a separator for each field and value.\n                    If None is set, it uses the default value, ``,``.\n        :param encoding: decodes the CSV files by the given encoding type. If None is set,\n                         it uses the default value, ``UTF-8``.\n        :param quote: sets a single character used for escaping quoted values where the\n                      separator can be part of the value. If None is set, it uses the default\n                      value, ``\"``. If you would like to turn off quotations, you need to set an\n                      empty string.\n        :param escape: sets a single character used for escaping quotes inside an already\n                      ", "docstring": "r\"\"\"Loads a CSV file and returns the result as a  :class:`DataFrame`.\n\n        This function will go through the input once to determine the input schema if\n        ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n        ``inferSchema`` option or specify the schema explicitly using ``schema``.\n\n        :param path: string, or list of strings, for input path(s),\n                     or RDD of Strings storing CSV rows.\n        :param schema: an optional :class:`pys", "func_name": "DataFrameReader.csv", "language": "python"}, {"code": "def orc(self, path):\n        \"\"\"Loads ORC files, returning the result as a :class:`DataFrame`.\n\n        >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n        >>> df.dtypes\n        [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\n        \"\"\"\n        if isinstance(path, basestring):\n            path = [path]\n        return self._df(self._jreader.orc(_to_seq(self._spark._sc, path)))", "docstring": "Loads ORC files, returning the result as a :class:`DataFrame`.\n\n        >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n        >>> df.dtypes\n        [('a', 'bigint'), ('b', 'int'), ('c', 'int')]", "func_name": "DataFrameReader.orc", "language": "python"}, {"code": "def jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None,\n             predicates=None, properties=None):\n        \"\"\"\n        Construct a :class:`DataFrame` representing the database table named ``table``\n        accessible via JDBC URL ``url`` and connection ``properties``.\n\n        Partitions of the table will be retrieved in parallel if either ``column`` or\n        ``predicates`` is specified. ``lowerBound`, ``upperBound`` and ``numPartitions``\n        is needed when ``column`` is specified.\n\n        If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n\n        .. note:: Don't create too many partitions in parallel on a large cluster;\n            otherwise Spark might crash your external database systems.\n\n        :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n        :param table: the name of the table\n        :param column: the name of an integer column that will be used for partitioning;\n                       if this parameter is specified, then ``numPartitions``, ``lowerBound``\n                       (inclusive), and ``upperBound`` (exclusive) will form partition strides\n                       for generated WHERE clause expressions used to split the column\n                       ``column`` evenly\n        :param lowerBound: the minimum value of ``column`` used to decide partition stride\n        :param upperBound: the maximum value of ``column`` used to decide partition stride\n        :param numPartitions: the number of partitions\n        :param predicates: a list of expressions suitable for inclusion in WHERE clauses;\n                           each one defines one partition of the :class:`DataFrame`\n        :param properties: a dictionary of JDBC database connection arguments. Normally at\n                           least properties \"user\" and \"password\" with their corresponding values.\n                           For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n        :", "docstring": "Construct a :class:`DataFrame` representing the database table named ``table``\n        accessible via JDBC URL ``url`` and connection ``properties``.\n\n        Partitions of the table will be retrieved in parallel if either ``column`` or\n        ``predicates`` is specified. ``lowerBound`, ``upperBound`` and ``numPartitions``\n        is needed when ``column`` is specified.\n\n        If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n\n        .. note:: Don't create too man", "func_name": "DataFrameReader.jdbc", "language": "python"}, {"code": "def mode(self, saveMode):\n        \"\"\"Specifies the behavior when data or table already exists.\n\n        Options include:\n\n        * `append`: Append contents of this :class:`DataFrame` to existing data.\n        * `overwrite`: Overwrite existing data.\n        * `error` or `errorifexists`: Throw an exception if data already exists.\n        * `ignore`: Silently ignore this operation if data already exists.\n\n        >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n        \"\"\"\n        # At the JVM side, the default value of mode is already set to \"error\".\n        # So, if the given saveMode is None, we will not call JVM-side's mode method.\n        if saveMode is not None:\n            self._jwrite = self._jwrite.mode(saveMode)\n        return self", "docstring": "Specifies the behavior when data or table already exists.\n\n        Options include:\n\n        * `append`: Append contents of this :class:`DataFrame` to existing data.\n        * `overwrite`: Overwrite existing data.\n        * `error` or `errorifexists`: Throw an exception if data already exists.\n        * `ignore`: Silently ignore this operation if data already exists.\n\n        >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))", "func_name": "DataFrameWriter.mode", "language": "python"}, {"code": "def format(self, source):\n        \"\"\"Specifies the underlying output data source.\n\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\n\n        >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))\n        \"\"\"\n        self._jwrite = self._jwrite.format(source)\n        return self", "docstring": "Specifies the underlying output data source.\n\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\n\n        >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))", "func_name": "DataFrameWriter.format", "language": "python"}, {"code": "def option(self, key, value):\n        \"\"\"Adds an output option for the underlying data source.\n\n        You can set the following option(s) for writing files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to format\n                timestamps in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.\n        \"\"\"\n        self._jwrite = self._jwrite.option(key, to_str(value))\n        return self", "docstring": "Adds an output option for the underlying data source.\n\n        You can set the following option(s) for writing files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to format\n                timestamps in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.", "func_name": "DataFrameWriter.option", "language": "python"}, {"code": "def options(self, **options):\n        \"\"\"Adds output options for the underlying data source.\n\n        You can set the following option(s) for writing files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to format\n                timestamps in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.\n        \"\"\"\n        for k in options:\n            self._jwrite = self._jwrite.option(k, to_str(options[k]))\n        return self", "docstring": "Adds output options for the underlying data source.\n\n        You can set the following option(s) for writing files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to format\n                timestamps in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.", "func_name": "DataFrameWriter.options", "language": "python"}, {"code": "def partitionBy(self, *cols):\n        \"\"\"Partitions the output by the given columns on the file system.\n\n        If specified, the output is laid out on the file system similar\n        to Hive's partitioning scheme.\n\n        :param cols: name of columns\n\n        >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n        \"\"\"\n        if len(cols) == 1 and isinstance(cols[0], (list, tuple)):\n            cols = cols[0]\n        self._jwrite = self._jwrite.partitionBy(_to_seq(self._spark._sc, cols))\n        return self", "docstring": "Partitions the output by the given columns on the file system.\n\n        If specified, the output is laid out on the file system similar\n        to Hive's partitioning scheme.\n\n        :param cols: name of columns\n\n        >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))", "func_name": "DataFrameWriter.partitionBy", "language": "python"}, {"code": "def sortBy(self, col, *cols):\n        \"\"\"Sorts the output in each bucket by the given columns on the file system.\n\n        :param col: a name of a column, or a list of names.\n        :param cols: additional names (optional). If `col` is a list it should be empty.\n\n        >>> (df.write.format('parquet')  # doctest: +SKIP\n        ...     .bucketBy(100, 'year', 'month')\n        ...     .sortBy('day')\n        ...     .mode(\"overwrite\")\n        ...     .saveAsTable('sorted_bucketed_table'))\n        \"\"\"\n        if isinstance(col, (list, tuple)):\n            if cols:\n                raise ValueError(\"col is a {0} but cols are not empty\".format(type(col)))\n\n            col, cols = col[0], col[1:]\n\n        if not all(isinstance(c, basestring) for c in cols) or not(isinstance(col, basestring)):\n            raise TypeError(\"all names should be `str`\")\n\n        self._jwrite = self._jwrite.sortBy(col, _to_seq(self._spark._sc, cols))\n        return self", "docstring": "Sorts the output in each bucket by the given columns on the file system.\n\n        :param col: a name of a column, or a list of names.\n        :param cols: additional names (optional). If `col` is a list it should be empty.\n\n        >>> (df.write.format('parquet')  # doctest: +SKIP\n        ...     .bucketBy(100, 'year', 'month')\n        ...     .sortBy('day')\n        ...     .mode(\"overwrite\")\n        ...     .saveAsTable('sorted_bucketed_table'))", "func_name": "DataFrameWriter.sortBy", "language": "python"}, {"code": "def save(self, path=None, format=None, mode=None, partitionBy=None, **options):\n        \"\"\"Saves the contents of the :class:`DataFrame` to a data source.\n\n        The data source is specified by the ``format`` and a set of ``options``.\n        If ``format`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used.\n\n        :param path: the path in a Hadoop supported file system\n        :param format: the format used to save\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n                exists.\n        :param partitionBy: names of partitioning columns\n        :param options: all other string options\n\n        >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n        \"\"\"\n        self.mode(mode).options(**options)\n        if partitionBy is not None:\n            self.partitionBy(partitionBy)\n        if format is not None:\n            self.format(format)\n        if path is None:\n            self._jwrite.save()\n        else:\n            self._jwrite.save(path)", "docstring": "Saves the contents of the :class:`DataFrame` to a data source.\n\n        The data source is specified by the ``format`` and a set of ``options``.\n        If ``format`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used.\n\n        :param path: the path in a Hadoop supported file system\n        :param format: the format used to save\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``appe", "func_name": "DataFrameWriter.save", "language": "python"}, {"code": "def insertInto(self, tableName, overwrite=False):\n        \"\"\"Inserts the content of the :class:`DataFrame` to the specified table.\n\n        It requires that the schema of the class:`DataFrame` is the same as the\n        schema of the table.\n\n        Optionally overwriting any existing data.\n        \"\"\"\n        self._jwrite.mode(\"overwrite\" if overwrite else \"append\").insertInto(tableName)", "docstring": "Inserts the content of the :class:`DataFrame` to the specified table.\n\n        It requires that the schema of the class:`DataFrame` is the same as the\n        schema of the table.\n\n        Optionally overwriting any existing data.", "func_name": "DataFrameWriter.insertInto", "language": "python"}, {"code": "def saveAsTable(self, name, format=None, mode=None, partitionBy=None, **options):\n        \"\"\"Saves the content of the :class:`DataFrame` as the specified table.\n\n        In the case the table already exists, behavior of this function depends on the\n        save mode, specified by the `mode` function (default to throwing an exception).\n        When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n        the same as that of the existing table.\n\n        * `append`: Append contents of this :class:`DataFrame` to existing data.\n        * `overwrite`: Overwrite existing data.\n        * `error` or `errorifexists`: Throw an exception if data already exists.\n        * `ignore`: Silently ignore this operation if data already exists.\n\n        :param name: the table name\n        :param format: the format used to save\n        :param mode: one of `append`, `overwrite`, `error`, `errorifexists`, `ignore` \\\n                     (default: error)\n        :param partitionBy: names of partitioning columns\n        :param options: all other string options\n        \"\"\"\n        self.mode(mode).options(**options)\n        if partitionBy is not None:\n            self.partitionBy(partitionBy)\n        if format is not None:\n            self.format(format)\n        self._jwrite.saveAsTable(name)", "docstring": "Saves the content of the :class:`DataFrame` as the specified table.\n\n        In the case the table already exists, behavior of this function depends on the\n        save mode, specified by the `mode` function (default to throwing an exception).\n        When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n        the same as that of the existing table.\n\n        * `append`: Append contents of this :class:`DataFrame` to existing data.\n        * `overwrite`: Overwrite ", "func_name": "DataFrameWriter.saveAsTable", "language": "python"}, {"code": "def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n             lineSep=None, encoding=None):\n        \"\"\"Saves the content of the :class:`DataFrame` in JSON format\n        (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n        specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n                exists.\n        :param compression: compression codec to use when saving to file. This can be one of the\n                            known case-insensitive shorten names (none, bzip2, gzip, lz4,\n                            snappy and deflate).\n        :param dateFormat: sets the string that indicates a date format. Custom date formats\n                           follow the formats at ``java.time.format.DateTimeFormatter``. This\n                           applies to date type. If None is set, it uses the\n                           default value, ``yyyy-MM-dd``.\n        :param timestampFormat: sets the string that indicates a timestamp format.\n                                Custom date formats follow the formats at\n                                ``java.time.format.DateTimeFormatter``.\n                                This applies to timestamp type. If None is set, it uses the\n                                default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n        :param encoding: specifies encoding (charset) of saved json files. If None is set,\n                        the default UTF-8 charset will be used.\n        :param lineSep: defines the line separator that should", "docstring": "Saves the content of the :class:`DataFrame` in JSON format\n        (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n        specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore`", "func_name": "DataFrameWriter.json", "language": "python"}, {"code": "def parquet(self, path, mode=None, partitionBy=None, compression=None):\n        \"\"\"Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n                exists.\n        :param partitionBy: names of partitioning columns\n        :param compression: compression codec to use when saving to file. This can be one of the\n                            known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n                            lzo, brotli, lz4, and zstd). This will override\n                            ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n                            value specified in ``spark.sql.parquet.compression.codec``.\n\n        >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n        \"\"\"\n        self.mode(mode)\n        if partitionBy is not None:\n            self.partitionBy(partitionBy)\n        self._set_opts(compression=compression)\n        self._jwrite.parquet(path)", "docstring": "Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``erro", "func_name": "DataFrameWriter.parquet", "language": "python"}, {"code": "def text(self, path, compression=None, lineSep=None):\n        \"\"\"Saves the content of the DataFrame in a text file at the specified path.\n        The text files will be encoded as UTF-8.\n\n        :param path: the path in any Hadoop supported file system\n        :param compression: compression codec to use when saving to file. This can be one of the\n                            known case-insensitive shorten names (none, bzip2, gzip, lz4,\n                            snappy and deflate).\n        :param lineSep: defines the line separator that should be used for writing. If None is\n                        set, it uses the default value, ``\\\\n``.\n\n        The DataFrame must have only one column that is of string type.\n        Each row becomes a new line in the output file.\n        \"\"\"\n        self._set_opts(compression=compression, lineSep=lineSep)\n        self._jwrite.text(path)", "docstring": "Saves the content of the DataFrame in a text file at the specified path.\n        The text files will be encoded as UTF-8.\n\n        :param path: the path in any Hadoop supported file system\n        :param compression: compression codec to use when saving to file. This can be one of the\n                            known case-insensitive shorten names (none, bzip2, gzip, lz4,\n                            snappy and deflate).\n        :param lineSep: defines the line separator that should be used for ", "func_name": "DataFrameWriter.text", "language": "python"}, {"code": "def csv(self, path, mode=None, compression=None, sep=None, quote=None, escape=None,\n            header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None,\n            timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None,\n            charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None):\n        r\"\"\"Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n                exists.\n\n        :param compression: compression codec to use when saving to file. This can be one of the\n                            known case-insensitive shorten names (none, bzip2, gzip, lz4,\n                            snappy and deflate).\n        :param sep: sets a single character as a separator for each field and value. If None is\n                    set, it uses the default value, ``,``.\n        :param quote: sets a single character used for escaping quoted values where the\n                      separator can be part of the value. If None is set, it uses the default\n                      value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n        :param escape: sets a single character used for escaping quotes inside an already\n                       quoted value. If None is set, it uses the default value, ``\\``\n        :param escapeQuotes: a flag indicating whether values containing quotes should always\n                             be enclosed in quotes. If None is set, it uses the default value\n    ", "docstring": "r\"\"\"Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``erro", "func_name": "DataFrameWriter.csv", "language": "python"}, {"code": "def orc(self, path, mode=None, partitionBy=None, compression=None):\n        \"\"\"Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n                exists.\n        :param partitionBy: names of partitioning columns\n        :param compression: compression codec to use when saving to file. This can be one of the\n                            known case-insensitive shorten names (none, snappy, zlib, and lzo).\n                            This will override ``orc.compress`` and\n                            ``spark.sql.orc.compression.codec``. If None is set, it uses the value\n                            specified in ``spark.sql.orc.compression.codec``.\n\n        >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n        >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\n        \"\"\"\n        self.mode(mode)\n        if partitionBy is not None:\n            self.partitionBy(partitionBy)\n        self._set_opts(compression=compression)\n        self._jwrite.orc(path)", "docstring": "Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorife", "func_name": "DataFrameWriter.orc", "language": "python"}, {"code": "def jdbc(self, url, table, mode=None, properties=None):\n        \"\"\"Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n\n        .. note:: Don't create too many partitions in parallel on a large cluster;\n            otherwise Spark might crash your external database systems.\n\n        :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n        :param table: Name of the table in the external database.\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n                exists.\n        :param properties: a dictionary of JDBC database connection arguments. Normally at\n                           least properties \"user\" and \"password\" with their corresponding values.\n                           For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n        \"\"\"\n        if properties is None:\n            properties = dict()\n        jprop = JavaClass(\"java.util.Properties\", self._spark._sc._gateway._gateway_client)()\n        for k in properties:\n            jprop.setProperty(k, properties[k])\n        self.mode(mode)._jwrite.jdbc(url, table, jprop)", "docstring": "Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n\n        .. note:: Don't create too many partitions in parallel on a large cluster;\n            otherwise Spark might crash your external database systems.\n\n        :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n        :param table: Name of the table in the external database.\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Ap", "func_name": "DataFrameWriter.jdbc", "language": "python"}, {"code": "def createStream(ssc, kinesisAppName, streamName, endpointUrl, regionName,\n                     initialPositionInStream, checkpointInterval,\n                     storageLevel=StorageLevel.MEMORY_AND_DISK_2,\n                     awsAccessKeyId=None, awsSecretKey=None, decoder=utf8_decoder,\n                     stsAssumeRoleArn=None, stsSessionName=None, stsExternalId=None):\n        \"\"\"\n        Create an input stream that pulls messages from a Kinesis stream. This uses the\n        Kinesis Client Library (KCL) to pull messages from Kinesis.\n\n        .. note:: The given AWS credentials will get saved in DStream checkpoints if checkpointing\n            is enabled. Make sure that your checkpoint directory is secure.\n\n        :param ssc:  StreamingContext object\n        :param kinesisAppName:  Kinesis application name used by the Kinesis Client Library (KCL) to\n                                update DynamoDB\n        :param streamName:  Kinesis stream name\n        :param endpointUrl:  Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)\n        :param regionName:  Name of region used by the Kinesis Client Library (KCL) to update\n                            DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)\n        :param initialPositionInStream:  In the absence of Kinesis checkpoint info, this is the\n                                         worker's initial starting position in the stream. The\n                                         values are either the beginning of the stream per Kinesis'\n                                         limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or\n                                         the tip of the stream (InitialPositionInStream.LATEST).\n        :param checkpointInterval:  Checkpoint interval for Kinesis checkpointing. See the Kinesis\n                                    Spark Streaming documentation for more details on the different\n                                    types of checkpoin", "docstring": "Create an input stream that pulls messages from a Kinesis stream. This uses the\n        Kinesis Client Library (KCL) to pull messages from Kinesis.\n\n        .. note:: The given AWS credentials will get saved in DStream checkpoints if checkpointing\n            is enabled. Make sure that your checkpoint directory is secure.\n\n        :param ssc:  StreamingContext object\n        :param kinesisAppName:  Kinesis application name used by the Kinesis Client Library (KCL) to\n                             ", "func_name": "KinesisUtils.createStream", "language": "python"}, {"code": "def choose_jira_assignee(issue, asf_jira):\n    \"\"\"\n    Prompt the user to choose who to assign the issue to in jira, given a list of candidates,\n    including the original reporter and all commentors\n    \"\"\"\n    while True:\n        try:\n            reporter = issue.fields.reporter\n            commentors = map(lambda x: x.author, issue.fields.comment.comments)\n            candidates = set(commentors)\n            candidates.add(reporter)\n            candidates = list(candidates)\n            print(\"JIRA is unassigned, choose assignee\")\n            for idx, author in enumerate(candidates):\n                if author.key == \"apachespark\":\n                    continue\n                annotations = [\"Reporter\"] if author == reporter else []\n                if author in commentors:\n                    annotations.append(\"Commentor\")\n                print(\"[%d] %s (%s)\" % (idx, author.displayName, \",\".join(annotations)))\n            raw_assignee = input(\n                \"Enter number of user, or userid, to assign to (blank to leave unassigned):\")\n            if raw_assignee == \"\":\n                return None\n            else:\n                try:\n                    id = int(raw_assignee)\n                    assignee = candidates[id]\n                except:\n                    # assume it's a user id, and try to assign (might fail, we just prompt again)\n                    assignee = asf_jira.user(raw_assignee)\n                asf_jira.assign_issue(issue.key, assignee.key)\n                return assignee\n        except KeyboardInterrupt:\n            raise\n        except:\n            traceback.print_exc()\n            print(\"Error assigning JIRA, try again (or leave blank and fix manually)\")", "docstring": "Prompt the user to choose who to assign the issue to in jira, given a list of candidates,\n    including the original reporter and all commentors", "func_name": "choose_jira_assignee", "language": "python"}, {"code": "def standardize_jira_ref(text):\n    \"\"\"\n    Standardize the [SPARK-XXXXX] [MODULE] prefix\n    Converts \"[SPARK-XXX][mllib] Issue\", \"[MLLib] SPARK-XXX. Issue\" or \"SPARK XXX [MLLIB]: Issue\" to\n    \"[SPARK-XXX][MLLIB] Issue\"\n\n    >>> standardize_jira_ref(\n    ...     \"[SPARK-5821] [SQL] ParquetRelation2 CTAS should check if delete is successful\")\n    '[SPARK-5821][SQL] ParquetRelation2 CTAS should check if delete is successful'\n    >>> standardize_jira_ref(\n    ...     \"[SPARK-4123][Project Infra][WIP]: Show new dependencies added in pull requests\")\n    '[SPARK-4123][PROJECT INFRA][WIP] Show new dependencies added in pull requests'\n    >>> standardize_jira_ref(\"[MLlib] Spark  5954: Top by key\")\n    '[SPARK-5954][MLLIB] Top by key'\n    >>> standardize_jira_ref(\"[SPARK-979] a LRU scheduler for load balancing in TaskSchedulerImpl\")\n    '[SPARK-979] a LRU scheduler for load balancing in TaskSchedulerImpl'\n    >>> standardize_jira_ref(\n    ...     \"SPARK-1094 Support MiMa for reporting binary compatibility across versions.\")\n    '[SPARK-1094] Support MiMa for reporting binary compatibility across versions.'\n    >>> standardize_jira_ref(\"[WIP]  [SPARK-1146] Vagrant support for Spark\")\n    '[SPARK-1146][WIP] Vagrant support for Spark'\n    >>> standardize_jira_ref(\n    ...     \"SPARK-1032. If Yarn app fails before registering, app master stays aroun...\")\n    '[SPARK-1032] If Yarn app fails before registering, app master stays aroun...'\n    >>> standardize_jira_ref(\n    ...     \"[SPARK-6250][SPARK-6146][SPARK-5911][SQL] Types are now reserved words in DDL parser.\")\n    '[SPARK-6250][SPARK-6146][SPARK-5911][SQL] Types are now reserved words in DDL parser.'\n    >>> standardize_jira_ref(\"Additional information for users building from source code\")\n    'Additional information for users building from source code'\n    \"\"\"\n    jira_refs = []\n    components = []\n\n    # If the string is compliant, no need to process any further\n    if (re.search(r'^\\[SPARK-[0-9]{3,6}\\](\\[[A-Z0-9_\\s,]+\\]", "docstring": "Standardize the [SPARK-XXXXX] [MODULE] prefix\n    Converts \"[SPARK-XXX][mllib] Issue\", \"[MLLib] SPARK-XXX. Issue\" or \"SPARK XXX [MLLIB]: Issue\" to\n    \"[SPARK-XXX][MLLIB] Issue\"\n\n    >>> standardize_jira_ref(\n    ...     \"[SPARK-5821] [SQL] ParquetRelation2 CTAS should check if delete is successful\")\n    '[SPARK-5821][SQL] ParquetRelation2 CTAS should check if delete is successful'\n    >>> standardize_jira_ref(\n    ...     \"[SPARK-4123][Project Infra][WIP]: Show new dependencies added in pull re", "func_name": "standardize_jira_ref", "language": "python"}, {"code": "def _parse_libsvm_line(line):\n        \"\"\"\n        Parses a line in LIBSVM format into (label, indices, values).\n        \"\"\"\n        items = line.split(None)\n        label = float(items[0])\n        nnz = len(items) - 1\n        indices = np.zeros(nnz, dtype=np.int32)\n        values = np.zeros(nnz)\n        for i in xrange(nnz):\n            index, value = items[1 + i].split(\":\")\n            indices[i] = int(index) - 1\n            values[i] = float(value)\n        return label, indices, values", "docstring": "Parses a line in LIBSVM format into (label, indices, values).", "func_name": "MLUtils._parse_libsvm_line", "language": "python"}, {"code": "def _convert_labeled_point_to_libsvm(p):\n        \"\"\"Converts a LabeledPoint to a string in LIBSVM format.\"\"\"\n        from pyspark.mllib.regression import LabeledPoint\n        assert isinstance(p, LabeledPoint)\n        items = [str(p.label)]\n        v = _convert_to_vector(p.features)\n        if isinstance(v, SparseVector):\n            nnz = len(v.indices)\n            for i in xrange(nnz):\n                items.append(str(v.indices[i] + 1) + \":\" + str(v.values[i]))\n        else:\n            for i in xrange(len(v)):\n                items.append(str(i + 1) + \":\" + str(v[i]))\n        return \" \".join(items)", "docstring": "Converts a LabeledPoint to a string in LIBSVM format.", "func_name": "MLUtils._convert_labeled_point_to_libsvm", "language": "python"}, {"code": "def loadLibSVMFile(sc, path, numFeatures=-1, minPartitions=None):\n        \"\"\"\n        Loads labeled data in the LIBSVM format into an RDD of\n        LabeledPoint. The LIBSVM format is a text-based format used by\n        LIBSVM and LIBLINEAR. Each line represents a labeled sparse\n        feature vector using the following format:\n\n        label index1:value1 index2:value2 ...\n\n        where the indices are one-based and in ascending order. This\n        method parses each line into a LabeledPoint, where the feature\n        indices are converted to zero-based.\n\n        :param sc: Spark context\n        :param path: file or directory path in any Hadoop-supported file\n                     system URI\n        :param numFeatures: number of features, which will be determined\n                            from the input data if a nonpositive value\n                            is given. This is useful when the dataset is\n                            already split into multiple files and you\n                            want to load them separately, because some\n                            features may not present in certain files,\n                            which leads to inconsistent feature\n                            dimensions.\n        :param minPartitions: min number of partitions\n        @return: labeled data stored as an RDD of LabeledPoint\n\n        >>> from tempfile import NamedTemporaryFile\n        >>> from pyspark.mllib.util import MLUtils\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> _ = tempFile.write(b\"+1 1:1.0 3:2.0 5:3.0\\\\n-1\\\\n-1 2:4.0 4:5.0 6:6.0\")\n        >>> tempFile.flush()\n        >>> examples = MLUtils.loadLibSVMFile(sc, tempFile.name).collect()\n        >>> tempFile.close()\n        >>> examples[0]\n        LabeledPoint(1.0, (6,[0,2,4],[1.0,2.0,3.0]))\n        >>> examples[1]\n        LabeledPoint(-1.0, (6,[],[]))\n        >>> examples[2]\n        LabeledPoint(-1.0, (6,[1,3,5],[4.0,5.", "docstring": "Loads labeled data in the LIBSVM format into an RDD of\n        LabeledPoint. The LIBSVM format is a text-based format used by\n        LIBSVM and LIBLINEAR. Each line represents a labeled sparse\n        feature vector using the following format:\n\n        label index1:value1 index2:value2 ...\n\n        where the indices are one-based and in ascending order. This\n        method parses each line into a LabeledPoint, where the feature\n        indices are converted to zero-based.\n\n        :param sc: Sp", "func_name": "MLUtils.loadLibSVMFile", "language": "python"}, {"code": "def saveAsLibSVMFile(data, dir):\n        \"\"\"\n        Save labeled data in LIBSVM format.\n\n        :param data: an RDD of LabeledPoint to be saved\n        :param dir: directory to save the data\n\n        >>> from tempfile import NamedTemporaryFile\n        >>> from fileinput import input\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from glob import glob\n        >>> from pyspark.mllib.util import MLUtils\n        >>> examples = [LabeledPoint(1.1, Vectors.sparse(3, [(0, 1.23), (2, 4.56)])),\n        ...             LabeledPoint(0.0, Vectors.dense([1.01, 2.02, 3.03]))]\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> MLUtils.saveAsLibSVMFile(sc.parallelize(examples), tempFile.name)\n        >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n        '0.0 1:1.01 2:2.02 3:3.03\\\\n1.1 1:1.23 3:4.56\\\\n'\n        \"\"\"\n        lines = data.map(lambda p: MLUtils._convert_labeled_point_to_libsvm(p))\n        lines.saveAsTextFile(dir)", "docstring": "Save labeled data in LIBSVM format.\n\n        :param data: an RDD of LabeledPoint to be saved\n        :param dir: directory to save the data\n\n        >>> from tempfile import NamedTemporaryFile\n        >>> from fileinput import input\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from glob import glob\n        >>> from pyspark.mllib.util import MLUtils\n        >>> examples = [LabeledPoint(1.1, Vectors.sparse(3, [(0, 1.23), (2, 4.56)])),\n        ...             LabeledPoi", "func_name": "MLUtils.saveAsLibSVMFile", "language": "python"}, {"code": "def loadLabeledPoints(sc, path, minPartitions=None):\n        \"\"\"\n        Load labeled points saved using RDD.saveAsTextFile.\n\n        :param sc: Spark context\n        :param path: file or directory path in any Hadoop-supported file\n                     system URI\n        :param minPartitions: min number of partitions\n        @return: labeled data stored as an RDD of LabeledPoint\n\n        >>> from tempfile import NamedTemporaryFile\n        >>> from pyspark.mllib.util import MLUtils\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> examples = [LabeledPoint(1.1, Vectors.sparse(3, [(0, -1.23), (2, 4.56e-7)])),\n        ...             LabeledPoint(0.0, Vectors.dense([1.01, 2.02, 3.03]))]\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> sc.parallelize(examples, 1).saveAsTextFile(tempFile.name)\n        >>> MLUtils.loadLabeledPoints(sc, tempFile.name).collect()\n        [LabeledPoint(1.1, (3,[0,2],[-1.23,4.56e-07])), LabeledPoint(0.0, [1.01,2.02,3.03])]\n        \"\"\"\n        minPartitions = minPartitions or min(sc.defaultParallelism, 2)\n        return callMLlibFunc(\"loadLabeledPoints\", sc, path, minPartitions)", "docstring": "Load labeled points saved using RDD.saveAsTextFile.\n\n        :param sc: Spark context\n        :param path: file or directory path in any Hadoop-supported file\n                     system URI\n        :param minPartitions: min number of partitions\n        @return: labeled data stored as an RDD of LabeledPoint\n\n        >>> from tempfile import NamedTemporaryFile\n        >>> from pyspark.mllib.util import MLUtils\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> examples = [L", "func_name": "MLUtils.loadLabeledPoints", "language": "python"}, {"code": "def appendBias(data):\n        \"\"\"\n        Returns a new vector with `1.0` (bias) appended to\n        the end of the input vector.\n        \"\"\"\n        vec = _convert_to_vector(data)\n        if isinstance(vec, SparseVector):\n            newIndices = np.append(vec.indices, len(vec))\n            newValues = np.append(vec.values, 1.0)\n            return SparseVector(len(vec) + 1, newIndices, newValues)\n        else:\n            return _convert_to_vector(np.append(vec.toArray(), 1.0))", "docstring": "Returns a new vector with `1.0` (bias) appended to\n        the end of the input vector.", "func_name": "MLUtils.appendBias", "language": "python"}, {"code": "def convertVectorColumnsToML(dataset, *cols):\n        \"\"\"\n        Converts vector columns in an input DataFrame from the\n        :py:class:`pyspark.mllib.linalg.Vector` type to the new\n        :py:class:`pyspark.ml.linalg.Vector` type under the `spark.ml`\n        package.\n\n        :param dataset:\n          input dataset\n        :param cols:\n          a list of vector columns to be converted.\n          New vector columns will be ignored. If unspecified, all old\n          vector columns will be converted excepted nested ones.\n        :return:\n          the input dataset with old vector columns converted to the\n          new vector type\n\n        >>> import pyspark\n        >>> from pyspark.mllib.linalg import Vectors\n        >>> from pyspark.mllib.util import MLUtils\n        >>> df = spark.createDataFrame(\n        ...     [(0, Vectors.sparse(2, [1], [1.0]), Vectors.dense(2.0, 3.0))],\n        ...     [\"id\", \"x\", \"y\"])\n        >>> r1 = MLUtils.convertVectorColumnsToML(df).first()\n        >>> isinstance(r1.x, pyspark.ml.linalg.SparseVector)\n        True\n        >>> isinstance(r1.y, pyspark.ml.linalg.DenseVector)\n        True\n        >>> r2 = MLUtils.convertVectorColumnsToML(df, \"x\").first()\n        >>> isinstance(r2.x, pyspark.ml.linalg.SparseVector)\n        True\n        >>> isinstance(r2.y, pyspark.mllib.linalg.DenseVector)\n        True\n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise TypeError(\"Input dataset must be a DataFrame but got {}.\".format(type(dataset)))\n        return callMLlibFunc(\"convertVectorColumnsToML\", dataset, list(cols))", "docstring": "Converts vector columns in an input DataFrame from the\n        :py:class:`pyspark.mllib.linalg.Vector` type to the new\n        :py:class:`pyspark.ml.linalg.Vector` type under the `spark.ml`\n        package.\n\n        :param dataset:\n          input dataset\n        :param cols:\n          a list of vector columns to be converted.\n          New vector columns will be ignored. If unspecified, all old\n          vector columns will be converted excepted nested ones.\n        :return:\n          the input", "func_name": "MLUtils.convertVectorColumnsToML", "language": "python"}, {"code": "def generateLinearInput(intercept, weights, xMean, xVariance,\n                            nPoints, seed, eps):\n        \"\"\"\n        :param: intercept bias factor, the term c in X'w + c\n        :param: weights   feature vector, the term w in X'w + c\n        :param: xMean     Point around which the data X is centered.\n        :param: xVariance Variance of the given data\n        :param: nPoints   Number of points to be generated\n        :param: seed      Random Seed\n        :param: eps       Used to scale the noise. If eps is set high,\n                          the amount of gaussian noise added is more.\n\n        Returns a list of LabeledPoints of length nPoints\n        \"\"\"\n        weights = [float(weight) for weight in weights]\n        xMean = [float(mean) for mean in xMean]\n        xVariance = [float(var) for var in xVariance]\n        return list(callMLlibFunc(\n            \"generateLinearInputWrapper\", float(intercept), weights, xMean,\n            xVariance, int(nPoints), int(seed), float(eps)))", "docstring": ":param: intercept bias factor, the term c in X'w + c\n        :param: weights   feature vector, the term w in X'w + c\n        :param: xMean     Point around which the data X is centered.\n        :param: xVariance Variance of the given data\n        :param: nPoints   Number of points to be generated\n        :param: seed      Random Seed\n        :param: eps       Used to scale the noise. If eps is set high,\n                          the amount of gaussian noise added is more.\n\n        Returns a list", "func_name": "LinearDataGenerator.generateLinearInput", "language": "python"}, {"code": "def generateLinearRDD(sc, nexamples, nfeatures, eps,\n                          nParts=2, intercept=0.0):\n        \"\"\"\n        Generate an RDD of LabeledPoints.\n        \"\"\"\n        return callMLlibFunc(\n            \"generateLinearRDDWrapper\", sc, int(nexamples), int(nfeatures),\n            float(eps), int(nParts), float(intercept))", "docstring": "Generate an RDD of LabeledPoints.", "func_name": "LinearDataGenerator.generateLinearRDD", "language": "python"}, {"code": "def train(cls, data, iterations=100, step=1.0, miniBatchFraction=1.0,\n              initialWeights=None, regParam=0.0, regType=None, intercept=False,\n              validateData=True, convergenceTol=0.001):\n        \"\"\"\n        Train a linear regression model using Stochastic Gradient\n        Descent (SGD). This solves the least squares regression\n        formulation\n\n            f(weights) = 1/(2n) ||A weights - y||^2\n\n        which is the mean squared error. Here the data matrix has n rows,\n        and the input RDD holds the set of rows of A, each with its\n        corresponding right hand side label y.\n        See also the documentation for the precise formulation.\n\n        :param data:\n          The training data, an RDD of LabeledPoint.\n        :param iterations:\n          The number of iterations.\n          (default: 100)\n        :param step:\n          The step parameter used in SGD.\n          (default: 1.0)\n        :param miniBatchFraction:\n          Fraction of data to be used for each SGD iteration.\n          (default: 1.0)\n        :param initialWeights:\n          The initial weights.\n          (default: None)\n        :param regParam:\n          The regularizer parameter.\n          (default: 0.0)\n        :param regType:\n          The type of regularizer used for training our model.\n          Supported values:\n\n            - \"l1\" for using L1 regularization\n            - \"l2\" for using L2 regularization\n            - None for no regularization (default)\n        :param intercept:\n          Boolean parameter which indicates the use or not of the\n          augmented representation for training data (i.e., whether bias\n          features are activated or not).\n          (default: False)\n        :param validateData:\n          Boolean parameter which indicates if the algorithm should\n          validate data before training.\n          (default: True)\n        :param convergenceTol:\n          A condition which decides iteration termination.\n          (default: 0.001)\n  ", "docstring": "Train a linear regression model using Stochastic Gradient\n        Descent (SGD). This solves the least squares regression\n        formulation\n\n            f(weights) = 1/(2n) ||A weights - y||^2\n\n        which is the mean squared error. Here the data matrix has n rows,\n        and the input RDD holds the set of rows of A, each with its\n        corresponding right hand side label y.\n        See also the documentation for the precise formulation.\n\n        :param data:\n          The training data, ", "func_name": "LinearRegressionWithSGD.train", "language": "python"}, {"code": "def predict(self, x):\n        \"\"\"\n        Predict labels for provided features.\n        Using a piecewise linear function.\n        1) If x exactly matches a boundary then associated prediction\n        is returned. In case there are multiple predictions with the\n        same boundary then one of them is returned. Which one is\n        undefined (same as java.util.Arrays.binarySearch).\n        2) If x is lower or higher than all boundaries then first or\n        last prediction is returned respectively. In case there are\n        multiple predictions with the same boundary then the lowest\n        or highest is returned respectively.\n        3) If x falls between two values in boundary array then\n        prediction is treated as piecewise linear function and\n        interpolated value is returned. In case there are multiple\n        values with the same boundary then the same rules as in 2)\n        are used.\n\n        :param x:\n          Feature or RDD of Features to be labeled.\n        \"\"\"\n        if isinstance(x, RDD):\n            return x.map(lambda v: self.predict(v))\n        return np.interp(x, self.boundaries, self.predictions)", "docstring": "Predict labels for provided features.\n        Using a piecewise linear function.\n        1) If x exactly matches a boundary then associated prediction\n        is returned. In case there are multiple predictions with the\n        same boundary then one of them is returned. Which one is\n        undefined (same as java.util.Arrays.binarySearch).\n        2) If x is lower or higher than all boundaries then first or\n        last prediction is returned respectively. In case there are\n        multiple pr", "func_name": "IsotonicRegressionModel.predict", "language": "python"}, {"code": "def save(self, sc, path):\n        \"\"\"Save an IsotonicRegressionModel.\"\"\"\n        java_boundaries = _py2java(sc, self.boundaries.tolist())\n        java_predictions = _py2java(sc, self.predictions.tolist())\n        java_model = sc._jvm.org.apache.spark.mllib.regression.IsotonicRegressionModel(\n            java_boundaries, java_predictions, self.isotonic)\n        java_model.save(sc._jsc.sc(), path)", "docstring": "Save an IsotonicRegressionModel.", "func_name": "IsotonicRegressionModel.save", "language": "python"}, {"code": "def load(cls, sc, path):\n        \"\"\"Load an IsotonicRegressionModel.\"\"\"\n        java_model = sc._jvm.org.apache.spark.mllib.regression.IsotonicRegressionModel.load(\n            sc._jsc.sc(), path)\n        py_boundaries = _java2py(sc, java_model.boundaryVector()).toArray()\n        py_predictions = _java2py(sc, java_model.predictionVector()).toArray()\n        return IsotonicRegressionModel(py_boundaries, py_predictions, java_model.isotonic)", "docstring": "Load an IsotonicRegressionModel.", "func_name": "IsotonicRegressionModel.load", "language": "python"}, {"code": "def train(cls, data, isotonic=True):\n        \"\"\"\n        Train an isotonic regression model on the given data.\n\n        :param data:\n          RDD of (label, feature, weight) tuples.\n        :param isotonic:\n          Whether this is isotonic (which is default) or antitonic.\n          (default: True)\n        \"\"\"\n        boundaries, predictions = callMLlibFunc(\"trainIsotonicRegressionModel\",\n                                                data.map(_convert_to_vector), bool(isotonic))\n        return IsotonicRegressionModel(boundaries.toArray(), predictions.toArray(), isotonic)", "docstring": "Train an isotonic regression model on the given data.\n\n        :param data:\n          RDD of (label, feature, weight) tuples.\n        :param isotonic:\n          Whether this is isotonic (which is default) or antitonic.\n          (default: True)", "func_name": "IsotonicRegression.train", "language": "python"}, {"code": "def columnSimilarities(self, threshold=0.0):\n        \"\"\"\n        Compute similarities between columns of this matrix.\n\n        The threshold parameter is a trade-off knob between estimate\n        quality and computational cost.\n\n        The default threshold setting of 0 guarantees deterministically\n        correct results, but uses the brute-force approach of computing\n        normalized dot products.\n\n        Setting the threshold to positive values uses a sampling\n        approach and incurs strictly less computational cost than the\n        brute-force approach. However the similarities computed will\n        be estimates.\n\n        The sampling guarantees relative-error correctness for those\n        pairs of columns that have similarity greater than the given\n        similarity threshold.\n\n        To describe the guarantee, we set some notation:\n            * Let A be the smallest in magnitude non-zero element of\n              this matrix.\n            * Let B be the largest in magnitude non-zero element of\n              this matrix.\n            * Let L be the maximum number of non-zeros per row.\n\n        For example, for {0,1} matrices: A=B=1.\n        Another example, for the Netflix matrix: A=1, B=5\n\n        For those column pairs that are above the threshold, the\n        computed similarity is correct to within 20% relative error\n        with probability at least 1 - (0.981)^10/B^\n\n        The shuffle size is bounded by the *smaller* of the following\n        two expressions:\n\n            * O(n log(n) L / (threshold * A))\n            * O(m L^2^)\n\n        The latter is the cost of the brute-force approach, so for\n        non-zero thresholds, the cost is always cheaper than the\n        brute-force approach.\n\n        :param: threshold: Set to 0 for deterministic guaranteed\n                           correctness. Similarities above this\n                           threshold are estimated with the cost vs\n                           estimate quality trade-off described ", "docstring": "Compute similarities between columns of this matrix.\n\n        The threshold parameter is a trade-off knob between estimate\n        quality and computational cost.\n\n        The default threshold setting of 0 guarantees deterministically\n        correct results, but uses the brute-force approach of computing\n        normalized dot products.\n\n        Setting the threshold to positive values uses a sampling\n        approach and incurs strictly less computational cost than the\n        brute-force app", "func_name": "RowMatrix.columnSimilarities", "language": "python"}, {"code": "def tallSkinnyQR(self, computeQ=False):\n        \"\"\"\n        Compute the QR decomposition of this RowMatrix.\n\n        The implementation is designed to optimize the QR decomposition\n        (factorization) for the RowMatrix of a tall and skinny shape.\n\n        Reference:\n         Paul G. Constantine, David F. Gleich. \"Tall and skinny QR\n         factorizations in MapReduce architectures\"\n         ([[https://doi.org/10.1145/1996092.1996103]])\n\n        :param: computeQ: whether to computeQ\n        :return: QRDecomposition(Q: RowMatrix, R: Matrix), where\n                 Q = None if computeQ = false.\n\n        >>> rows = sc.parallelize([[3, -6], [4, -8], [0, 1]])\n        >>> mat = RowMatrix(rows)\n        >>> decomp = mat.tallSkinnyQR(True)\n        >>> Q = decomp.Q\n        >>> R = decomp.R\n\n        >>> # Test with absolute values\n        >>> absQRows = Q.rows.map(lambda row: abs(row.toArray()).tolist())\n        >>> absQRows.collect()\n        [[0.6..., 0.0], [0.8..., 0.0], [0.0, 1.0]]\n\n        >>> # Test with absolute values\n        >>> abs(R.toArray()).tolist()\n        [[5.0, 10.0], [0.0, 1.0]]\n        \"\"\"\n        decomp = JavaModelWrapper(self._java_matrix_wrapper.call(\"tallSkinnyQR\", computeQ))\n        if computeQ:\n            java_Q = decomp.call(\"Q\")\n            Q = RowMatrix(java_Q)\n        else:\n            Q = None\n        R = decomp.call(\"R\")\n        return QRDecomposition(Q, R)", "docstring": "Compute the QR decomposition of this RowMatrix.\n\n        The implementation is designed to optimize the QR decomposition\n        (factorization) for the RowMatrix of a tall and skinny shape.\n\n        Reference:\n         Paul G. Constantine, David F. Gleich. \"Tall and skinny QR\n         factorizations in MapReduce architectures\"\n         ([[https://doi.org/10.1145/1996092.1996103]])\n\n        :param: computeQ: whether to computeQ\n        :return: QRDecomposition(Q: RowMatrix, R: Matrix), where\n   ", "func_name": "RowMatrix.tallSkinnyQR", "language": "python"}, {"code": "def computeSVD(self, k, computeU=False, rCond=1e-9):\n        \"\"\"\n        Computes the singular value decomposition of the RowMatrix.\n\n        The given row matrix A of dimension (m X n) is decomposed into\n        U * s * V'T where\n\n        * U: (m X k) (left singular vectors) is a RowMatrix whose\n             columns are the eigenvectors of (A X A')\n        * s: DenseVector consisting of square root of the eigenvalues\n             (singular values) in descending order.\n        * v: (n X k) (right singular vectors) is a Matrix whose columns\n             are the eigenvectors of (A' X A)\n\n        For more specific details on implementation, please refer\n        the Scala documentation.\n\n        :param k: Number of leading singular values to keep (`0 < k <= n`).\n                  It might return less than k if there are numerically zero singular values\n                  or there are not enough Ritz values converged before the maximum number of\n                  Arnoldi update iterations is reached (in case that matrix A is ill-conditioned).\n        :param computeU: Whether or not to compute U. If set to be\n                         True, then U is computed by A * V * s^-1\n        :param rCond: Reciprocal condition number. All singular values\n                      smaller than rCond * s[0] are treated as zero\n                      where s[0] is the largest singular value.\n        :returns: :py:class:`SingularValueDecomposition`\n\n        >>> rows = sc.parallelize([[3, 1, 1], [-1, 3, 1]])\n        >>> rm = RowMatrix(rows)\n\n        >>> svd_model = rm.computeSVD(2, True)\n        >>> svd_model.U.rows.collect()\n        [DenseVector([-0.7071, 0.7071]), DenseVector([-0.7071, -0.7071])]\n        >>> svd_model.s\n        DenseVector([3.4641, 3.1623])\n        >>> svd_model.V\n        DenseMatrix(3, 2, [-0.4082, -0.8165, -0.4082, 0.8944, -0.4472, 0.0], 0)\n        \"\"\"\n        j_model = self._java_matrix_wrapper.call(\n            \"computeSVD\", int(k), bool(computeU), float(rCond))\n        ", "docstring": "Computes the singular value decomposition of the RowMatrix.\n\n        The given row matrix A of dimension (m X n) is decomposed into\n        U * s * V'T where\n\n        * U: (m X k) (left singular vectors) is a RowMatrix whose\n             columns are the eigenvectors of (A X A')\n        * s: DenseVector consisting of square root of the eigenvalues\n             (singular values) in descending order.\n        * v: (n X k) (right singular vectors) is a Matrix whose columns\n             are the eigenv", "func_name": "RowMatrix.computeSVD", "language": "python"}, {"code": "def multiply(self, matrix):\n        \"\"\"\n        Multiply this matrix by a local dense matrix on the right.\n\n        :param matrix: a local dense matrix whose number of rows must match the number of columns\n                       of this matrix\n        :returns: :py:class:`RowMatrix`\n\n        >>> rm = RowMatrix(sc.parallelize([[0, 1], [2, 3]]))\n        >>> rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n        [DenseVector([2.0, 3.0]), DenseVector([6.0, 11.0])]\n        \"\"\"\n        if not isinstance(matrix, DenseMatrix):\n            raise ValueError(\"Only multiplication with DenseMatrix \"\n                             \"is supported.\")\n        j_model = self._java_matrix_wrapper.call(\"multiply\", matrix)\n        return RowMatrix(j_model)", "docstring": "Multiply this matrix by a local dense matrix on the right.\n\n        :param matrix: a local dense matrix whose number of rows must match the number of columns\n                       of this matrix\n        :returns: :py:class:`RowMatrix`\n\n        >>> rm = RowMatrix(sc.parallelize([[0, 1], [2, 3]]))\n        >>> rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n        [DenseVector([2.0, 3.0]), DenseVector([6.0, 11.0])]", "func_name": "RowMatrix.multiply", "language": "python"}, {"code": "def U(self):\n        \"\"\"\n        Returns a distributed matrix whose columns are the left\n        singular vectors of the SingularValueDecomposition if computeU was set to be True.\n        \"\"\"\n        u = self.call(\"U\")\n        if u is not None:\n            mat_name = u.getClass().getSimpleName()\n            if mat_name == \"RowMatrix\":\n                return RowMatrix(u)\n            elif mat_name == \"IndexedRowMatrix\":\n                return IndexedRowMatrix(u)\n            else:\n                raise TypeError(\"Expected RowMatrix/IndexedRowMatrix got %s\" % mat_name)", "docstring": "Returns a distributed matrix whose columns are the left\n        singular vectors of the SingularValueDecomposition if computeU was set to be True.", "func_name": "SingularValueDecomposition.U", "language": "python"}, {"code": "def rows(self):\n        \"\"\"\n        Rows of the IndexedRowMatrix stored as an RDD of IndexedRows.\n\n        >>> mat = IndexedRowMatrix(sc.parallelize([IndexedRow(0, [1, 2, 3]),\n        ...                                        IndexedRow(1, [4, 5, 6])]))\n        >>> rows = mat.rows\n        >>> rows.first()\n        IndexedRow(0, [1.0,2.0,3.0])\n        \"\"\"\n        # We use DataFrames for serialization of IndexedRows from\n        # Java, so we first convert the RDD of rows to a DataFrame\n        # on the Scala/Java side. Then we map each Row in the\n        # DataFrame back to an IndexedRow on this side.\n        rows_df = callMLlibFunc(\"getIndexedRows\", self._java_matrix_wrapper._java_model)\n        rows = rows_df.rdd.map(lambda row: IndexedRow(row[0], row[1]))\n        return rows", "docstring": "Rows of the IndexedRowMatrix stored as an RDD of IndexedRows.\n\n        >>> mat = IndexedRowMatrix(sc.parallelize([IndexedRow(0, [1, 2, 3]),\n        ...                                        IndexedRow(1, [4, 5, 6])]))\n        >>> rows = mat.rows\n        >>> rows.first()\n        IndexedRow(0, [1.0,2.0,3.0])", "func_name": "IndexedRowMatrix.rows", "language": "python"}, {"code": "def toBlockMatrix(self, rowsPerBlock=1024, colsPerBlock=1024):\n        \"\"\"\n        Convert this matrix to a BlockMatrix.\n\n        :param rowsPerBlock: Number of rows that make up each block.\n                             The blocks forming the final rows are not\n                             required to have the given number of rows.\n        :param colsPerBlock: Number of columns that make up each block.\n                             The blocks forming the final columns are not\n                             required to have the given number of columns.\n\n        >>> rows = sc.parallelize([IndexedRow(0, [1, 2, 3]),\n        ...                        IndexedRow(6, [4, 5, 6])])\n        >>> mat = IndexedRowMatrix(rows).toBlockMatrix()\n\n        >>> # This IndexedRowMatrix will have 7 effective rows, due to\n        >>> # the highest row index being 6, and the ensuing\n        >>> # BlockMatrix will have 7 rows as well.\n        >>> print(mat.numRows())\n        7\n\n        >>> print(mat.numCols())\n        3\n        \"\"\"\n        java_block_matrix = self._java_matrix_wrapper.call(\"toBlockMatrix\",\n                                                           rowsPerBlock,\n                                                           colsPerBlock)\n        return BlockMatrix(java_block_matrix, rowsPerBlock, colsPerBlock)", "docstring": "Convert this matrix to a BlockMatrix.\n\n        :param rowsPerBlock: Number of rows that make up each block.\n                             The blocks forming the final rows are not\n                             required to have the given number of rows.\n        :param colsPerBlock: Number of columns that make up each block.\n                             The blocks forming the final columns are not\n                             required to have the given number of columns.\n\n        >>> rows = sc.paral", "func_name": "IndexedRowMatrix.toBlockMatrix", "language": "python"}, {"code": "def multiply(self, matrix):\n        \"\"\"\n        Multiply this matrix by a local dense matrix on the right.\n\n        :param matrix: a local dense matrix whose number of rows must match the number of columns\n                       of this matrix\n        :returns: :py:class:`IndexedRowMatrix`\n\n        >>> mat = IndexedRowMatrix(sc.parallelize([(0, (0, 1)), (1, (2, 3))]))\n        >>> mat.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n        [IndexedRow(0, [2.0,3.0]), IndexedRow(1, [6.0,11.0])]\n        \"\"\"\n        if not isinstance(matrix, DenseMatrix):\n            raise ValueError(\"Only multiplication with DenseMatrix \"\n                             \"is supported.\")\n        return IndexedRowMatrix(self._java_matrix_wrapper.call(\"multiply\", matrix))", "docstring": "Multiply this matrix by a local dense matrix on the right.\n\n        :param matrix: a local dense matrix whose number of rows must match the number of columns\n                       of this matrix\n        :returns: :py:class:`IndexedRowMatrix`\n\n        >>> mat = IndexedRowMatrix(sc.parallelize([(0, (0, 1)), (1, (2, 3))]))\n        >>> mat.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n        [IndexedRow(0, [2.0,3.0]), IndexedRow(1, [6.0,11.0])]", "func_name": "IndexedRowMatrix.multiply", "language": "python"}, {"code": "def entries(self):\n        \"\"\"\n        Entries of the CoordinateMatrix stored as an RDD of\n        MatrixEntries.\n\n        >>> mat = CoordinateMatrix(sc.parallelize([MatrixEntry(0, 0, 1.2),\n        ...                                        MatrixEntry(6, 4, 2.1)]))\n        >>> entries = mat.entries\n        >>> entries.first()\n        MatrixEntry(0, 0, 1.2)\n        \"\"\"\n        # We use DataFrames for serialization of MatrixEntry entries\n        # from Java, so we first convert the RDD of entries to a\n        # DataFrame on the Scala/Java side. Then we map each Row in\n        # the DataFrame back to a MatrixEntry on this side.\n        entries_df = callMLlibFunc(\"getMatrixEntries\", self._java_matrix_wrapper._java_model)\n        entries = entries_df.rdd.map(lambda row: MatrixEntry(row[0], row[1], row[2]))\n        return entries", "docstring": "Entries of the CoordinateMatrix stored as an RDD of\n        MatrixEntries.\n\n        >>> mat = CoordinateMatrix(sc.parallelize([MatrixEntry(0, 0, 1.2),\n        ...                                        MatrixEntry(6, 4, 2.1)]))\n        >>> entries = mat.entries\n        >>> entries.first()\n        MatrixEntry(0, 0, 1.2)", "func_name": "CoordinateMatrix.entries", "language": "python"}, {"code": "def blocks(self):\n        \"\"\"\n        The RDD of sub-matrix blocks\n        ((blockRowIndex, blockColIndex), sub-matrix) that form this\n        distributed matrix.\n\n        >>> mat = BlockMatrix(\n        ...     sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n        ...                     ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))]), 3, 2)\n        >>> blocks = mat.blocks\n        >>> blocks.first()\n        ((0, 0), DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0))\n\n        \"\"\"\n        # We use DataFrames for serialization of sub-matrix blocks\n        # from Java, so we first convert the RDD of blocks to a\n        # DataFrame on the Scala/Java side. Then we map each Row in\n        # the DataFrame back to a sub-matrix block on this side.\n        blocks_df = callMLlibFunc(\"getMatrixBlocks\", self._java_matrix_wrapper._java_model)\n        blocks = blocks_df.rdd.map(lambda row: ((row[0][0], row[0][1]), row[1]))\n        return blocks", "docstring": "The RDD of sub-matrix blocks\n        ((blockRowIndex, blockColIndex), sub-matrix) that form this\n        distributed matrix.\n\n        >>> mat = BlockMatrix(\n        ...     sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n        ...                     ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))]), 3, 2)\n        >>> blocks = mat.blocks\n        >>> blocks.first()\n        ((0, 0), DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0))", "func_name": "BlockMatrix.blocks", "language": "python"}, {"code": "def persist(self, storageLevel):\n        \"\"\"\n        Persists the underlying RDD with the specified storage level.\n        \"\"\"\n        if not isinstance(storageLevel, StorageLevel):\n            raise TypeError(\"`storageLevel` should be a StorageLevel, got %s\" % type(storageLevel))\n        javaStorageLevel = self._java_matrix_wrapper._sc._getJavaStorageLevel(storageLevel)\n        self._java_matrix_wrapper.call(\"persist\", javaStorageLevel)\n        return self", "docstring": "Persists the underlying RDD with the specified storage level.", "func_name": "BlockMatrix.persist", "language": "python"}, {"code": "def add(self, other):\n        \"\"\"\n        Adds two block matrices together. The matrices must have the\n        same size and matching `rowsPerBlock` and `colsPerBlock` values.\n        If one of the sub matrix blocks that are being added is a\n        SparseMatrix, the resulting sub matrix block will also be a\n        SparseMatrix, even if it is being added to a DenseMatrix. If\n        two dense sub matrix blocks are added, the output block will\n        also be a DenseMatrix.\n\n        >>> dm1 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n        >>> dm2 = Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12])\n        >>> sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 1, 2], [7, 11, 12])\n        >>> blocks1 = sc.parallelize([((0, 0), dm1), ((1, 0), dm2)])\n        >>> blocks2 = sc.parallelize([((0, 0), dm1), ((1, 0), dm2)])\n        >>> blocks3 = sc.parallelize([((0, 0), sm), ((1, 0), dm2)])\n        >>> mat1 = BlockMatrix(blocks1, 3, 2)\n        >>> mat2 = BlockMatrix(blocks2, 3, 2)\n        >>> mat3 = BlockMatrix(blocks3, 3, 2)\n\n        >>> mat1.add(mat2).toLocalMatrix()\n        DenseMatrix(6, 2, [2.0, 4.0, 6.0, 14.0, 16.0, 18.0, 8.0, 10.0, 12.0, 20.0, 22.0, 24.0], 0)\n\n        >>> mat1.add(mat3).toLocalMatrix()\n        DenseMatrix(6, 2, [8.0, 2.0, 3.0, 14.0, 16.0, 18.0, 4.0, 16.0, 18.0, 20.0, 22.0, 24.0], 0)\n        \"\"\"\n        if not isinstance(other, BlockMatrix):\n            raise TypeError(\"Other should be a BlockMatrix, got %s\" % type(other))\n\n        other_java_block_matrix = other._java_matrix_wrapper._java_model\n        java_block_matrix = self._java_matrix_wrapper.call(\"add\", other_java_block_matrix)\n        return BlockMatrix(java_block_matrix, self.rowsPerBlock, self.colsPerBlock)", "docstring": "Adds two block matrices together. The matrices must have the\n        same size and matching `rowsPerBlock` and `colsPerBlock` values.\n        If one of the sub matrix blocks that are being added is a\n        SparseMatrix, the resulting sub matrix block will also be a\n        SparseMatrix, even if it is being added to a DenseMatrix. If\n        two dense sub matrix blocks are added, the output block will\n        also be a DenseMatrix.\n\n        >>> dm1 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n   ", "func_name": "BlockMatrix.add", "language": "python"}, {"code": "def transpose(self):\n        \"\"\"\n        Transpose this BlockMatrix. Returns a new BlockMatrix\n        instance sharing the same underlying data. Is a lazy operation.\n\n        >>> blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n        ...                          ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n        >>> mat = BlockMatrix(blocks, 3, 2)\n\n        >>> mat_transposed = mat.transpose()\n        >>> mat_transposed.toLocalMatrix()\n        DenseMatrix(2, 6, [1.0, 4.0, 2.0, 5.0, 3.0, 6.0, 7.0, 10.0, 8.0, 11.0, 9.0, 12.0], 0)\n        \"\"\"\n        java_transposed_matrix = self._java_matrix_wrapper.call(\"transpose\")\n        return BlockMatrix(java_transposed_matrix, self.colsPerBlock, self.rowsPerBlock)", "docstring": "Transpose this BlockMatrix. Returns a new BlockMatrix\n        instance sharing the same underlying data. Is a lazy operation.\n\n        >>> blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n        ...                          ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n        >>> mat = BlockMatrix(blocks, 3, 2)\n\n        >>> mat_transposed = mat.transpose()\n        >>> mat_transposed.toLocalMatrix()\n        DenseMatrix(2, 6, [1.0, 4.0, 2.0, 5.0, 3.0, 6.0, 7", "func_name": "BlockMatrix.transpose", "language": "python"}, {"code": "def _vector_size(v):\n    \"\"\"\n    Returns the size of the vector.\n\n    >>> _vector_size([1., 2., 3.])\n    3\n    >>> _vector_size((1., 2., 3.))\n    3\n    >>> _vector_size(array.array('d', [1., 2., 3.]))\n    3\n    >>> _vector_size(np.zeros(3))\n    3\n    >>> _vector_size(np.zeros((3, 1)))\n    3\n    >>> _vector_size(np.zeros((1, 3)))\n    Traceback (most recent call last):\n        ...\n    ValueError: Cannot treat an ndarray of shape (1, 3) as a vector\n    \"\"\"\n    if isinstance(v, Vector):\n        return len(v)\n    elif type(v) in (array.array, list, tuple, xrange):\n        return len(v)\n    elif type(v) == np.ndarray:\n        if v.ndim == 1 or (v.ndim == 2 and v.shape[1] == 1):\n            return len(v)\n        else:\n            raise ValueError(\"Cannot treat an ndarray of shape %s as a vector\" % str(v.shape))\n    elif _have_scipy and scipy.sparse.issparse(v):\n        assert v.shape[1] == 1, \"Expected column vector\"\n        return v.shape[0]\n    else:\n        raise TypeError(\"Cannot treat type %s as a vector\" % type(v))", "docstring": "Returns the size of the vector.\n\n    >>> _vector_size([1., 2., 3.])\n    3\n    >>> _vector_size((1., 2., 3.))\n    3\n    >>> _vector_size(array.array('d', [1., 2., 3.]))\n    3\n    >>> _vector_size(np.zeros(3))\n    3\n    >>> _vector_size(np.zeros((3, 1)))\n    3\n    >>> _vector_size(np.zeros((1, 3)))\n    Traceback (most recent call last):\n        ...\n    ValueError: Cannot treat an ndarray of shape (1, 3) as a vector", "func_name": "_vector_size", "language": "python"}, {"code": "def parse(s):\n        \"\"\"\n        Parse string representation back into the DenseVector.\n\n        >>> DenseVector.parse(' [ 0.0,1.0,2.0,  3.0]')\n        DenseVector([0.0, 1.0, 2.0, 3.0])\n        \"\"\"\n        start = s.find('[')\n        if start == -1:\n            raise ValueError(\"Array should start with '['.\")\n        end = s.find(']')\n        if end == -1:\n            raise ValueError(\"Array should end with ']'.\")\n        s = s[start + 1: end]\n\n        try:\n            values = [float(val) for val in s.split(',') if val]\n        except ValueError:\n            raise ValueError(\"Unable to parse values from %s\" % s)\n        return DenseVector(values)", "docstring": "Parse string representation back into the DenseVector.\n\n        >>> DenseVector.parse(' [ 0.0,1.0,2.0,  3.0]')\n        DenseVector([0.0, 1.0, 2.0, 3.0])", "func_name": "DenseVector.parse", "language": "python"}, {"code": "def dot(self, other):\n        \"\"\"\n        Compute the dot product of two Vectors. We support\n        (Numpy array, list, SparseVector, or SciPy sparse)\n        and a target NumPy array that is either 1- or 2-dimensional.\n        Equivalent to calling numpy.dot of the two vectors.\n\n        >>> dense = DenseVector(array.array('d', [1., 2.]))\n        >>> dense.dot(dense)\n        5.0\n        >>> dense.dot(SparseVector(2, [0, 1], [2., 1.]))\n        4.0\n        >>> dense.dot(range(1, 3))\n        5.0\n        >>> dense.dot(np.array(range(1, 3)))\n        5.0\n        >>> dense.dot([1.,])\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        >>> dense.dot(np.reshape([1., 2., 3., 4.], (2, 2), order='F'))\n        array([  5.,  11.])\n        >>> dense.dot(np.reshape([1., 2., 3.], (3, 1), order='F'))\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        \"\"\"\n        if type(other) == np.ndarray:\n            if other.ndim > 1:\n                assert len(self) == other.shape[0], \"dimension mismatch\"\n            return np.dot(self.array, other)\n        elif _have_scipy and scipy.sparse.issparse(other):\n            assert len(self) == other.shape[0], \"dimension mismatch\"\n            return other.transpose().dot(self.toArray())\n        else:\n            assert len(self) == _vector_size(other), \"dimension mismatch\"\n            if isinstance(other, SparseVector):\n                return other.dot(self)\n            elif isinstance(other, Vector):\n                return np.dot(self.toArray(), other.toArray())\n            else:\n                return np.dot(self.toArray(), other)", "docstring": "Compute the dot product of two Vectors. We support\n        (Numpy array, list, SparseVector, or SciPy sparse)\n        and a target NumPy array that is either 1- or 2-dimensional.\n        Equivalent to calling numpy.dot of the two vectors.\n\n        >>> dense = DenseVector(array.array('d', [1., 2.]))\n        >>> dense.dot(dense)\n        5.0\n        >>> dense.dot(SparseVector(2, [0, 1], [2., 1.]))\n        4.0\n        >>> dense.dot(range(1, 3))\n        5.0\n        >>> dense.dot(np.array(range(1, 3))", "func_name": "DenseVector.dot", "language": "python"}, {"code": "def squared_distance(self, other):\n        \"\"\"\n        Squared distance of two Vectors.\n\n        >>> dense1 = DenseVector(array.array('d', [1., 2.]))\n        >>> dense1.squared_distance(dense1)\n        0.0\n        >>> dense2 = np.array([2., 1.])\n        >>> dense1.squared_distance(dense2)\n        2.0\n        >>> dense3 = [2., 1.]\n        >>> dense1.squared_distance(dense3)\n        2.0\n        >>> sparse1 = SparseVector(2, [0, 1], [2., 1.])\n        >>> dense1.squared_distance(sparse1)\n        2.0\n        >>> dense1.squared_distance([1.,])\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        >>> dense1.squared_distance(SparseVector(1, [0,], [1.,]))\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        \"\"\"\n        assert len(self) == _vector_size(other), \"dimension mismatch\"\n        if isinstance(other, SparseVector):\n            return other.squared_distance(self)\n        elif _have_scipy and scipy.sparse.issparse(other):\n            return _convert_to_vector(other).squared_distance(self)\n\n        if isinstance(other, Vector):\n            other = other.toArray()\n        elif not isinstance(other, np.ndarray):\n            other = np.array(other)\n        diff = self.toArray() - other\n        return np.dot(diff, diff)", "docstring": "Squared distance of two Vectors.\n\n        >>> dense1 = DenseVector(array.array('d', [1., 2.]))\n        >>> dense1.squared_distance(dense1)\n        0.0\n        >>> dense2 = np.array([2., 1.])\n        >>> dense1.squared_distance(dense2)\n        2.0\n        >>> dense3 = [2., 1.]\n        >>> dense1.squared_distance(dense3)\n        2.0\n        >>> sparse1 = SparseVector(2, [0, 1], [2., 1.])\n        >>> dense1.squared_distance(sparse1)\n        2.0\n        >>> dense1.squared_distance([1.,])\n        Tra", "func_name": "DenseVector.squared_distance", "language": "python"}, {"code": "def parse(s):\n        \"\"\"\n        Parse string representation back into the SparseVector.\n\n        >>> SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] )')\n        SparseVector(4, {0: 4.0, 1: 5.0})\n        \"\"\"\n        start = s.find('(')\n        if start == -1:\n            raise ValueError(\"Tuple should start with '('\")\n        end = s.find(')')\n        if end == -1:\n            raise ValueError(\"Tuple should end with ')'\")\n        s = s[start + 1: end].strip()\n\n        size = s[: s.find(',')]\n        try:\n            size = int(size)\n        except ValueError:\n            raise ValueError(\"Cannot parse size %s.\" % size)\n\n        ind_start = s.find('[')\n        if ind_start == -1:\n            raise ValueError(\"Indices array should start with '['.\")\n        ind_end = s.find(']')\n        if ind_end == -1:\n            raise ValueError(\"Indices array should end with ']'\")\n        new_s = s[ind_start + 1: ind_end]\n        ind_list = new_s.split(',')\n        try:\n            indices = [int(ind) for ind in ind_list if ind]\n        except ValueError:\n            raise ValueError(\"Unable to parse indices from %s.\" % new_s)\n        s = s[ind_end + 1:].strip()\n\n        val_start = s.find('[')\n        if val_start == -1:\n            raise ValueError(\"Values array should start with '['.\")\n        val_end = s.find(']')\n        if val_end == -1:\n            raise ValueError(\"Values array should end with ']'.\")\n        val_list = s[val_start + 1: val_end].split(',')\n        try:\n            values = [float(val) for val in val_list if val]\n        except ValueError:\n            raise ValueError(\"Unable to parse values from %s.\" % s)\n        return SparseVector(size, indices, values)", "docstring": "Parse string representation back into the SparseVector.\n\n        >>> SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] )')\n        SparseVector(4, {0: 4.0, 1: 5.0})", "func_name": "SparseVector.parse", "language": "python"}, {"code": "def dot(self, other):\n        \"\"\"\n        Dot product with a SparseVector or 1- or 2-dimensional Numpy array.\n\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n        >>> a.dot(a)\n        25.0\n        >>> a.dot(array.array('d', [1., 2., 3., 4.]))\n        22.0\n        >>> b = SparseVector(4, [2], [1.0])\n        >>> a.dot(b)\n        0.0\n        >>> a.dot(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]))\n        array([ 22.,  22.])\n        >>> a.dot([1., 2., 3.])\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        >>> a.dot(np.array([1., 2.]))\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        >>> a.dot(DenseVector([1., 2.]))\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        >>> a.dot(np.zeros((3, 2)))\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        \"\"\"\n\n        if isinstance(other, np.ndarray):\n            if other.ndim not in [2, 1]:\n                raise ValueError(\"Cannot call dot with %d-dimensional array\" % other.ndim)\n            assert len(self) == other.shape[0], \"dimension mismatch\"\n            return np.dot(self.values, other[self.indices])\n\n        assert len(self) == _vector_size(other), \"dimension mismatch\"\n\n        if isinstance(other, DenseVector):\n            return np.dot(other.array[self.indices], self.values)\n\n        elif isinstance(other, SparseVector):\n            # Find out common indices.\n            self_cmind = np.in1d(self.indices, other.indices, assume_unique=True)\n            self_values = self.values[self_cmind]\n            if self_values.size == 0:\n                return 0.0\n            else:\n                other_cmind = np.in1d(other.indices, self.indices, assume_unique=True)\n                return np.dot(self_values, other.values[other_cmind])\n\n        else:\n            return self.dot(_convert_to_vect", "docstring": "Dot product with a SparseVector or 1- or 2-dimensional Numpy array.\n\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n        >>> a.dot(a)\n        25.0\n        >>> a.dot(array.array('d', [1., 2., 3., 4.]))\n        22.0\n        >>> b = SparseVector(4, [2], [1.0])\n        >>> a.dot(b)\n        0.0\n        >>> a.dot(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]))\n        array([ 22.,  22.])\n        >>> a.dot([1., 2., 3.])\n        Traceback (most recent call last):\n            ...\n        AssertionErr", "func_name": "SparseVector.dot", "language": "python"}, {"code": "def squared_distance(self, other):\n        \"\"\"\n        Squared distance from a SparseVector or 1-dimensional NumPy array.\n\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n        >>> a.squared_distance(a)\n        0.0\n        >>> a.squared_distance(array.array('d', [1., 2., 3., 4.]))\n        11.0\n        >>> a.squared_distance(np.array([1., 2., 3., 4.]))\n        11.0\n        >>> b = SparseVector(4, [2], [1.0])\n        >>> a.squared_distance(b)\n        26.0\n        >>> b.squared_distance(a)\n        26.0\n        >>> b.squared_distance([1., 2.])\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        >>> b.squared_distance(SparseVector(3, [1,], [1.0,]))\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        \"\"\"\n        assert len(self) == _vector_size(other), \"dimension mismatch\"\n\n        if isinstance(other, np.ndarray) or isinstance(other, DenseVector):\n            if isinstance(other, np.ndarray) and other.ndim != 1:\n                raise Exception(\"Cannot call squared_distance with %d-dimensional array\" %\n                                other.ndim)\n            if isinstance(other, DenseVector):\n                other = other.array\n            sparse_ind = np.zeros(other.size, dtype=bool)\n            sparse_ind[self.indices] = True\n            dist = other[sparse_ind] - self.values\n            result = np.dot(dist, dist)\n\n            other_ind = other[~sparse_ind]\n            result += np.dot(other_ind, other_ind)\n            return result\n\n        elif isinstance(other, SparseVector):\n            result = 0.0\n            i, j = 0, 0\n            while i < len(self.indices) and j < len(other.indices):\n                if self.indices[i] == other.indices[j]:\n                    diff = self.values[i] - other.values[j]\n                    result += diff * diff\n                    i += 1\n                    j += 1\n                elif self.indices[i] < other.indi", "docstring": "Squared distance from a SparseVector or 1-dimensional NumPy array.\n\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\n        >>> a.squared_distance(a)\n        0.0\n        >>> a.squared_distance(array.array('d', [1., 2., 3., 4.]))\n        11.0\n        >>> a.squared_distance(np.array([1., 2., 3., 4.]))\n        11.0\n        >>> b = SparseVector(4, [2], [1.0])\n        >>> a.squared_distance(b)\n        26.0\n        >>> b.squared_distance(a)\n        26.0\n        >>> b.squared_distance([1., 2.])\n   ", "func_name": "SparseVector.squared_distance", "language": "python"}, {"code": "def toArray(self):\n        \"\"\"\n        Returns a copy of this SparseVector as a 1-dimensional NumPy array.\n        \"\"\"\n        arr = np.zeros((self.size,), dtype=np.float64)\n        arr[self.indices] = self.values\n        return arr", "docstring": "Returns a copy of this SparseVector as a 1-dimensional NumPy array.", "func_name": "SparseVector.toArray", "language": "python"}, {"code": "def asML(self):\n        \"\"\"\n        Convert this vector to the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :return: :py:class:`pyspark.ml.linalg.SparseVector`\n\n        .. versionadded:: 2.0.0\n        \"\"\"\n        return newlinalg.SparseVector(self.size, self.indices, self.values)", "docstring": "Convert this vector to the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :return: :py:class:`pyspark.ml.linalg.SparseVector`\n\n        .. versionadded:: 2.0.0", "func_name": "SparseVector.asML", "language": "python"}, {"code": "def dense(*elements):\n        \"\"\"\n        Create a dense vector of 64-bit floats from a Python list or numbers.\n\n        >>> Vectors.dense([1, 2, 3])\n        DenseVector([1.0, 2.0, 3.0])\n        >>> Vectors.dense(1.0, 2.0)\n        DenseVector([1.0, 2.0])\n        \"\"\"\n        if len(elements) == 1 and not isinstance(elements[0], (float, int, long)):\n            # it's list, numpy.array or other iterable object.\n            elements = elements[0]\n        return DenseVector(elements)", "docstring": "Create a dense vector of 64-bit floats from a Python list or numbers.\n\n        >>> Vectors.dense([1, 2, 3])\n        DenseVector([1.0, 2.0, 3.0])\n        >>> Vectors.dense(1.0, 2.0)\n        DenseVector([1.0, 2.0])", "func_name": "Vectors.dense", "language": "python"}, {"code": "def fromML(vec):\n        \"\"\"\n        Convert a vector from the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :param vec: a :py:class:`pyspark.ml.linalg.Vector`\n        :return: a :py:class:`pyspark.mllib.linalg.Vector`\n\n        .. versionadded:: 2.0.0\n        \"\"\"\n        if isinstance(vec, newlinalg.DenseVector):\n            return DenseVector(vec.array)\n        elif isinstance(vec, newlinalg.SparseVector):\n            return SparseVector(vec.size, vec.indices, vec.values)\n        else:\n            raise TypeError(\"Unsupported vector type %s\" % type(vec))", "docstring": "Convert a vector from the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :param vec: a :py:class:`pyspark.ml.linalg.Vector`\n        :return: a :py:class:`pyspark.mllib.linalg.Vector`\n\n        .. versionadded:: 2.0.0", "func_name": "Vectors.fromML", "language": "python"}, {"code": "def squared_distance(v1, v2):\n        \"\"\"\n        Squared distance between two vectors.\n        a and b can be of type SparseVector, DenseVector, np.ndarray\n        or array.array.\n\n        >>> a = Vectors.sparse(4, [(0, 1), (3, 4)])\n        >>> b = Vectors.dense([2, 5, 4, 1])\n        >>> a.squared_distance(b)\n        51.0\n        \"\"\"\n        v1, v2 = _convert_to_vector(v1), _convert_to_vector(v2)\n        return v1.squared_distance(v2)", "docstring": "Squared distance between two vectors.\n        a and b can be of type SparseVector, DenseVector, np.ndarray\n        or array.array.\n\n        >>> a = Vectors.sparse(4, [(0, 1), (3, 4)])\n        >>> b = Vectors.dense([2, 5, 4, 1])\n        >>> a.squared_distance(b)\n        51.0", "func_name": "Vectors.squared_distance", "language": "python"}, {"code": "def parse(s):\n        \"\"\"Parse a string representation back into the Vector.\n\n        >>> Vectors.parse('[2,1,2 ]')\n        DenseVector([2.0, 1.0, 2.0])\n        >>> Vectors.parse(' ( 100,  [0],  [2])')\n        SparseVector(100, {0: 2.0})\n        \"\"\"\n        if s.find('(') == -1 and s.find('[') != -1:\n            return DenseVector.parse(s)\n        elif s.find('(') != -1:\n            return SparseVector.parse(s)\n        else:\n            raise ValueError(\n                \"Cannot find tokens '[' or '(' from the input string.\")", "docstring": "Parse a string representation back into the Vector.\n\n        >>> Vectors.parse('[2,1,2 ]')\n        DenseVector([2.0, 1.0, 2.0])\n        >>> Vectors.parse(' ( 100,  [0],  [2])')\n        SparseVector(100, {0: 2.0})", "func_name": "Vectors.parse", "language": "python"}, {"code": "def _equals(v1_indices, v1_values, v2_indices, v2_values):\n        \"\"\"\n        Check equality between sparse/dense vectors,\n        v1_indices and v2_indices assume to be strictly increasing.\n        \"\"\"\n        v1_size = len(v1_values)\n        v2_size = len(v2_values)\n        k1 = 0\n        k2 = 0\n        all_equal = True\n        while all_equal:\n            while k1 < v1_size and v1_values[k1] == 0:\n                k1 += 1\n            while k2 < v2_size and v2_values[k2] == 0:\n                k2 += 1\n\n            if k1 >= v1_size or k2 >= v2_size:\n                return k1 >= v1_size and k2 >= v2_size\n\n            all_equal = v1_indices[k1] == v2_indices[k2] and v1_values[k1] == v2_values[k2]\n            k1 += 1\n            k2 += 1\n        return all_equal", "docstring": "Check equality between sparse/dense vectors,\n        v1_indices and v2_indices assume to be strictly increasing.", "func_name": "Vectors._equals", "language": "python"}, {"code": "def _convert_to_array(array_like, dtype):\n        \"\"\"\n        Convert Matrix attributes which are array-like or buffer to array.\n        \"\"\"\n        if isinstance(array_like, bytes):\n            return np.frombuffer(array_like, dtype=dtype)\n        return np.asarray(array_like, dtype=dtype)", "docstring": "Convert Matrix attributes which are array-like or buffer to array.", "func_name": "Matrix._convert_to_array", "language": "python"}, {"code": "def toArray(self):\n        \"\"\"\n        Return an numpy.ndarray\n\n        >>> m = DenseMatrix(2, 2, range(4))\n        >>> m.toArray()\n        array([[ 0.,  2.],\n               [ 1.,  3.]])\n        \"\"\"\n        if self.isTransposed:\n            return np.asfortranarray(\n                self.values.reshape((self.numRows, self.numCols)))\n        else:\n            return self.values.reshape((self.numRows, self.numCols), order='F')", "docstring": "Return an numpy.ndarray\n\n        >>> m = DenseMatrix(2, 2, range(4))\n        >>> m.toArray()\n        array([[ 0.,  2.],\n               [ 1.,  3.]])", "func_name": "DenseMatrix.toArray", "language": "python"}, {"code": "def toSparse(self):\n        \"\"\"Convert to SparseMatrix\"\"\"\n        if self.isTransposed:\n            values = np.ravel(self.toArray(), order='F')\n        else:\n            values = self.values\n        indices = np.nonzero(values)[0]\n        colCounts = np.bincount(indices // self.numRows)\n        colPtrs = np.cumsum(np.hstack(\n            (0, colCounts, np.zeros(self.numCols - colCounts.size))))\n        values = values[indices]\n        rowIndices = indices % self.numRows\n\n        return SparseMatrix(self.numRows, self.numCols, colPtrs, rowIndices, values)", "docstring": "Convert to SparseMatrix", "func_name": "DenseMatrix.toSparse", "language": "python"}, {"code": "def asML(self):\n        \"\"\"\n        Convert this matrix to the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :return: :py:class:`pyspark.ml.linalg.DenseMatrix`\n\n        .. versionadded:: 2.0.0\n        \"\"\"\n        return newlinalg.DenseMatrix(self.numRows, self.numCols, self.values, self.isTransposed)", "docstring": "Convert this matrix to the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :return: :py:class:`pyspark.ml.linalg.DenseMatrix`\n\n        .. versionadded:: 2.0.0", "func_name": "DenseMatrix.asML", "language": "python"}, {"code": "def toArray(self):\n        \"\"\"\n        Return an numpy.ndarray\n        \"\"\"\n        A = np.zeros((self.numRows, self.numCols), dtype=np.float64, order='F')\n        for k in xrange(self.colPtrs.size - 1):\n            startptr = self.colPtrs[k]\n            endptr = self.colPtrs[k + 1]\n            if self.isTransposed:\n                A[k, self.rowIndices[startptr:endptr]] = self.values[startptr:endptr]\n            else:\n                A[self.rowIndices[startptr:endptr], k] = self.values[startptr:endptr]\n        return A", "docstring": "Return an numpy.ndarray", "func_name": "SparseMatrix.toArray", "language": "python"}, {"code": "def asML(self):\n        \"\"\"\n        Convert this matrix to the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :return: :py:class:`pyspark.ml.linalg.SparseMatrix`\n\n        .. versionadded:: 2.0.0\n        \"\"\"\n        return newlinalg.SparseMatrix(self.numRows, self.numCols, self.colPtrs, self.rowIndices,\n                                      self.values, self.isTransposed)", "docstring": "Convert this matrix to the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :return: :py:class:`pyspark.ml.linalg.SparseMatrix`\n\n        .. versionadded:: 2.0.0", "func_name": "SparseMatrix.asML", "language": "python"}, {"code": "def sparse(numRows, numCols, colPtrs, rowIndices, values):\n        \"\"\"\n        Create a SparseMatrix\n        \"\"\"\n        return SparseMatrix(numRows, numCols, colPtrs, rowIndices, values)", "docstring": "Create a SparseMatrix", "func_name": "Matrices.sparse", "language": "python"}, {"code": "def fromML(mat):\n        \"\"\"\n        Convert a matrix from the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :param mat: a :py:class:`pyspark.ml.linalg.Matrix`\n        :return: a :py:class:`pyspark.mllib.linalg.Matrix`\n\n        .. versionadded:: 2.0.0\n        \"\"\"\n        if isinstance(mat, newlinalg.DenseMatrix):\n            return DenseMatrix(mat.numRows, mat.numCols, mat.values, mat.isTransposed)\n        elif isinstance(mat, newlinalg.SparseMatrix):\n            return SparseMatrix(mat.numRows, mat.numCols, mat.colPtrs, mat.rowIndices,\n                                mat.values, mat.isTransposed)\n        else:\n            raise TypeError(\"Unsupported matrix type %s\" % type(mat))", "docstring": "Convert a matrix from the new mllib-local representation.\n        This does NOT copy the data; it copies references.\n\n        :param mat: a :py:class:`pyspark.ml.linalg.Matrix`\n        :return: a :py:class:`pyspark.mllib.linalg.Matrix`\n\n        .. versionadded:: 2.0.0", "func_name": "Matrices.fromML", "language": "python"}, {"code": "def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n        \"\"\"\n        Given a large dataset and an item, approximately find at most k items which have the\n        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n        caching of the transformed data when necessary.\n\n        .. note:: This method is experimental and will likely change behavior in the next release.\n\n        :param dataset: The dataset to search for nearest neighbors of the key.\n        :param key: Feature vector representing the item to search for.\n        :param numNearestNeighbors: The maximum number of nearest neighbors.\n        :param distCol: Output column for storing the distance between each result row and the key.\n                        Use \"distCol\" as default value if it's not specified.\n        :return: A dataset containing at most k items closest to the key. A column \"distCol\" is\n                 added to show the distance between each row and the key.\n        \"\"\"\n        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n                               distCol)", "docstring": "Given a large dataset and an item, approximately find at most k items which have the\n        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n        caching of the transformed data when necessary.\n\n        .. note:: This method is experimental and will likely change behavior in the next release.\n\n        :param dataset: The dataset to search for nearest neighbors of", "func_name": "LSHModel.approxNearestNeighbors", "language": "python"}, {"code": "def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n        \"\"\"\n        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n        transformed data when necessary.\n\n        :param datasetA: One of the datasets to join.\n        :param datasetB: Another dataset to join.\n        :param threshold: The threshold for the distance of row pairs.\n        :param distCol: Output column for storing the distance between each pair of rows. Use\n                        \"distCol\" as default value if it's not specified.\n        :return: A joined dataset containing pairs of rows. The original rows are in columns\n                 \"datasetA\" and \"datasetB\", and a column \"distCol\" is added to show the distance\n                 between each pair.\n        \"\"\"\n        threshold = TypeConverters.toFloat(threshold)\n        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)", "docstring": "Join two datasets to approximately find all pairs of rows whose distance are smaller than\n        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n        transformed data when necessary.\n\n        :param datasetA: One of the datasets to join.\n        :param datasetB: Another dataset to join.\n        :param threshold: The threshold for the distance of row pairs.\n     ", "func_name": "LSHModel.approxSimilarityJoin", "language": "python"}, {"code": "def from_labels(cls, labels, inputCol, outputCol=None, handleInvalid=None):\n        \"\"\"\n        Construct the model directly from an array of label strings,\n        requires an active SparkContext.\n        \"\"\"\n        sc = SparkContext._active_spark_context\n        java_class = sc._gateway.jvm.java.lang.String\n        jlabels = StringIndexerModel._new_java_array(labels, java_class)\n        model = StringIndexerModel._create_from_java_class(\n            \"org.apache.spark.ml.feature.StringIndexerModel\", jlabels)\n        model.setInputCol(inputCol)\n        if outputCol is not None:\n            model.setOutputCol(outputCol)\n        if handleInvalid is not None:\n            model.setHandleInvalid(handleInvalid)\n        return model", "docstring": "Construct the model directly from an array of label strings,\n        requires an active SparkContext.", "func_name": "StringIndexerModel.from_labels", "language": "python"}, {"code": "def from_arrays_of_labels(cls, arrayOfLabels, inputCols, outputCols=None,\n                              handleInvalid=None):\n        \"\"\"\n        Construct the model directly from an array of array of label strings,\n        requires an active SparkContext.\n        \"\"\"\n        sc = SparkContext._active_spark_context\n        java_class = sc._gateway.jvm.java.lang.String\n        jlabels = StringIndexerModel._new_java_array(arrayOfLabels, java_class)\n        model = StringIndexerModel._create_from_java_class(\n            \"org.apache.spark.ml.feature.StringIndexerModel\", jlabels)\n        model.setInputCols(inputCols)\n        if outputCols is not None:\n            model.setOutputCols(outputCols)\n        if handleInvalid is not None:\n            model.setHandleInvalid(handleInvalid)\n        return model", "docstring": "Construct the model directly from an array of array of label strings,\n        requires an active SparkContext.", "func_name": "StringIndexerModel.from_arrays_of_labels", "language": "python"}, {"code": "def setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=False,\n                  locale=None):\n        \"\"\"\n        setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false, \\\n        locale=None)\n        Sets params for this StopWordRemover.\n        \"\"\"\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)", "docstring": "setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false, \\\n        locale=None)\n        Sets params for this StopWordRemover.", "func_name": "StopWordsRemover.setParams", "language": "python"}, {"code": "def loadDefaultStopWords(language):\n        \"\"\"\n        Loads the default stop words for the given language.\n        Supported languages: danish, dutch, english, finnish, french, german, hungarian,\n        italian, norwegian, portuguese, russian, spanish, swedish, turkish\n        \"\"\"\n        stopWordsObj = _jvm().org.apache.spark.ml.feature.StopWordsRemover\n        return list(stopWordsObj.loadDefaultStopWords(language))", "docstring": "Loads the default stop words for the given language.\n        Supported languages: danish, dutch, english, finnish, french, german, hungarian,\n        italian, norwegian, portuguese, russian, spanish, swedish, turkish", "func_name": "StopWordsRemover.loadDefaultStopWords", "language": "python"}, {"code": "def findSynonyms(self, word, num):\n        \"\"\"\n        Find \"num\" number of words closest in similarity to \"word\".\n        word can be a string or vector representation.\n        Returns a dataframe with two fields word and similarity (which\n        gives the cosine similarity).\n        \"\"\"\n        if not isinstance(word, basestring):\n            word = _convert_to_vector(word)\n        return self._call_java(\"findSynonyms\", word, num)", "docstring": "Find \"num\" number of words closest in similarity to \"word\".\n        word can be a string or vector representation.\n        Returns a dataframe with two fields word and similarity (which\n        gives the cosine similarity).", "func_name": "Word2VecModel.findSynonyms", "language": "python"}, {"code": "def findSynonymsArray(self, word, num):\n        \"\"\"\n        Find \"num\" number of words closest in similarity to \"word\".\n        word can be a string or vector representation.\n        Returns an array with two fields word and similarity (which\n        gives the cosine similarity).\n        \"\"\"\n        if not isinstance(word, basestring):\n            word = _convert_to_vector(word)\n        tuples = self._java_obj.findSynonymsArray(word, num)\n        return list(map(lambda st: (st._1(), st._2()), list(tuples)))", "docstring": "Find \"num\" number of words closest in similarity to \"word\".\n        word can be a string or vector representation.\n        Returns an array with two fields word and similarity (which\n        gives the cosine similarity).", "func_name": "Word2VecModel.findSynonymsArray", "language": "python"}, {"code": "def install_exception_handler():\n    \"\"\"\n    Hook an exception handler into Py4j, which could capture some SQL exceptions in Java.\n\n    When calling Java API, it will call `get_return_value` to parse the returned object.\n    If any exception happened in JVM, the result will be Java exception object, it raise\n    py4j.protocol.Py4JJavaError. We replace the original `get_return_value` with one that\n    could capture the Java exception and throw a Python one (with the same error message).\n\n    It's idempotent, could be called multiple times.\n    \"\"\"\n    original = py4j.protocol.get_return_value\n    # The original `get_return_value` is not patched, it's idempotent.\n    patched = capture_sql_exception(original)\n    # only patch the one used in py4j.java_gateway (call Java API)\n    py4j.java_gateway.get_return_value = patched", "docstring": "Hook an exception handler into Py4j, which could capture some SQL exceptions in Java.\n\n    When calling Java API, it will call `get_return_value` to parse the returned object.\n    If any exception happened in JVM, the result will be Java exception object, it raise\n    py4j.protocol.Py4JJavaError. We replace the original `get_return_value` with one that\n    could capture the Java exception and throw a Python one (with the same error message).\n\n    It's idempotent, could be called multiple times.", "func_name": "install_exception_handler", "language": "python"}, {"code": "def toJArray(gateway, jtype, arr):\n    \"\"\"\n    Convert python list to java type array\n    :param gateway: Py4j Gateway\n    :param jtype: java type of element in array\n    :param arr: python type list\n    \"\"\"\n    jarr = gateway.new_array(jtype, len(arr))\n    for i in range(0, len(arr)):\n        jarr[i] = arr[i]\n    return jarr", "docstring": "Convert python list to java type array\n    :param gateway: Py4j Gateway\n    :param jtype: java type of element in array\n    :param arr: python type list", "func_name": "toJArray", "language": "python"}, {"code": "def require_minimum_pandas_version():\n    \"\"\" Raise ImportError if minimum version of Pandas is not installed\n    \"\"\"\n    # TODO(HyukjinKwon): Relocate and deduplicate the version specification.\n    minimum_pandas_version = \"0.19.2\"\n\n    from distutils.version import LooseVersion\n    try:\n        import pandas\n        have_pandas = True\n    except ImportError:\n        have_pandas = False\n    if not have_pandas:\n        raise ImportError(\"Pandas >= %s must be installed; however, \"\n                          \"it was not found.\" % minimum_pandas_version)\n    if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n        raise ImportError(\"Pandas >= %s must be installed; however, \"\n                          \"your version was %s.\" % (minimum_pandas_version, pandas.__version__))", "docstring": "Raise ImportError if minimum version of Pandas is not installed", "func_name": "require_minimum_pandas_version", "language": "python"}, {"code": "def require_minimum_pyarrow_version():\n    \"\"\" Raise ImportError if minimum version of pyarrow is not installed\n    \"\"\"\n    # TODO(HyukjinKwon): Relocate and deduplicate the version specification.\n    minimum_pyarrow_version = \"0.12.1\"\n\n    from distutils.version import LooseVersion\n    try:\n        import pyarrow\n        have_arrow = True\n    except ImportError:\n        have_arrow = False\n    if not have_arrow:\n        raise ImportError(\"PyArrow >= %s must be installed; however, \"\n                          \"it was not found.\" % minimum_pyarrow_version)\n    if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n        raise ImportError(\"PyArrow >= %s must be installed; however, \"\n                          \"your version was %s.\" % (minimum_pyarrow_version, pyarrow.__version__))", "docstring": "Raise ImportError if minimum version of pyarrow is not installed", "func_name": "require_minimum_pyarrow_version", "language": "python"}, {"code": "def launch_gateway(conf=None, popen_kwargs=None):\n    \"\"\"\n    launch jvm gateway\n    :param conf: spark configuration passed to spark-submit\n    :param popen_kwargs: Dictionary of kwargs to pass to Popen when spawning\n        the py4j JVM. This is a developer feature intended for use in\n        customizing how pyspark interacts with the py4j JVM (e.g., capturing\n        stdout/stderr).\n    :return:\n    \"\"\"\n    if \"PYSPARK_GATEWAY_PORT\" in os.environ:\n        gateway_port = int(os.environ[\"PYSPARK_GATEWAY_PORT\"])\n        gateway_secret = os.environ[\"PYSPARK_GATEWAY_SECRET\"]\n        # Process already exists\n        proc = None\n    else:\n        SPARK_HOME = _find_spark_home()\n        # Launch the Py4j gateway using Spark's run command so that we pick up the\n        # proper classpath and settings from spark-env.sh\n        on_windows = platform.system() == \"Windows\"\n        script = \"./bin/spark-submit.cmd\" if on_windows else \"./bin/spark-submit\"\n        command = [os.path.join(SPARK_HOME, script)]\n        if conf:\n            for k, v in conf.getAll():\n                command += ['--conf', '%s=%s' % (k, v)]\n        submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"pyspark-shell\")\n        if os.environ.get(\"SPARK_TESTING\"):\n            submit_args = ' '.join([\n                \"--conf spark.ui.enabled=false\",\n                submit_args\n            ])\n        command = command + shlex.split(submit_args)\n\n        # Create a temporary directory where the gateway server should write the connection\n        # information.\n        conn_info_dir = tempfile.mkdtemp()\n        try:\n            fd, conn_info_file = tempfile.mkstemp(dir=conn_info_dir)\n            os.close(fd)\n            os.unlink(conn_info_file)\n\n            env = dict(os.environ)\n            env[\"_PYSPARK_DRIVER_CONN_INFO_PATH\"] = conn_info_file\n\n            # Launch the Java gateway.\n            popen_kwargs = {} if popen_kwargs is None else popen_kwargs\n            # We open a pipe to stdin so that the Java ", "docstring": "launch jvm gateway\n    :param conf: spark configuration passed to spark-submit\n    :param popen_kwargs: Dictionary of kwargs to pass to Popen when spawning\n        the py4j JVM. This is a developer feature intended for use in\n        customizing how pyspark interacts with the py4j JVM (e.g., capturing\n        stdout/stderr).\n    :return:", "func_name": "launch_gateway", "language": "python"}, {"code": "def _do_server_auth(conn, auth_secret):\n    \"\"\"\n    Performs the authentication protocol defined by the SocketAuthHelper class on the given\n    file-like object 'conn'.\n    \"\"\"\n    write_with_length(auth_secret.encode(\"utf-8\"), conn)\n    conn.flush()\n    reply = UTF8Deserializer().loads(conn)\n    if reply != \"ok\":\n        conn.close()\n        raise Exception(\"Unexpected reply from iterator server.\")", "docstring": "Performs the authentication protocol defined by the SocketAuthHelper class on the given\n    file-like object 'conn'.", "func_name": "_do_server_auth", "language": "python"}]